{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "335f2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, tarfile, math\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from scipy import sparse\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "datafile = \"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphsNEWFEATURES.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddfcdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each graph is stored under its own sub-directory *inside* one .tar:\n",
    "\n",
    "        graphs.tar\n",
    "        ├── 0001/X.npz\n",
    "        ├── 0001/E.npz\n",
    "        ├── 0001/edge_index.npy\n",
    "        ├── 0001/labels.npz\n",
    "        ├── 0001/label_index.npy\n",
    "        ├── 0001/label_value.npy\n",
    "        ├── 0002/…\n",
    "        └── …\n",
    "\n",
    "    The tar is opened once; __getitem__ streams the six files for graph *idx*\n",
    "    straight into memory, converts them to native PyTorch tensors and returns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tar_path: str | Path):\n",
    "        self.tar = tarfile.open(tar_path, mode=\"r:*\")      # gzip/none/…\n",
    "        self.index: dict[str, dict[str, tarfile.TarInfo]] = {}\n",
    "        self.sublen = {}\n",
    "\n",
    "        # Build a small lookup table in RAM  {gid: {filename: tarinfo}}\n",
    "        for member in self.tar.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "\n",
    "            p     = Path(member.name)\n",
    "            gid   = str(p.parent)   # '0007'\n",
    "            fname = p.name          # 'X.npz'\n",
    "\n",
    "            # keep only folders that really are 4-digit graph IDs\n",
    "            if gid[-4:].isdigit():\n",
    "                self.index.setdefault(gid, {})[fname] = member\n",
    "\n",
    "        self.gids = sorted(self.index)\n",
    "\n",
    "        # Remove those with no labels\n",
    "        for gid, files in self.index.items():\n",
    "            if not files.get(\"labels.npz\"):\n",
    "                self.gids.remove(gid)\n",
    "\n",
    "        # Count\n",
    "        name, counts = np.unique([Path(gid).parent.name for gid in self.gids], return_counts=True)\n",
    "\n",
    "        # Get cumsum\n",
    "        running = 0\n",
    "        for lbl, cnt in zip(name, counts):\n",
    "            self.sublen[lbl] = (running, running + cnt)\n",
    "            running += cnt\n",
    "\n",
    "    # ------------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _npz_to_csr(buf: bytes, dtype=torch.float32):\n",
    "        csr = sparse.load_npz(io.BytesIO(buf)).tocsr()\n",
    "        crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "        col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "        val  = torch.from_numpy(csr.data).to(dtype)\n",
    "        return torch.sparse_csr_tensor(\n",
    "            crow, col, val, size=csr.shape, dtype=dtype, requires_grad=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _npy_to_tensor(buf: bytes, dtype):\n",
    "        arr = np.load(io.BytesIO(buf), allow_pickle=False)\n",
    "        return torch.from_numpy(arr).to(dtype)\n",
    "\n",
    "    def get_sublen(self, name):\n",
    "        return self.sublen[name]\n",
    "\n",
    "    # ------------- Dataset API ---------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.gids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gid   = self.gids[idx]\n",
    "        files = self.index[gid]\n",
    "\n",
    "        get = lambda name: self.tar.extractfile(files[name]).read()\n",
    "        \n",
    "        fileinfo = gid\n",
    "\n",
    "        X   = self._npz_to_csr(get(\"X.npz\"),       dtype=torch.float32)\n",
    "        Aef = self._npz_to_csr(get(\"E.npz\"),       dtype=torch.float32)\n",
    "        Lef = self._npz_to_csr(get(\"labels.npz\"),  dtype=torch.float32)\n",
    "\n",
    "        Aei = self._npy_to_tensor(get(\"edge_index.npy\"),  dtype=torch.int64)\n",
    "        Lei = self._npy_to_tensor(get(\"label_index.npy\"), dtype=torch.int64)\n",
    "        y   = self._npy_to_tensor(get(\"label_value.npy\"), dtype=torch.int64)\n",
    "\n",
    "        return fileinfo, X, Aei.t().contiguous(), Aef, Lei.t().contiguous(), Lef, y\n",
    "\n",
    "\n",
    "def concat_csr(blocks):\n",
    "    \"\"\"\n",
    "    Vertically stack CSR matrices that all share the same n_cols.\n",
    "    Keeps sparsity and returns a single torch.sparse_csr_tensor.\n",
    "    \"\"\"\n",
    "    crow_bufs, col_bufs, val_bufs = [], [], []\n",
    "    nnz_so_far, n_rows, n_cols = 0, 0, blocks[0].size(1)\n",
    "\n",
    "    for k, csr in enumerate(blocks):\n",
    "        crow = csr.crow_indices().clone()          # (n_rows_k + 1,)\n",
    "\n",
    "        # 1) shift by *cumulative* nnz so far\n",
    "        crow += nnz_so_far\n",
    "\n",
    "        # 2) drop the leading 0 for every block after the first\n",
    "        if k > 0:\n",
    "            crow = crow[1:]\n",
    "\n",
    "        crow_bufs.append(crow)\n",
    "        col_bufs.append(csr.col_indices())\n",
    "        val_bufs.append(csr.values())\n",
    "\n",
    "        nnz_so_far += csr.values().numel()\n",
    "        n_rows     += csr.size(0)\n",
    "\n",
    "    crow_cat = torch.cat(crow_bufs)\n",
    "    col_cat  = torch.cat(col_bufs)\n",
    "    val_cat  = torch.cat(val_bufs)\n",
    "\n",
    "    return torch.sparse_csr_tensor(\n",
    "        crow_cat, col_cat, val_cat,\n",
    "        size=(n_rows, n_cols),\n",
    "        dtype=val_cat.dtype,\n",
    "        device=val_cat.device,\n",
    "        requires_grad=False\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_graph_collate(batch):\n",
    "    # unpack each graph\n",
    "    filenames, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "\n",
    "    # node-count prefix sum for shifting\n",
    "    node_offsets = torch.cumsum(\n",
    "        torch.tensor([0] + [x.size(0) for x in xs[:-1]]), 0)\n",
    "\n",
    "    # ----- merge node features (CSR) -----------------------------\n",
    "    X_batch = concat_csr(xs)\n",
    "\n",
    "    # ----- merge structural edges --------------------------------\n",
    "    Aei_shifted = []\n",
    "    for off, ei in zip(node_offsets, aei):\n",
    "        Aei_shifted.append(ei + off)   # shift both rows\n",
    "    Aei_batch = torch.cat(Aei_shifted, dim=1)   # (2 , E_tot)\n",
    "\n",
    "    Aef_batch = concat_csr(aef)\n",
    "\n",
    "    # ----- merge label edges -------------------------------------\n",
    "    Lei_shifted = []\n",
    "    for off, ei in zip(node_offsets, lei):\n",
    "        Lei_shifted.append(ei + off)\n",
    "    Lei_batch = torch.cat(Lei_shifted, dim=1)\n",
    "\n",
    "    Lef_batch = concat_csr(lef)\n",
    "    y_batch   = torch.cat(ys)\n",
    "\n",
    "    return filenames, X_batch, Aei_batch, Aef_batch, Lei_batch, Lef_batch, y_batch\n",
    "\n",
    "\n",
    "def debug_collate(batch):\n",
    "    _, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "    print(\"--- one mini-batch ---\")\n",
    "    for i, X in enumerate(xs):\n",
    "        print(f\"graph {i}:  nodes={X.size(0):4d}   \"\n",
    "              f\"struct-edges={aei[i].shape[1]:4d}   \"\n",
    "              f\"label-edges={lei[i].shape[1]:3d}\")\n",
    "    # then call the real collate to keep training code unchanged\n",
    "    return sparse_graph_collate(batch)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── loader utilities\n",
    "def identity_collate(batch):\n",
    "    \"\"\"batch == list of length 1 → return that single sample untouched.\"\"\"\n",
    "    return batch[0]\n",
    "\n",
    "def make_loader(ds, batch_size=1, shuffle=False):\n",
    "    return DataLoader(ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=sparse_graph_collate,\n",
    "                      num_workers=0,\n",
    "                      pin_memory=True)    # fast GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9346e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_e2n_minibatch_from_collate(\n",
    "    X_dense: torch.Tensor,           # (N, Dx) float32\n",
    "    Aei: torch.Tensor,               # (2, EA) int64\n",
    "    Aef_dense: torch.Tensor,         # (EA, De) float32\n",
    "    Lei_chunk: torch.Tensor,         # (2, B) int64\n",
    "    Lef_chunk: torch.Tensor,         # (B, De) float32\n",
    "    y_chunk: torch.Tensor            # (B,) int64/float\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "    Returns a homogeneous PyG Data where:\n",
    "      - DOM nodes are rows [0..N-1]\n",
    "      - Each selected candidate edge becomes a dummy node appended after DOM nodes\n",
    "      - Structural edges are preserved\n",
    "      - We add edges t<->u and t<->v for each dummy node t\n",
    "      - Loss is computed only on dummy nodes (we return y_dummy)\n",
    "    \"\"\"\n",
    "    N, Dx = X_dense.size(0), X_dense.size(1)\n",
    "    De    = Lef_chunk.size(1)\n",
    "    B     = Lef_chunk.size(0)\n",
    "    if B == 0:\n",
    "        return None\n",
    "\n",
    "    # Placeholder x to satisfy PyG; model projects per-type\n",
    "    x_all = torch.zeros((N + B, max(Dx, De)), dtype=X_dense.dtype, device=X_dense.device)\n",
    "    x_all[:N, :Dx] = X_dense\n",
    "    x_all[N:, :De] = Lef_chunk\n",
    "\n",
    "    # Structural edges (+ attrs)\n",
    "    A_ei = Aei.to(torch.long)\n",
    "    A_ea = Aef_dense\n",
    "\n",
    "    # u/v endpoints for the selected candidate edges\n",
    "    u = Lei_chunk[0].to(device=device, dtype=torch.long)\n",
    "    v = Lei_chunk[1].to(device=device, dtype=torch.long)\n",
    "\n",
    "    # Dummy node indices\n",
    "    t = torch.arange(N, N + B, dtype=torch.long, device=X_dense.device)\n",
    "\n",
    "    # Connect dummy to endpoints (both directions)\n",
    "    e1 = torch.stack([t, u], dim=0)\n",
    "    e2 = torch.stack([u, t], dim=0)\n",
    "    e3 = torch.stack([t, v], dim=0)\n",
    "    e4 = torch.stack([v, t], dim=0)\n",
    "    E_ei = torch.cat([e1, e2, e3, e4], dim=1)    # (2, 4B)\n",
    "\n",
    "    # Edge attrs for these links: repeat the candidate feature 4x\n",
    "    E_ea = Lef_chunk.repeat_interleave(4, dim=0)   # (4B, De)\n",
    "\n",
    "    # Concatenate structural + dummy edges\n",
    "    edge_index_all = torch.cat([A_ei, E_ei], dim=1)                 # (2, EA + 4B)\n",
    "    edge_attr_all  = torch.cat([A_ea, E_ea], dim=0)                 # (EA + 4B, De)\n",
    "\n",
    "    # Mask & labels for dummy nodes\n",
    "    mask_dummy = torch.zeros(N + B, dtype=torch.bool, device=X_dense.device)\n",
    "    mask_dummy[N:] = True\n",
    "    y_dummy = y_chunk.to(device=X_dense.device, dtype=torch.float32)\n",
    "\n",
    "    return Data(\n",
    "        x=x_all,\n",
    "        edge_index=edge_index_all,\n",
    "        edge_attr=edge_attr_all,\n",
    "        mask_dummy=mask_dummy,\n",
    "        y_dummy=y_dummy,\n",
    "        N_dom=N, B_dummy=B, Dx=Dx, De=De\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf3607fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge2NodeTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 edge_emb_dim: int = 64,    # project De → edge_emb_dim\n",
    "                 hidden1: int = 128,\n",
    "                 hidden2: int = 64,\n",
    "                 heads: int = 4,\n",
    "                 dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.node_in  = nn.Linear(node_in_dim,  hidden1 * heads)\n",
    "        self.dummy_in = nn.Linear(edge_in_dim,  hidden1 * heads)\n",
    "        self.edge_proj = nn.Linear(edge_in_dim, edge_emb_dim, bias=False)\n",
    "\n",
    "        self.tr1 = TransformerConv(\n",
    "            in_channels=hidden1 * heads,\n",
    "            out_channels=hidden1,\n",
    "            heads=heads,\n",
    "            edge_dim=edge_emb_dim,\n",
    "            dropout=dropout,\n",
    "            beta=True\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(hidden1 * heads)\n",
    "\n",
    "        self.tr2 = TransformerConv(\n",
    "            in_channels=hidden1 * heads,\n",
    "            out_channels=hidden2,\n",
    "            heads=1,\n",
    "            edge_dim=edge_emb_dim,\n",
    "            dropout=dropout,\n",
    "            beta=True\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(hidden2)\n",
    "\n",
    "        self.cls = nn.Linear(hidden2, 1)\n",
    "\n",
    "        # init β-gate ~0.5 so messages aren't suppressed at start\n",
    "        for tr in (self.tr1, self.tr2):\n",
    "            if getattr(tr, \"lin_beta\", None) is not None:\n",
    "                nn.init.zeros_(tr.lin_beta.weight)\n",
    "                if tr.lin_beta.bias is not None:\n",
    "                    nn.init.zeros_(tr.lin_beta.bias)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x_all = data.x                     # (N+B, max(Dx,De))\n",
    "        N, B, Dx, De = data.N_dom, data.B_dummy, data.Dx, data.De\n",
    "\n",
    "        # Type-specific input adapters\n",
    "        h = torch.zeros((N + B, self.node_in.out_features),\n",
    "                        dtype=x_all.dtype, device=x_all.device)\n",
    "        h[:N] = self.node_in(x_all[:N, :Dx])   # DOM\n",
    "        h[N:] = self.dummy_in(x_all[N:, :De])  # dummy (edge-as-node)\n",
    "\n",
    "        # Project edge attrs once\n",
    "        eattr = self.edge_proj(data.edge_attr)\n",
    "\n",
    "        # Two TransformerConv layers\n",
    "        h = F.relu(self.tr1(h, data.edge_index, eattr)); h = self.ln1(h)\n",
    "        h = F.relu(self.tr2(h, data.edge_index, eattr)); h = self.ln2(h)\n",
    "\n",
    "        logits_all = self.cls(h).squeeze(-1)\n",
    "        return logits_all[N:]   # only dummy nodes (candidate-edge nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c8cfa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_e2n_minibatch_from_collate(\n",
    "    X_dense: torch.Tensor,           # (N, Dx) float32 on device\n",
    "    Aei: torch.Tensor,               # (2, EA) int64 on device\n",
    "    Aef_dense: torch.Tensor,         # (EA, De) float32 on device\n",
    "    Lei_chunk: torch.Tensor,         # (2, B) int64 (CPU or device)\n",
    "    Lef_chunk: torch.Tensor,         # (B, De) float32 on device\n",
    "    y_chunk: torch.Tensor            # (B,) int/float (CPU or device)\n",
    ") -> Data:\n",
    "    N, Dx = X_dense.size(0), X_dense.size(1)\n",
    "    De    = Lef_chunk.size(1)\n",
    "    B     = Lef_chunk.size(0)\n",
    "    if B == 0:\n",
    "        return None\n",
    "\n",
    "    device = X_dense.device\n",
    "\n",
    "    # Placeholder x; model will project per type\n",
    "    x_all = torch.zeros((N + B, max(Dx, De)), dtype=X_dense.dtype, device=device)\n",
    "    x_all[:N, :Dx] = X_dense\n",
    "    x_all[N:, :De] = Lef_chunk\n",
    "\n",
    "    # u/v endpoints → move to device\n",
    "    u = Lei_chunk[0].to(device=device, dtype=torch.long)\n",
    "    v = Lei_chunk[1].to(device=device, dtype=torch.long)\n",
    "\n",
    "    # Dummy node indices\n",
    "    t = torch.arange(N, N + B, dtype=torch.long, device=device)\n",
    "\n",
    "    # Connect dummy ↔ endpoints (both directions)\n",
    "    e1 = torch.stack([t, u], dim=0)\n",
    "    e2 = torch.stack([u, t], dim=0)\n",
    "    e3 = torch.stack([t, v], dim=0)\n",
    "    e4 = torch.stack([v, t], dim=0)\n",
    "    E_ei = torch.cat([e1, e2, e3, e4], dim=1)          # (2, 4B)\n",
    "\n",
    "    # Edge attrs for these links: repeat candidate feature 4×\n",
    "    E_ea = Lef_chunk.repeat_interleave(4, dim=0)       # (4B, De)\n",
    "\n",
    "    # Concatenate structural + dummy edges\n",
    "    edge_index_all = torch.cat([Aei, E_ei], dim=1)     # (2, EA+4B)\n",
    "    edge_attr_all  = torch.cat([Aef_dense, E_ea], dim=0)\n",
    "\n",
    "    # Dummy mask & labels\n",
    "    mask_dummy = torch.zeros(N + B, dtype=torch.bool, device=device)\n",
    "    mask_dummy[N:] = True\n",
    "    y_dummy = y_chunk.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    return Data(\n",
    "        x=x_all,\n",
    "        edge_index=edge_index_all,\n",
    "        edge_attr=edge_attr_all,\n",
    "        mask_dummy=mask_dummy,\n",
    "        y_dummy=y_dummy,\n",
    "        N_dom=N, B_dummy=B, Dx=Dx, De=De\n",
    "    )\n",
    "\n",
    "def iterate_e2n_minibatches_from_loader_batch(batch, device, edge_bs=20000):\n",
    "    # Your collate output:\n",
    "    # (filenames, X_csr, Aei, Aef_csr, Lei, Lef_csr, y)\n",
    "    _, X_csr, Aei, Aef_csr, Lei, Lef_csr, y = batch\n",
    "\n",
    "    # Densify once per collated batch\n",
    "    X_dense   = X_csr.to_dense().to(device, non_blocking=True)     # (N, Dx)\n",
    "    Aei       = Aei.to(device, non_blocking=True)                  # (2, EA)\n",
    "    Aef_dense = Aef_csr.to_dense().to(device, non_blocking=True)   # (EA, De)\n",
    "    Lef_dense = Lef_csr.to_dense()                                 # (M, De) CPU ok\n",
    "    # Lei, y stay on CPU; we’ll move slices as needed\n",
    "\n",
    "    M = Lei.size(1)\n",
    "    for s in range(0, M, edge_bs):\n",
    "        e = min(s + edge_bs, M)\n",
    "        Lei_chunk = Lei[:, s:e]                                    # CPU ok\n",
    "        Lef_chunk = Lef_dense[s:e, :].to(device, non_blocking=True)\n",
    "        y_chunk   = y[s:e]                                          # CPU ok\n",
    "        data = build_e2n_minibatch_from_collate(\n",
    "            X_dense, Aei, Aef_dense, Lei_chunk, Lef_chunk, y_chunk\n",
    "        )\n",
    "        if data is None:\n",
    "            continue\n",
    "        yield data  # already on device\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, device,\n",
    "                    edge_bs=20000, clip_norm=1.0, amp=True):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    total_loss, total_B = 0.0, 0\n",
    "    count=0\n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        bs = edge_bs\n",
    "        while True:\n",
    "            try:\n",
    "                for data in iterate_e2n_minibatches_from_loader_batch(batch, device, edge_bs=bs):\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    with torch.cuda.amp.autocast(enabled=amp):\n",
    "                        logits = model(data)\n",
    "                        loss = bce(logits, data.y_dummy)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    if clip_norm:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                    total_loss += float(loss.detach()) * int(data.y_dummy.numel())\n",
    "                    total_B    += int(data.y_dummy.numel())\n",
    "                    if count % 500 == 0:\n",
    "                        print(f\"Epoch {count}/{len(loader)} loss: {loss}\")\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                torch.cuda.empty_cache()\n",
    "                bs = max(bs // 2, 2048)\n",
    "                if bs <= 2048:\n",
    "                    raise\n",
    "    return total_loss / max(total_B, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(loader, model, device, edge_bs=20000):\n",
    "    model.eval()\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    total_loss, total_B = 0.0, 0\n",
    "    all_p, all_y = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        for data in iterate_e2n_minibatches_from_loader_batch(batch, device, edge_bs=edge_bs):\n",
    "            logits = model(data)\n",
    "            loss = bce(logits, data.y_dummy)\n",
    "            total_loss += float(loss.cpu())\n",
    "            total_B    += int(data.y_dummy.numel())\n",
    "            all_p.append(torch.sigmoid(logits).cpu())\n",
    "            all_y.append(data.y_dummy.cpu())\n",
    "\n",
    "    if total_B == 0:\n",
    "        return {\"loss\": 0.0, \"aupr\": 0.0}\n",
    "    p = torch.cat(all_p).numpy()\n",
    "    y = torch.cat(all_y).numpy()\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    aupr = float(average_precision_score(y, p))\n",
    "    return {\"loss\": total_loss / total_B, \"aupr\": aupr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba625bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022132/1073533470.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "/tmp/ipykernel_2022132/1073533470.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/774 loss: 0.05423086881637573\n",
      "epoch 01 | train 0.1460 | val 2.0835 | AUPR 0.6435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022132/1073533470.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "/tmp/ipykernel_2022132/1073533470.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/774 loss: 0.025969387963414192\n",
      "epoch 02 | train 0.0344 | val 2.3053 | AUPR 0.6624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022132/1073533470.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "/tmp/ipykernel_2022132/1073533470.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/774 loss: 0.014316810294985771\n",
      "epoch 03 | train 0.0238 | val 2.1365 | AUPR 0.7099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022132/1073533470.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "/tmp/ipykernel_2022132/1073533470.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/774 loss: 0.023809099569916725\n",
      "epoch 04 | train 0.0214 | val 2.6550 | AUPR 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022132/1073533470.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "/tmp/ipykernel_2022132/1073533470.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/774 loss: 0.017748890444636345\n",
      "epoch 05 | train 0.0191 | val 2.2922 | AUPR 0.7027\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATAFILE_TRAIN = \"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphsNEWFEATURES.tar\"\n",
    "dataset = TarGraphDataset(DATAFILE_TRAIN)\n",
    "N = len(dataset)\n",
    "\n",
    "matchcollege_start, matchcollege_end = dataset.get_sublen('university-matchcollege(2000)')\n",
    "allmovie_start, allmovie_end = dataset.get_sublen('movie-allmovie(2000)')\n",
    "imdb_start, imdb_end = dataset.get_sublen('movie-imdb(2000)')\n",
    "usatoday_start, usatoday_end = dataset.get_sublen('nbaplayer-usatoday(436)')\n",
    "yahoo_start, yahoo_end = dataset.get_sublen('nbaplayer-yahoo(438)')\n",
    "matchcollege_idx = list(range(matchcollege_start, matchcollege_end))\n",
    "allmovie_idx = list(range(allmovie_start, allmovie_end))\n",
    "imdb_idx = list(range(imdb_start, imdb_end))\n",
    "usatoday_idx=list(range(usatoday_start, usatoday_end))\n",
    "yahoo_idx=list(range(yahoo_start, yahoo_end))\n",
    "\n",
    "val_idx = list(set(allmovie_idx))#list(set(matchcollege_idx[-10:])) + list(set(allmovie_idx[-10:]))#\n",
    "train_idx = list(set(range(N)) - set(val_idx) - set(usatoday_idx) - set(yahoo_idx))#list(set(matchcollege_idx + allmovie_idx) - set(val_idx))#\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = make_loader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = make_loader(val_ds,   batch_size=32, shuffle=False)\n",
    "\n",
    "model = Edge2NodeTransformer(\n",
    "    node_in_dim=114, edge_in_dim=200,\n",
    "    edge_emb_dim=64, hidden1=128, hidden2=64, heads=4, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    tl = train_one_epoch(train_loader, model, optimizer, device,\n",
    "                            edge_bs=20000, clip_norm=1.0, amp=True)\n",
    "    metrics = eval_epoch(val_loader, model, device, edge_bs=20000)\n",
    "    print(f\"epoch {epoch:02d} | train {tl:.4f} | val {metrics['loss']:.4f} | AUPR {metrics['aupr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2313a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"firstAttempt.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
