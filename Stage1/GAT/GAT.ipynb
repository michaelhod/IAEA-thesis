{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff77184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import io, tarfile, os\n",
    "import copy\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEwLogits\n",
    "from GATModel import GraphAttentionNetwork\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.current_device()\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "datafile = \"../../data/swde_HTMLgraphsNEWFEATURES.tar\"\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562900ca",
   "metadata": {},
   "source": [
    "***BELOW***\n",
    "If data-loading < 5-10 % of total epoch time with num_workers=0, stick with the simple path.\n",
    "Otherwise, parallel loading with share-friendly torch_sparse.SparseTensor\n",
    "almost always pays off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fd8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────── Tar-reader dataset\n",
    "class TarGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each graph is stored under its own sub-directory *inside* one .tar:\n",
    "\n",
    "        graphs.tar\n",
    "        ├── 0001/X.npz\n",
    "        ├── 0001/E.npz\n",
    "        ├── 0001/edge_index.npy\n",
    "        ├── 0001/labels.npz\n",
    "        ├── 0001/label_index.npy\n",
    "        ├── 0001/label_value.npy\n",
    "        ├── 0002/…\n",
    "        └── …\n",
    "\n",
    "    The tar is opened once; __getitem__ streams the six files for graph *idx*\n",
    "    straight into memory, converts them to native PyTorch tensors and returns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tar_path: str | Path):\n",
    "        self.tar = tarfile.open(tar_path, mode=\"r:*\")      # gzip/none/…\n",
    "        self.index: dict[str, dict[str, tarfile.TarInfo]] = {}\n",
    "        self.sublen = {}\n",
    "\n",
    "        # Build a small lookup table in RAM  {gid: {filename: tarinfo}}\n",
    "        for member in self.tar.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "\n",
    "            p     = Path(member.name)\n",
    "            gid   = str(p.parent)   # '0007'\n",
    "            fname = p.name          # 'X.npz'\n",
    "\n",
    "            # keep only folders that really are 4-digit graph IDs\n",
    "            if gid[-4:].isdigit():\n",
    "                self.index.setdefault(gid, {})[fname] = member\n",
    "\n",
    "        self.gids = sorted(self.index)\n",
    "\n",
    "        # Remove thos with no labels\n",
    "        for gid, files in self.index.items():\n",
    "            if not files.get(\"labels.npz\"):\n",
    "                self.gids.remove(gid)\n",
    "\n",
    "        # Count\n",
    "        name, counts = np.unique([Path(gid).parent.name for gid in self.gids], return_counts=True)\n",
    "\n",
    "        # Get cumsum\n",
    "        running = 0\n",
    "        for lbl, cnt in zip(name, counts):\n",
    "            self.sublen[lbl] = (running, running + cnt)\n",
    "            running += cnt\n",
    "\n",
    "    # ------------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _npz_to_csr(buf: bytes, dtype=torch.float32):\n",
    "        csr = sparse.load_npz(io.BytesIO(buf)).tocsr()\n",
    "        crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "        col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "        val  = torch.from_numpy(csr.data).to(dtype)\n",
    "        return torch.sparse_csr_tensor(\n",
    "            crow, col, val, size=csr.shape, dtype=dtype, requires_grad=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _npy_to_tensor(buf: bytes, dtype):\n",
    "        arr = np.load(io.BytesIO(buf), allow_pickle=False)\n",
    "        return torch.from_numpy(arr).to(dtype)\n",
    "\n",
    "    def get_sublen(self, name):\n",
    "        return self.sublen[name]\n",
    "\n",
    "    # ------------- Dataset API ---------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.gids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gid   = self.gids[idx]\n",
    "        files = self.index[gid]\n",
    "\n",
    "        get = lambda name: self.tar.extractfile(files[name]).read()\n",
    "        \n",
    "        fileinfo = gid\n",
    "\n",
    "        X   = self._npz_to_csr(get(\"X.npz\"),       dtype=torch.float32)\n",
    "        Aef = self._npz_to_csr(get(\"E.npz\"),       dtype=torch.float32)\n",
    "        Lef = self._npz_to_csr(get(\"labels.npz\"),  dtype=torch.float32)\n",
    "\n",
    "        Aei = self._npy_to_tensor(get(\"edge_index.npy\"),  dtype=torch.int64)\n",
    "        Lei = self._npy_to_tensor(get(\"label_index.npy\"), dtype=torch.int64)\n",
    "        y   = self._npy_to_tensor(get(\"label_value.npy\"), dtype=torch.int64)\n",
    "\n",
    "        return fileinfo, X, Aei.t().contiguous(), Aef, Lei.t().contiguous(), Lef, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316ad981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csr(blocks):\n",
    "    \"\"\"\n",
    "    Vertically stack CSR matrices that all share the same n_cols.\n",
    "    Keeps sparsity and returns a single torch.sparse_csr_tensor.\n",
    "    \"\"\"\n",
    "    crow_bufs, col_bufs, val_bufs = [], [], []\n",
    "    nnz_so_far, n_rows, n_cols = 0, 0, blocks[0].size(1)\n",
    "\n",
    "    for k, csr in enumerate(blocks):\n",
    "        crow = csr.crow_indices().clone()          # (n_rows_k + 1,)\n",
    "\n",
    "        # 1) shift by *cumulative* nnz so far\n",
    "        crow += nnz_so_far\n",
    "\n",
    "        # 2) drop the leading 0 for every block after the first\n",
    "        if k > 0:\n",
    "            crow = crow[1:]\n",
    "\n",
    "        crow_bufs.append(crow)\n",
    "        col_bufs.append(csr.col_indices())\n",
    "        val_bufs.append(csr.values())\n",
    "\n",
    "        nnz_so_far += csr.values().numel()\n",
    "        n_rows     += csr.size(0)\n",
    "\n",
    "    crow_cat = torch.cat(crow_bufs)\n",
    "    col_cat  = torch.cat(col_bufs)\n",
    "    val_cat  = torch.cat(val_bufs)\n",
    "\n",
    "    return torch.sparse_csr_tensor(\n",
    "        crow_cat, col_cat, val_cat,\n",
    "        size=(n_rows, n_cols),\n",
    "        dtype=val_cat.dtype,\n",
    "        device=val_cat.device,\n",
    "        requires_grad=False\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_graph_collate(batch):\n",
    "    # unpack each graph\n",
    "    filenames, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "\n",
    "    # node-count prefix sum for shifting\n",
    "    node_counts = [x.size(0) for x in xs]\n",
    "    node_offsets = torch.cumsum(\n",
    "        torch.tensor([0] + [x.size(0) for x in xs[:-1]]), 0)\n",
    "\n",
    "    # ----- merge node features (CSR) -----------------------------\n",
    "    X_batch = concat_csr(xs)\n",
    "\n",
    "    # ----- merge structural edges --------------------------------\n",
    "    Aei_shifted = []\n",
    "    for off, ei in zip(node_offsets, aei):\n",
    "        Aei_shifted.append(ei + off)   # shift both rows\n",
    "    Aei_batch = torch.cat(Aei_shifted, dim=1)   # (2 , E_tot)\n",
    "\n",
    "    Aef_batch = concat_csr(aef)\n",
    "\n",
    "    # ----- merge label edges -------------------------------------\n",
    "    Lei_shifted = []\n",
    "    for off, ei in zip(node_offsets, lei):\n",
    "        Lei_shifted.append(ei + off)\n",
    "    Lei_batch = torch.cat(Lei_shifted, dim=1)\n",
    "\n",
    "    Lef_batch = concat_csr(lef)\n",
    "    y_batch   = torch.cat(ys)\n",
    "\n",
    "    batch_vecs = []\n",
    "    for gid, count in enumerate(node_counts):\n",
    "        batch_vecs.append(torch.full((count,), gid, dtype=torch.long))\n",
    "    batch = torch.cat(batch_vecs, dim=0)\n",
    "\n",
    "    return filenames, X_batch, Aei_batch, Aef_batch, Lei_batch, Lef_batch, y_batch, batch\n",
    "\n",
    "def debug_collate(batch):\n",
    "    _, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "    print(\"--- one mini-batch ---\")\n",
    "    for i, X in enumerate(xs):\n",
    "        print(f\"graph {i}:  nodes={X.size(0):4d}   \"\n",
    "              f\"struct-edges={aei[i].shape[1]:4d}   \"\n",
    "              f\"label-edges={lei[i].shape[1]:3d}\")\n",
    "    # then call the real collate to keep training code unchanged\n",
    "    return sparse_graph_collate(batch)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── loader utilities\n",
    "def identity_collate(batch):\n",
    "    \"\"\"batch == list of length 1 → return that single sample untouched.\"\"\"\n",
    "    return batch[0]\n",
    "\n",
    "def make_loader(ds, batch_size=1, shuffle=False):\n",
    "    return DataLoader(ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=sparse_graph_collate,\n",
    "                      num_workers=0,\n",
    "                      pin_memory=True)    # fast GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec604253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset   = TarGraphDataset(\"../../data/swde_HTMLgraphs.tar\")\n",
    "# loader    = make_loader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# next(iter(loader))\n",
    "\n",
    "# count = 0\n",
    "# for fileinfo, X, Aei, Aef, Lei, Lef, y, batch in loader:\n",
    "#     print(fileinfo)\n",
    "#     count +=1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lazy loader for individual files\n",
    "\n",
    "# class LazyGraphDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Each graph lives in its own .npz / .pt / whatever on disk.\n",
    "#     __getitem__ loads it just-in-time.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, folderpaths):\n",
    "#         \"\"\"\n",
    "#         meta_csv: a CSV (or list of dicts) with columns:\n",
    "#             path_X, path_A_index, path_A_feat, path_L_index, path_L_feat, path_y\n",
    "#         Only these tiny strings stay in RAM.\n",
    "#         \"\"\"\n",
    "#         self.folderpaths = list(folderpaths)\n",
    "\n",
    "#     def _import_tensor(self, filename: str, dtype: torch.dtype, is_sparse: bool = False):\n",
    "#         \"\"\"\n",
    "#         Load a .npz CSR matrix and return either\n",
    "#         • a torch.sparse_csr_tensor              (if is_sparse=True)\n",
    "#         • a torch.Tensor (dense)                 (otherwise)\n",
    "#         \"\"\"\n",
    "#         csr = sparse.load_npz(filename).tocsr()\n",
    "\n",
    "#         if is_sparse:\n",
    "#             crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "#             col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "#             val  = torch.from_numpy(csr.data).to(dtype)\n",
    "#             return torch.sparse_csr_tensor(crow, col, val,size=csr.shape, dtype=dtype, requires_grad=False)\n",
    "#         # — otherwise densify —\n",
    "#         return torch.from_numpy(csr.toarray()).to(dtype)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         folder_path = self.folderpaths[idx]\n",
    "\n",
    "#         X = self._import_tensor((folder_path/\"X.npz\"), torch.float32, is_sparse=False)\n",
    "#         #A = self._import_tensor(folder_path/\"A.npz\", torch.long, True)\n",
    "#         Aef = self._import_tensor((folder_path/\"E.npz\"), torch.float32, is_sparse=True)\n",
    "#         Aei = torch.from_numpy(np.load((folder_path/\"edge_index.npy\")))\n",
    "#         Lef = self._import_tensor((folder_path/\"labels.npz\"), torch.float32, is_sparse=True)\n",
    "#         Lei = torch.from_numpy(np.load((folder_path/\"label_index.npy\")))\n",
    "#         y = torch.from_numpy(np.load((folder_path/\"label_value.npy\")))\n",
    "\n",
    "#         return X, Aei, Aef, Lei, Lef, y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.folderpaths)\n",
    "\n",
    "# def graph_collate(batch):\n",
    "#     # batch is a list of tuples\n",
    "#     xs, aei, aef, lei, lef, ys = zip(*batch)   # tuples of length B\n",
    "\n",
    "#     return (list(xs),                          # list of sparse X\n",
    "#             list(aei),                         # list of edge_index\n",
    "#             list(aef),                         # list of sparse A_edge_feat\n",
    "#             list(lei),\n",
    "#             list(lef),\n",
    "#             list(ys))                          # dense y can still be list/stack\n",
    "\n",
    "# def make_loader(dataset, batch_size=1, shuffle=False):\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=graph_collate, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f02934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def walk_limited(root: Path, max_depth: int, pat: str):\n",
    "#     root_depth = len(root.parts)\n",
    "#     for dirpath, dirnames, _ in os.walk(root):\n",
    "#         depth = len(Path(dirpath).parts) - root_depth\n",
    "#         if depth > max_depth:\n",
    "#             # prune traversal\n",
    "#             dirnames[:] = []\n",
    "#             continue\n",
    "#         for d in dirnames:\n",
    "#             p = Path(dirpath, d)\n",
    "#             if p.match(pat):\n",
    "#                 yield p\n",
    "\n",
    "# src = Path(\"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphs/movie/movie\")\n",
    "# batch_dirs = list(walk_limited(src, max_depth=2, pat='[0-9][0-9][0-9][0-9]'))\n",
    "# print(src.exists())\n",
    "# batchFiles = list(src.rglob(\"[0-9][0-9][0-9][0-9]\"))\n",
    "# print(len(batchFiles))\n",
    "# dataset = LazyGraphDataset(batchFiles)\n",
    "# dataloader = make_loader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d224978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Xs, Aeis, Aefs, Leis, Lefs, ys in dataloader:\n",
    "#     print(Xs[0].shape, Aeis[0].shape, Aefs[0].shape, Leis[0].shape, Lefs[0].shape, ys[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e379c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to normalise the A matrix\n",
    "# def symmetric_normalize(A_tilde):\n",
    "#     \"\"\"\n",
    "#     Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "#       A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "#     Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "#     A_tilde (N, N): Adj. matrix with self loops\n",
    "#     Returns:\n",
    "#       A_norm : (N, N)\n",
    "#     \"\"\"\n",
    "\n",
    "#     eps = 1e-5\n",
    "#     d = A_tilde.sum(dim=1) + eps\n",
    "#     D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "#     return (D_inv @ A_tilde @ D_inv).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0048a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_live(\n",
    "    train_vals,\n",
    "    val_vals,\n",
    "    p, r, f1,                 # new metric lists (same length as train/val)\n",
    "    save_path,\n",
    "    xlabel=\"Epoch\",\n",
    "    ylabel_left=\"Loss / Accuracy\",\n",
    "    ylabel_right=\"P · R · F1\",\n",
    "    title=\"Training progress\",\n",
    "    fig_ax=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Live-updating dual-axis plot.\n",
    "    Call once per epoch with the (growing) metric lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_vals, val_vals : list[float]\n",
    "        Main metric to compare (e.g. loss or accuracy).\n",
    "    p, r, f1 : list[float]\n",
    "        Precision, recall, f1 – plotted on a secondary y-axis.\n",
    "    save_path : str or Path\n",
    "        Where to write the PNG each time.\n",
    "    fig_ax : tuple(fig, (ax_left, ax_right)) | None\n",
    "        Pass back what you got from the previous call to avoid flicker.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (fig, (ax_left, ax_right))\n",
    "        Feed this straight back into the next call.\n",
    "    \"\"\"\n",
    "    # ---------- figure / axes boilerplate ----------\n",
    "    if fig_ax is None:\n",
    "        fig, ax_left = plt.subplots(figsize=(8, 5))\n",
    "        ax_right = ax_left.twinx()\n",
    "    else:\n",
    "        fig, (ax_left, ax_right) = fig_ax\n",
    "\n",
    "    # ---------- clear and redraw ----------\n",
    "    ax_left.cla()\n",
    "    ax_right.cla()\n",
    "\n",
    "    epochs = range(1, len(train_vals) + 1)\n",
    "\n",
    "    # left-axis curves\n",
    "    ax_left.plot(epochs, train_vals, \"-o\", label=\"Train\", markersize=4)\n",
    "    ax_left.plot(epochs, val_vals,   \"-s\", label=\"Val\",   markersize=4)\n",
    "    ax_left.set_xlabel(xlabel)\n",
    "    ax_left.set_ylabel(ylabel_left)\n",
    "    ax_left.grid(True, axis=\"both\")\n",
    "\n",
    "    # right-axis curves\n",
    "    ax_right.plot(epochs, p,  \"--d\", label=\"Precision\", markersize=4)\n",
    "    ax_right.plot(epochs, r,  \"--^\", label=\"Recall\",    markersize=4)\n",
    "    ax_right.plot(epochs, f1, \"--*\", label=\"F1\",        markersize=4)\n",
    "    ax_right.set_ylabel(ylabel_right)\n",
    "\n",
    "    # one combined legend\n",
    "    lines_l, labels_l = ax_left.get_legend_handles_labels()\n",
    "    lines_r, labels_r = ax_right.get_legend_handles_labels()\n",
    "    ax_left.legend(lines_l + lines_r, labels_l + labels_r, loc=\"upper center\", ncol=5)\n",
    "\n",
    "    ax_left.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(Path(save_path), dpi=150)\n",
    "\n",
    "    return fig, (ax_left, ax_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749c7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To advance the model, use the methods in https://arxiv.org/pdf/2311.02921\n",
    "\n",
    "# class GraphAttentionNetwork(nn.Module):\n",
    "#     \"\"\"\n",
    "#     HTML‑graph model\n",
    "\n",
    "#         X  ─╮\n",
    "#             │  GAT( 96 → 64 )\n",
    "#             │  ReLU\n",
    "#             │  GAT( 64 → 32 )\n",
    "#             │  ReLU\n",
    "#             └─ Edge‑feature constructor\n",
    "#                       [h_i ‖ h_j ‖ φ(e_ij)] ─► MLP(69 → 1)\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     in_dim          : node‑feature size   (= 96)\n",
    "#     edge_in_dim     : raw edge‑feature size (= 197)\n",
    "#     edge_emb_dim    : Edge-feature MLP output dims\n",
    "#     \"\"\"\n",
    "#     def __init__(self,\n",
    "#                  in_dim: int        = 96,\n",
    "#                  edge_in_dim: int   = 197,\n",
    "#                  edge_emb_dim: int  = 8,\n",
    "#                  hidden1: int       = 128,\n",
    "#                  hidden2: int       = 64,\n",
    "#                  hidden3: int       = 32,\n",
    "#                  heads:  int        = 4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # ── Node-level encoder (edge-aware) ────────────────────────────\n",
    "#         self.tr1 = TransformerConv(\n",
    "#             in_channels      = in_dim,\n",
    "#             out_channels     = hidden1,\n",
    "#             heads            = heads,\n",
    "#             edge_dim         = edge_emb_dim,\n",
    "#             dropout          = 0.1,\n",
    "#             beta             = False         # learnable α in α·x + (1-α)·attn\n",
    "#         )\n",
    "#         self.ln1 = nn.LayerNorm(hidden1 * heads)\n",
    "\n",
    "#         self.tr2 = TransformerConv(\n",
    "#             in_channels      = hidden1 * heads,\n",
    "#             out_channels     = hidden2,\n",
    "#             heads            = 1,\n",
    "#             edge_dim         = edge_emb_dim,\n",
    "#             dropout          = 0.1,\n",
    "#             beta             = False\n",
    "#         )\n",
    "#         self.ln2 = nn.LayerNorm(hidden2)\n",
    "#         self.tr3 = TransformerConv(\n",
    "#             in_channels      = hidden2,\n",
    "#             out_channels     = hidden3,\n",
    "#             heads            = 1,\n",
    "#             edge_dim         = edge_emb_dim,\n",
    "#             dropout          = 0.1,\n",
    "#             beta             = False\n",
    "#         )\n",
    "\n",
    "#         # ── Edge feature projector ────────────── (It is not an explicit linear layer as it works on a sparse matrix)\n",
    "#         self.AW_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "#         nn.init.xavier_uniform_(self.AW_edge)\n",
    "#         # self.EW_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "#         # nn.init.xavier_uniform_(self.EW_edge)\n",
    "\n",
    "#         # ── Edge-level MLP decoder (unchanged) ────────────────────────\n",
    "#         self.edge_mlp = nn.Sequential(\n",
    "#             nn.Linear(hidden3 * 2 + edge_emb_dim, hidden3),\n",
    "#             nn.ReLU(),\n",
    "#             #nn.Dropout(0.2),\n",
    "#             nn.Linear(hidden3, 1)\n",
    "#         )\n",
    "\n",
    "#         # init beta gate around 0.5 to avoid identity lock\n",
    "#         for tr in (self.tr1, self.tr2):\n",
    "#             if getattr(tr, \"lin_beta\", None) is not None:\n",
    "#                 nn.init.zeros_(tr.lin_beta.weight)\n",
    "#                 if tr.lin_beta.bias is not None:\n",
    "#                     nn.init.zeros_(tr.lin_beta.bias)\n",
    "\n",
    "#     # ---------------------------------------------------------------------\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x_sparse: torch.Tensor,        # (N_nodes, 96)          sparse\n",
    "#         A_edge_index: torch.Tensor,   # (2, nnz_A)             COO  (from A)\n",
    "#         A_edge_attr: torch.Tensor,    # (nnz_A, 197)           dense / sparse.mm\n",
    "#         E_edge_index: torch.Tensor,   # (2, N_E)               candidates\n",
    "#         E_edge_attr: torch.Tensor,    # (N_E, 197)             sparse features\n",
    "#         E_attr_dropout=0.0,            # Probability of dropping out a whole edge_attr when training\n",
    "#         E_attr_include=True,\n",
    "#         A_attr_include=True\n",
    "#     ):\n",
    "#         # 1) node features\n",
    "#         x_dense = x_sparse.to_dense()\n",
    "#         A_edge_emb = torch.sparse.mm(A_edge_attr, self.AW_edge)     # (nnz_A , 8)\n",
    "#         #A_edge_emb = A_edge_attr.to_dense()\n",
    "#         #E_edge_emb = E_edge_attr.to_dense()\n",
    "\n",
    "#         if not A_attr_include:\n",
    "#             A_edge_emb = torch.zeros_like(A_edge_emb)\n",
    "\n",
    "#         # 2) edge-aware GATv2 layers\n",
    "#         h = F.relu( self.tr1(x_dense, A_edge_index, A_edge_emb) )\n",
    "#         #h = self.ln1(h)\n",
    "#         #Try a linlayer here to condense heads\n",
    "#         h = F.relu( self.tr2(h,        A_edge_index, A_edge_emb) )\n",
    "#         #h = self.ln2(h)\n",
    "#         h = F.relu( self.tr3(h,        A_edge_index, A_edge_emb) )\n",
    "\n",
    "#         # 3) candidate-edge projection  φ(E) = E @ W_edge\n",
    "#         E_edge_emb = torch.sparse.mm(E_edge_attr, self.AW_edge)     # (N_E , 8)\n",
    "        \n",
    "#         if self.training:\n",
    "#             mask = torch.rand(E_edge_emb.size(0), 1,\n",
    "#                             device=E_edge_emb.device) > E_attr_dropout   # (N_E,1)\n",
    "#             E_edge_emb = E_edge_emb * mask\n",
    "\n",
    "#         if not E_attr_include:\n",
    "#             E_edge_emb = torch.zeros_like(E_edge_emb)\n",
    "\n",
    "#         # 4) gather node embeddings and classify\n",
    "#         src, dst = E_edge_index\n",
    "#         z = torch.cat([h[src], h[dst], E_edge_emb], dim=1)      # (N_E , 72)\n",
    "#         return self.edge_mlp(z).squeeze(-1)                   # (N_E ,) returns the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb1095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, targets, alpha = 0.25, gamma = 2.0, reduction: str = \"mean\"):\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    p_t = torch.exp(-bce)          # = σ(z) if y==1 else 1-σ(z)\n",
    "    loss = (alpha * (1.0 - p_t).pow(gamma) * bce)\n",
    "    return loss.mean() if reduction == \"mean\" else loss.sum()\n",
    "\n",
    "\n",
    "# ---------- 2. Pair-wise AUC (logistic ranking) loss -------------------------\n",
    "def pairwise_auc_loss(logits, targets, sample_k: int | None = None):\n",
    "    \"\"\"\n",
    "    logits   : float tensor (B,)\n",
    "    targets  : {0,1} tensor (B,)\n",
    "    sample_k : optional int – #negatives to sample per positive.  If None,\n",
    "               uses *all* positives × negatives (can be heavy for big batches).\n",
    "    \"\"\"\n",
    "    pos_logits = logits[targets == 1]      # shape (P,)\n",
    "    neg_logits = logits[targets == 0]      # shape (N,)\n",
    "\n",
    "    if pos_logits.numel() == 0 or neg_logits.numel() == 0:\n",
    "        # No valid pairs (edge cases in small batches) – return 0 so it\n",
    "        # doesn't break the graph.\n",
    "        return logits.new_tensor(0.0, requires_grad=True)\n",
    "\n",
    "    # --- optional negative subsampling to save memory ---\n",
    "    if sample_k is not None and neg_logits.numel() > sample_k:\n",
    "        idx = torch.randperm(neg_logits.numel(), device=logits.device)[:sample_k]\n",
    "        neg_logits = neg_logits[idx]\n",
    "\n",
    "    # Broadcast positives against negatives: diff = s_pos - s_neg\n",
    "    diff = pos_logits[:, None] - neg_logits[None, :]        # (P, N) or (P, k)\n",
    "    loss = F.softplus(-diff)                                # log(1+e^(-diff))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# ---------- 3. Combined wrapper ----------------------------------------------\n",
    "class PairwiseAUCFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    total_loss = pairwise_auc_loss + lambda_focal * focal_loss\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 gamma: float = 2.0,\n",
    "                 alpha: float = 0.25,\n",
    "                 lambda_focal: float = 0.5,\n",
    "                 sample_k: int | None = None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambda_focal = lambda_focal\n",
    "        self.sample_k = sample_k\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        loss_rank = pairwise_auc_loss(\n",
    "            logits, targets, sample_k=self.sample_k\n",
    "        )\n",
    "        loss_focal = focal_loss(\n",
    "            logits, targets, alpha=self.alpha, gamma=self.gamma\n",
    "        )\n",
    "        return loss_rank * (1 - self.lambda_focal) + self.lambda_focal * loss_focal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_NORM = 2.0           # gradient clipping\n",
    "\n",
    "# ---------- one epoch --------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer,\n",
    "                criterion, sched, epoch, totalEpoch, device=\"cpu\", **kwargs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    running_loss, running_edges = 0.0, 0\n",
    "    count = 0\n",
    "    l = len(loader)\n",
    "\n",
    "    for f, X_sparse, Aei, Aef, Lei, Lef, y, batch in loader:\n",
    "        count += 1\n",
    "        X_sparse, Aei, Aef = X_sparse.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X_sparse, batch, Aei, Aef, Lei, Lef, kwargs[\"p_Lef_drop\"], kwargs[\"use_E_attr\"], kwargs[\"use_A_attr\"])          # (N_label,)\n",
    "        loss   = criterion(logits, y.float())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "\n",
    "        if count % 20 == 0:\n",
    "            print(f\"epoch {count}/{l} \"\n",
    "                    f\"loss={loss:.4f}\")\n",
    "\n",
    "            # loss, p, r, f1 = eval_edge_model(model, loader, criterion, device=device, use_E_attr=kwargs[\"use_E_attr\"], use_A_attr = kwargs[\"use_A_attr\"])\n",
    "            # print(f\"\\t\\tloss={loss:.4f}  P={p:.3f} R={r:.3f} F1={f1:.3f}  E_features={kwargs[\"use_E_attr\"]} A_features={kwargs[\"use_A_attr\"]}\")\n",
    "\n",
    "    return running_loss / running_edges\n",
    "\n",
    "\n",
    "# ---------- evaluation -------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def eval_edge_model(model, loader, criterion, device=\"cpu\", thr_type=\"median\", **kwargs):\n",
    "    model.eval()\n",
    "    TP = FP = FN = 0\n",
    "    TP2 = FP2 = FN2 = 0\n",
    "    running_loss, running_edges = 0.0, 0\n",
    "\n",
    "    filenames = []\n",
    "    for f, X_sparse, Aei, Aef, Lei, Lef, y, batch in loader:\n",
    "        filenames += f\n",
    "        X_sparse, Aei, Aef = X_sparse.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        #Complete Model\n",
    "        logits = model(X_sparse, batch, Aei, Aef, Lei, Lef, 0, kwargs[\"use_E_attr\"], kwargs[\"use_A_attr\"])\n",
    "        loss   = criterion(logits, y.float())\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "        probs  = torch.sigmoid(logits)\n",
    "        if thr_type==\"median\":\n",
    "            thr = torch.median(probs)\n",
    "\n",
    "        pred = (probs >= thr).long()\n",
    "        TP  += ((pred == 1) & (y == 1)).sum().item()\n",
    "        FP  += ((pred == 1) & (y == 0)).sum().item()\n",
    "        FN  += ((pred == 0) & (y == 1)).sum().item()\n",
    "\n",
    "    print(f\"Validating {np.unique([filename[:-5] for filename in filenames])} website type\")\n",
    "    \n",
    "    prec = TP / (TP + FP + 1e-9)\n",
    "    rec  = TP / (TP + FN + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return running_loss / running_edges, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67146255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                load_checkpoint,\n",
    "                num_epochs     = 100,\n",
    "                lr             = 1e-3,\n",
    "                validate_every = 10,\n",
    "                patience       = 10,\n",
    "                device         = \"cpu\"):\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    model_path = \"./model_in_training.pt\"\n",
    "    if os.path.exists(model_path) and load_checkpoint:\n",
    "        print(\"loading existing model...\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    sched = lr_scheduler.OneCycleLR(opt, max_lr=4e-4, epochs=num_epochs, steps_per_epoch=len(train_loader),\n",
    "                   pct_start=0.1, anneal_strategy='cos', div_factor=25, final_div_factor=1e4, cycle_momentum=False)\n",
    "                        #StepLR(opt, step_size=3, gamma=0.9)\n",
    "    criterion = focal_loss\n",
    "    # criterion = PairwiseAUCFocalLoss(\n",
    "    #             gamma=2.0,\n",
    "    #             alpha=0.25,\n",
    "    #             lambda_focal=1,  # 0 ⇒ pure ranking loss; 1 ⇒ equal weight\n",
    "    #             sample_k=128     # speeds up training; set None for exact loss\n",
    "    #         )\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_f1, fig_ax, best_state = 0.0, None, None\n",
    "    train_loss, val_loss, precision, recall, f1score = [], [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        p_Lef_drop = 0#.3 - 0.3 * (epoch-2)/(num_epochs-2 + 1e-9)        \n",
    "        use_E_attr,  use_A_attr = (epoch>0), (epoch>0)\n",
    "\n",
    "        loss = train_epoch(model, train_loader, opt, criterion, sched, epoch, num_epochs, device=device, use_E_attr=use_E_attr, use_A_attr = use_A_attr, p_Lef_drop = p_Lef_drop)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if epoch % validate_every == 0 or epoch == num_epochs:\n",
    "            loss, p, r, f1 = eval_edge_model(model, val_loader, criterion, device=device, use_E_attr=use_E_attr, use_A_attr = use_A_attr)\n",
    "            val_loss.append(loss)\n",
    "\n",
    "            lr_now = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d}/{num_epochs} \"\n",
    "                  f\"loss={loss:.4f}  P={p:.3f} R={r:.3f} F1={f1:.3f}  lr={lr_now:.2e}  E_features={use_E_attr} A_features={use_A_attr}\")\n",
    "            precision.append(p)\n",
    "            recall.append(r)\n",
    "            f1score.append(f1)\n",
    "\n",
    "            if f1 >= best_f1:\n",
    "                best_f1, best_state = f1, copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # if lr_now < 1e-5:\n",
    "            #     print(\"Stop: LR < 1e-5\")\n",
    "            #     break\n",
    "\n",
    "            fig_ax = plot_metrics_live(\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                precision,recall,f1score,\n",
    "                \"CurrentRun\",\n",
    "                xlabel=\"Epoch\",\n",
    "                ylabel_left=\"Loss\",\n",
    "                ylabel_right=\"P · R · F1\",\n",
    "                title=\"Model Performance\",\n",
    "                fig_ax=fig_ax\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return best_state, train_loss, val_loss, fig_ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bbfe9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphAttentionNetwork(\n",
      "  (pe_lin): Linear(in_features=18, out_features=12, bias=True)\n",
      "  (pe_norm): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convs): ModuleList(\n",
      "    (0-5): 6 x GPSConv(108, conv=GINEConv(nn=Sequential(\n",
      "      (0): Linear(in_features=108, out_features=108, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=108, out_features=108, bias=True)\n",
      "    )), heads=4, attn_type=multihead)\n",
      "  )\n",
      "  (edge_mlp): Sequential(\n",
      "    (0): Linear(in_features=248, out_features=108, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=108, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch 1/3095 loss=0.0441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m model = GraphAttentionNetwork(in_dim = \u001b[32m114\u001b[39m, pe_dim=\u001b[32m12\u001b[39m, edge_in_dim = \u001b[32m200\u001b[39m, edge_emb_dim = \u001b[32m32\u001b[39m, heads = \u001b[32m4\u001b[39m)\u001b[38;5;66;03m#16,32,4 was the winner\u001b[39;00m\n\u001b[32m     27\u001b[39m load_checkpoint = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m _, trainloss, valloss, fig_ax = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_every\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, load_checkpoint, num_epochs, lr, validate_every, patience, device)\u001b[39m\n\u001b[32m     37\u001b[39m p_Lef_drop = \u001b[32m0\u001b[39m\u001b[38;5;66;03m#.3 - 0.3 * (epoch-2)/(num_epochs-2 + 1e-9)        \u001b[39;00m\n\u001b[32m     38\u001b[39m use_E_attr,  use_A_attr = (epoch>\u001b[32m0\u001b[39m), (epoch>\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_E_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_E_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_A_attr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_A_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_Lef_drop\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_Lef_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m train_loss.append(loss)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % validate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch == num_epochs:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, sched, epoch, totalEpoch, device, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m count % \u001b[32m1\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     33\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         loss, p, r, f1 = \u001b[43meval_edge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_E_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_E_attr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_A_attr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_A_attr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mloss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  P=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  E_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[33m\"\u001b[39m\u001b[33muse_E_attr\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m A_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[33m\"\u001b[39m\u001b[33muse_A_attr\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss / running_edges\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36meval_edge_model\u001b[39m\u001b[34m(model, loader, criterion, device, thr_type, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m batch = batch.to(device)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#Complete Model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_E_attr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_A_attr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m loss   = criterion(logits, y.float())\n\u001b[32m     60\u001b[39m running_loss  += loss.item() * y.numel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/Stage1/GAT/GATModel.py:91\u001b[39m, in \u001b[36mGraphAttentionNetwork.forward\u001b[39m\u001b[34m(self, x_sparse, batch, A_edge_index, A_edge_attr, E_edge_index, E_edge_attr, E_attr_dropout, E_attr_include, A_attr_include)\u001b[39m\n\u001b[32m     88\u001b[39m h = torch.cat((x, \u001b[38;5;28mself\u001b[39m.pe_lin(x_pe)), \u001b[32m1\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convs:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     h = \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA_edge_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# 3) candidate-edge projection  φ(E) = E @ W_edge\u001b[39;00m\n\u001b[32m     94\u001b[39m E_edge_emb = torch.sparse.mm(E_edge_attr, \u001b[38;5;28mself\u001b[39m.AW_edge)     \u001b[38;5;66;03m# (N_E , 8)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch_geometric/nn/conv/gps_conv.py:156\u001b[39m, in \u001b[36mGPSConv.forward\u001b[39m\u001b[34m(self, x, edge_index, batch, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m h, mask = to_dense_batch(x, batch)\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.attn, torch.nn.MultiheadAttention):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     h, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.attn, PerformerAttention):\n\u001b[32m    159\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.attn(h, mask=mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1313\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1308\u001b[39m         merged_mask, mask_type = \u001b[38;5;28mself\u001b[39m.merge_masks(\n\u001b[32m   1309\u001b[39m             attn_mask, key_padding_mask, query\n\u001b[32m   1310\u001b[39m         )\n\u001b[32m   1312\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_proj_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1313\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_native_multi_head_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m                \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m                \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m                \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m                \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1329\u001b[39m any_nested = query.is_nested \u001b[38;5;129;01mor\u001b[39;00m key.is_nested \u001b[38;5;129;01mor\u001b[39;00m value.is_nested\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\n\u001b[32m   1331\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m     + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1333\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = TarGraphDataset(datafile)\n",
    "N = len(dataset)\n",
    "# n_train = int(0.95 * N)\n",
    "# n_val   = N - n_train\n",
    "# train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "matchcollege_start, matchcollege_end = dataset.get_sublen('university-matchcollege(2000)')\n",
    "allmovie_start, allmovie_end = dataset.get_sublen('movie-allmovie(2000)')\n",
    "imdb_start, imdb_end = dataset.get_sublen('movie-imdb(2000)')\n",
    "usatoday_start, usatoday_end = dataset.get_sublen('nbaplayer-usatoday(436)')\n",
    "yahoo_start, yahoo_end = dataset.get_sublen('nbaplayer-yahoo(438)')\n",
    "matchcollege_idx = list(range(matchcollege_start, matchcollege_end))\n",
    "allmovie_idx = list(range(allmovie_start, allmovie_end))\n",
    "imdb_idx = list(range(imdb_start, imdb_end))\n",
    "usatoday_idx=list(range(usatoday_start, usatoday_end))\n",
    "yahoo_idx=list(range(yahoo_start, yahoo_end))\n",
    "\n",
    "val_idx = list(set(allmovie_idx))#list(set(matchcollege_idx[-10:])) + list(set(allmovie_idx[-10:]))#\n",
    "train_idx = list(set(range(N)) - set(val_idx) - set(usatoday_idx) - set(yahoo_idx))#list(set(matchcollege_idx + allmovie_idx) - set(val_idx))#\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = make_loader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = make_loader(val_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "model = GraphAttentionNetwork(in_dim = 114, pe_dim=12, edge_in_dim = 200, edge_emb_dim = 32, heads = 4)#16,32,4 was the winner\n",
    "\n",
    "load_checkpoint = False\n",
    "_, trainloss, valloss, fig_ax = train_model(model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            load_checkpoint,\n",
    "            num_epochs     = 12,\n",
    "            lr             = 1e-3,\n",
    "            validate_every = 1,\n",
    "            patience       = 1,\n",
    "            device         = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), \"FULLTRAINEDALLDATAModelf1-74-learning-bettersched.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d59625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04156546794519418,\n",
       " 0.7552735881680096,\n",
       " 0.7551079869696979,\n",
       " 0.7551907779904338)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = \"./FULLTRAINEDALLDATAModelf1-74-learning.pt\"\n",
    "# if os.path.exists(model_path) and load_checkpoint:\n",
    "#     print(\"loading existing model...\")\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "\n",
    "eval_edge_model(model, val_loader, focal_loss, device=\"cuda\", use_E_attr=True, use_A_attr=True)\n",
    "#b4 submitting to A100\n",
    "#Experiemnt with the comparison loss\n",
    "#Do self layers myself\n",
    "#\n",
    "#Graphs of also without edges\n",
    "\n",
    "#32 32 layers\n",
    "#Just train everything from the start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
