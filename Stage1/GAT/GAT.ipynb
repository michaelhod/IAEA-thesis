{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 16\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tensor(filename, dtype, is_sparse=False):\n",
    "\n",
    "    csr = sparse.load_npz(filename)\n",
    "\n",
    "    if is_sparse:\n",
    "        coo = csr.tocoo()\n",
    "\n",
    "        # Build a PyTorch sparse tensor\n",
    "        indices = torch.vstack([\n",
    "            torch.from_numpy(coo.row.astype(np.int64)),\n",
    "            torch.from_numpy(coo.col.astype(np.int64))\n",
    "        ])\n",
    "        values = torch.from_numpy(coo.data)\n",
    "\n",
    "        sparse_tensor = torch.sparse_coo_tensor(indices, values, coo.shape, dtype=dtype).coalesce()\n",
    "        # A_dense  = A_sparse.to_dense()\n",
    "\n",
    "        return sparse_tensor\n",
    "\n",
    "    else:\n",
    "        dense = torch.from_numpy(csr.toarray())\n",
    "\n",
    "        return dense\n",
    "\n",
    "def read_data(folder_paths, has_label=True, is_sparse=False):\n",
    "    graph_data = []\n",
    "    for folder_path in folder_paths:\n",
    "        X = import_tensor(folder_path/\"X.npz\", torch.long, is_sparse)\n",
    "        A = import_tensor(folder_path/\"A.npz\", torch.long, is_sparse)\n",
    "        E = import_tensor(folder_path/\"E.npz\", torch.float32, is_sparse)\n",
    "        edge_index = np.load(folder_path/\"edge_index.npy\")\n",
    "        if has_label:\n",
    "            y = import_tensor(folder_path/\"labels.npz\", torch.long, is_sparse)\n",
    "        else:\n",
    "            y = None\n",
    "        \n",
    "        graph_data.append((X,A,E,edge_index,y))\n",
    "    \n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f02934d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([[  0,   0,   1,  ..., 619, 620, 620],\n",
       "                        [ 37,  95,   9,  ...,  95,   0,  95]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(621, 96), nnz=1242, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[  0,   1,   1,  ..., 618, 619, 620],\n",
       "                        [  1,   0,   2,  ..., 585, 586, 587]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(621, 621), nnz=2758, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[   0,    0,    0,  ..., 2757, 2757, 2757],\n",
       "                        [  37,   95,  105,  ...,  192,  195,  196]]),\n",
       "        values=tensor([ 1.0000,  1.0000,  1.0000,  ...,  1.0000, 12.0312,\n",
       "                        1.0000]),\n",
       "        size=(2758, 197), nnz=18566, layout=torch.sparse_coo),\n",
       " array([[  0,   1],\n",
       "        [  1,   0],\n",
       "        [  1,   2],\n",
       "        ...,\n",
       "        [618, 585],\n",
       "        [619, 586],\n",
       "        [620, 587]], shape=(2758, 2)),\n",
       " tensor(indices=tensor([[257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 278,\n",
       "                         301, 325, 513, 514, 516, 516, 518, 519, 521, 522, 523,\n",
       "                         524, 526, 526, 526, 528, 528, 528, 528, 530, 530, 530,\n",
       "                         530, 530, 530, 530, 530, 530, 530, 530, 532, 532, 532,\n",
       "                         532, 532, 534, 534, 534, 534, 534, 534, 536, 538, 540,\n",
       "                         542, 544, 544, 544, 546],\n",
       "                        [305, 307, 309, 311, 313, 315, 317, 319, 321, 323, 325,\n",
       "                         381, 326, 456, 387, 548, 549, 588, 398, 470, 403, 473,\n",
       "                         589, 590, 591, 592, 555, 556, 557, 558, 593, 594, 595,\n",
       "                         596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606,\n",
       "                         607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617,\n",
       "                         584, 618, 619, 620, 509]]),\n",
       "        values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1]),\n",
       "        size=(621, 621), nnz=60, layout=torch.sparse_coo))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = Path(\"../../data/swde_HTMLgraphs/movie/movie\")\n",
    "batchFiles = list(src.rglob(\"[0-9][0-9][0-9][0-9]\"))\n",
    "data = read_data(batchFiles[0:400], True, True)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e379c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to normalise the A matrix\n",
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return (D_inv @ A_tilde @ D_inv).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0352923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
    "        super(AttentionGCNLayer, self).__init__()\n",
    "        self.use_nonlinearity = use_nonlinearity\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.B_k = nn.Parameter(torch.randn(output_dim))\n",
    "        self.omega_k = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        self.phi_k = nn.Linear(output_dim, output_dim, bias=False)\n",
    "        \n",
    "    def forward(self, H_k, A):\n",
    "        num_nodes = H_k.shape[0]\n",
    "        \n",
    "        # Step 1: Compute H'_k\n",
    "        B_k_expanded = self.B_k.unsqueeze(0).expand(num_nodes, -1)  # Expand to (num_nodes, output_dim)\n",
    "        H_prime = B_k_expanded + H_k @ self.omega_k  # (num_nodes, output_dim)\n",
    "        \n",
    "        # Step 2: Compute attention-based similarity matrix S\n",
    "        H_transformed = self.phi_k(H_prime)  # (num_nodes, output_dim)\n",
    "        S = torch.matmul(H_transformed, H_transformed.T)  # (num_nodes, num_nodes)\n",
    "        S = torch.sigmoid(S)  # Normalize  # Normalize with a non-linearity\n",
    "        \n",
    "        # Step 3: Soft-masked aggregation\n",
    "        A_hat = A + torch.eye(num_nodes, device=A.device)  # Add self-loops\n",
    "        S_masked = S.masked_fill(A_hat == 0, float('-inf'))  # Apply masking correctly\n",
    "        S_normalized = F.softmax(S_masked, dim=0)  # Apply softmax column-wise\n",
    "        H_k_next = torch.matmul(S_normalized, H_prime)\n",
    "        \n",
    "        return F.relu(H_k_next) if self.use_nonlinearity else H_k_next\n",
    "    \n",
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
    "# #        super().__init__(input_dim, output_dim)\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.use_nonlinearity = use_nonlinearity\n",
    "\n",
    "#         self.b = nn.Parameter(torch.randn(output_dim))\n",
    "#         self.W = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         y = F.linear(X, self.W, self.b)\n",
    "#         if self.use_nl:\n",
    "#             y = F.leaky_relu(y)\n",
    "#         return y\n",
    "\n",
    "class GraphAttentionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GraphAttentionNetwork, self).__init__()\n",
    "        self.gcn1 = AttentionGCNLayer(input_dim, hidden_dim*2, False)\n",
    "        self.gcn2 = AttentionGCNLayer(hidden_dim*2, hidden_dim, False)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, A, X, **kwargs):\n",
    "        H1 = self.gcn1(X, A)\n",
    "        H2 = self.gcn2(H1, A)\n",
    "        edge_classification = self.linear(H2)\n",
    "        output = torch.sigmoid(edge_classification)\n",
    "        \n",
    "        if torch.isnan(output).any():\n",
    "            output = torch.where(torch.isnan(output), torch.zeros_like(output), output)\n",
    "        \n",
    "        if kwargs.get(\"return_embeddings\", None):\n",
    "            return output, H1, H2\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96e35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEwLogits\n",
    "\n",
    "IGNORE_LABEL = -1      # change if you use another sentinel for “no label”\n",
    "\n",
    "# ---------- 1. One training epoch -------------------------------------------\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,               # iterable that yields (X, A, E, edge_index, y)\n",
    "    optimizer,\n",
    "    criterion=BCEwLogits,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    model.train()\n",
    "    total_loss, total_edges = 0.0, 0\n",
    "\n",
    "    for X, A, E, edge_index, y in dataloader:\n",
    "        # Move to device ------------------------------------------------------\n",
    "        X          = X.to(device)\n",
    "        A          = A.to(device)\n",
    "        E          = E.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        y          = y.to(device)\n",
    "\n",
    "        mask = (y != IGNORE_LABEL)          # only supervise labelled edges\n",
    "        if mask.sum() == 0:                 # nothing to learn in this sample\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(X, A, E, edge_index) # (N_edges,)\n",
    "\n",
    "        loss = criterion(logits[mask], y[mask].float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss   += loss.item() * mask.sum().item()\n",
    "        total_edges  += mask.sum().item()\n",
    "\n",
    "    return total_loss / max(total_edges, 1)   # average over labelled edges\n",
    "\n",
    "\n",
    "# ---------- 2. Full training loop -------------------------------------------\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,                 # edge‑level dataloader\n",
    "    val_loader,                   # edge‑level dataloader\n",
    "    num_epochs       = 100,\n",
    "    lr               = 1e-3,\n",
    "    validate_every   = 10,\n",
    "    patience         = 10,\n",
    "    device           = \"cpu\"\n",
    "):\n",
    "    \"\"\"Train `model` to predict whether an edge exists.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer  = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler  = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=0.5, verbose=False\n",
    "    )\n",
    "\n",
    "    best_val_f1, best_state = 0.0, None\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # ---- validation -----------------------------------------------------\n",
    "        if epoch % validate_every == 0 or epoch == num_epochs:\n",
    "            val_prec, val_rec, val_f1 = evaluate_edge_model(\n",
    "                model, val_loader, device=device\n",
    "            )\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(\n",
    "                f\"Epoch {epoch:03d}/{num_epochs}  \"\n",
    "                f\"loss={loss:.4f}  \"\n",
    "                f\"P={val_prec:.3f}  R={val_rec:.3f}  F1={val_f1:.3f}  \"\n",
    "                f\"lr={current_lr:.2e}\"\n",
    "            )\n",
    "\n",
    "            # keep the best‑F1 checkpoint\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_state  = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # early stop if LR too small\n",
    "            if current_lr < 1e-5:\n",
    "                print(\"LR below 1e‑5 → stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return loss_history, best_state\n",
    "\n",
    "\n",
    "# ---------- 3. Helper: evaluation (precision / recall / F1) -----------------\n",
    "@torch.no_grad()\n",
    "def evaluate_edge_model(model, dataloader, device=\"cpu\", thr=0.5):\n",
    "    model.eval()\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    for X, A, E, edge_index, y in dataloader:\n",
    "        X, A, E = X.to(device), A.to(device), E.to(device)\n",
    "        edge_index, y = edge_index.to(device), y.to(device)\n",
    "\n",
    "        mask = (y != IGNORE_LABEL)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        logits = model(X, A, E, edge_index)\n",
    "        probs  = torch.sigmoid(logits)\n",
    "\n",
    "        pred = (probs >= thr).long()\n",
    "        tp  += ((pred == 1) & (y == 1) & mask).sum().item()\n",
    "        fp  += ((pred == 1) & (y == 0) & mask).sum().item()\n",
    "        fn  += ((pred == 0) & (y == 1) & mask).sum().item()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-9)\n",
    "    recall    = tp / (tp + fn + 1e-9)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    return precision, recall, f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
