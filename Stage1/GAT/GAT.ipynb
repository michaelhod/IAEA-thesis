{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff77184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, TransformerConv\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import io, tarfile, os\n",
    "import copy\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEwLogits\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.current_device()\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "datafile = \"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphsNEWFEATURES.tar\"\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "SEED = 16\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562900ca",
   "metadata": {},
   "source": [
    "***BELOW***\n",
    "If data-loading < 5-10 % of total epoch time with num_workers=0, stick with the simple path.\n",
    "Otherwise, parallel loading with share-friendly torch_sparse.SparseTensor\n",
    "almost always pays off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83fd8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────── Tar-reader dataset\n",
    "class TarGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each graph is stored under its own sub-directory *inside* one .tar:\n",
    "\n",
    "        graphs.tar\n",
    "        ├── 0001/X.npz\n",
    "        ├── 0001/E.npz\n",
    "        ├── 0001/edge_index.npy\n",
    "        ├── 0001/labels.npz\n",
    "        ├── 0001/label_index.npy\n",
    "        ├── 0001/label_value.npy\n",
    "        ├── 0002/…\n",
    "        └── …\n",
    "\n",
    "    The tar is opened once; __getitem__ streams the six files for graph *idx*\n",
    "    straight into memory, converts them to native PyTorch tensors and returns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tar_path: str | Path):\n",
    "        self.tar = tarfile.open(tar_path, mode=\"r:*\")      # gzip/none/…\n",
    "        self.index: dict[str, dict[str, tarfile.TarInfo]] = {}\n",
    "        self.sublen = {}\n",
    "\n",
    "        # Build a small lookup table in RAM  {gid: {filename: tarinfo}}\n",
    "        for member in self.tar.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "\n",
    "            p     = Path(member.name)\n",
    "            gid   = str(p.parent)   # '0007'\n",
    "            fname = p.name          # 'X.npz'\n",
    "\n",
    "            # keep only folders that really are 4-digit graph IDs\n",
    "            if gid[-4:].isdigit():\n",
    "                self.index.setdefault(gid, {})[fname] = member\n",
    "\n",
    "        self.gids = sorted(self.index)\n",
    "\n",
    "        # Remove thos with no labels\n",
    "        for gid, files in self.index.items():\n",
    "            if not files.get(\"labels.npz\"):\n",
    "                self.gids.remove(gid)\n",
    "\n",
    "        # Count\n",
    "        name, counts = np.unique([Path(gid).parent.name for gid in self.gids], return_counts=True)\n",
    "\n",
    "        # Get cumsum\n",
    "        running = 0\n",
    "        for lbl, cnt in zip(name, counts):\n",
    "            self.sublen[lbl] = (running, running + cnt)\n",
    "            running += cnt\n",
    "\n",
    "    # ------------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _npz_to_csr(buf: bytes, dtype=torch.float32):\n",
    "        csr = sparse.load_npz(io.BytesIO(buf)).tocsr()\n",
    "        crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "        col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "        val  = torch.from_numpy(csr.data).to(dtype)\n",
    "        return torch.sparse_csr_tensor(\n",
    "            crow, col, val, size=csr.shape, dtype=dtype, requires_grad=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _npy_to_tensor(buf: bytes, dtype):\n",
    "        arr = np.load(io.BytesIO(buf), allow_pickle=False)\n",
    "        return torch.from_numpy(arr).to(dtype)\n",
    "\n",
    "    def get_sublen(self, name):\n",
    "        return self.sublen[name]\n",
    "\n",
    "    # ------------- Dataset API ---------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.gids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gid   = self.gids[idx]\n",
    "        files = self.index[gid]\n",
    "\n",
    "        get = lambda name: self.tar.extractfile(files[name]).read()\n",
    "        \n",
    "        fileinfo = gid\n",
    "\n",
    "        X   = self._npz_to_csr(get(\"X.npz\"),       dtype=torch.float32)\n",
    "        Aef = self._npz_to_csr(get(\"E.npz\"),       dtype=torch.float32)\n",
    "        Lef = self._npz_to_csr(get(\"labels.npz\"),  dtype=torch.float32)\n",
    "\n",
    "        Aei = self._npy_to_tensor(get(\"edge_index.npy\"),  dtype=torch.int64)\n",
    "        Lei = self._npy_to_tensor(get(\"label_index.npy\"), dtype=torch.int64)\n",
    "        y   = self._npy_to_tensor(get(\"label_value.npy\"), dtype=torch.int64)\n",
    "\n",
    "        return fileinfo, X, Aei.t().contiguous(), Aef, Lei.t().contiguous(), Lef, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316ad981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csr(blocks):\n",
    "    \"\"\"\n",
    "    Vertically stack CSR matrices that all share the same n_cols.\n",
    "    Keeps sparsity and returns a single torch.sparse_csr_tensor.\n",
    "    \"\"\"\n",
    "    crow_bufs, col_bufs, val_bufs = [], [], []\n",
    "    nnz_so_far, n_rows, n_cols = 0, 0, blocks[0].size(1)\n",
    "\n",
    "    for k, csr in enumerate(blocks):\n",
    "        crow = csr.crow_indices().clone()          # (n_rows_k + 1,)\n",
    "\n",
    "        # 1) shift by *cumulative* nnz so far\n",
    "        crow += nnz_so_far\n",
    "\n",
    "        # 2) drop the leading 0 for every block after the first\n",
    "        if k > 0:\n",
    "            crow = crow[1:]\n",
    "\n",
    "        crow_bufs.append(crow)\n",
    "        col_bufs.append(csr.col_indices())\n",
    "        val_bufs.append(csr.values())\n",
    "\n",
    "        nnz_so_far += csr.values().numel()\n",
    "        n_rows     += csr.size(0)\n",
    "\n",
    "    crow_cat = torch.cat(crow_bufs)\n",
    "    col_cat  = torch.cat(col_bufs)\n",
    "    val_cat  = torch.cat(val_bufs)\n",
    "\n",
    "    return torch.sparse_csr_tensor(\n",
    "        crow_cat, col_cat, val_cat,\n",
    "        size=(n_rows, n_cols),\n",
    "        dtype=val_cat.dtype,\n",
    "        device=val_cat.device,\n",
    "        requires_grad=False\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_graph_collate(batch):\n",
    "    # unpack each graph\n",
    "    filenames, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "\n",
    "    # node-count prefix sum for shifting\n",
    "    node_offsets = torch.cumsum(\n",
    "        torch.tensor([0] + [x.size(0) for x in xs[:-1]]), 0)\n",
    "\n",
    "    # ----- merge node features (CSR) -----------------------------\n",
    "    X_batch = concat_csr(xs)\n",
    "\n",
    "    # ----- merge structural edges --------------------------------\n",
    "    Aei_shifted = []\n",
    "    for off, ei in zip(node_offsets, aei):\n",
    "        Aei_shifted.append(ei + off)   # shift both rows\n",
    "    Aei_batch = torch.cat(Aei_shifted, dim=1)   # (2 , E_tot)\n",
    "\n",
    "    Aef_batch = concat_csr(aef)\n",
    "\n",
    "    # ----- merge label edges -------------------------------------\n",
    "    Lei_shifted = []\n",
    "    for off, ei in zip(node_offsets, lei):\n",
    "        Lei_shifted.append(ei + off)\n",
    "    Lei_batch = torch.cat(Lei_shifted, dim=1)\n",
    "\n",
    "    Lef_batch = concat_csr(lef)\n",
    "    y_batch   = torch.cat(ys)\n",
    "\n",
    "    return filenames, X_batch, Aei_batch, Aef_batch, Lei_batch, Lef_batch, y_batch\n",
    "\n",
    "def debug_collate(batch):\n",
    "    _, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "    print(\"--- one mini-batch ---\")\n",
    "    for i, X in enumerate(xs):\n",
    "        print(f\"graph {i}:  nodes={X.size(0):4d}   \"\n",
    "              f\"struct-edges={aei[i].shape[1]:4d}   \"\n",
    "              f\"label-edges={lei[i].shape[1]:3d}\")\n",
    "    # then call the real collate to keep training code unchanged\n",
    "    return sparse_graph_collate(batch)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── loader utilities\n",
    "def identity_collate(batch):\n",
    "    \"\"\"batch == list of length 1 → return that single sample untouched.\"\"\"\n",
    "    return batch[0]\n",
    "\n",
    "def make_loader(ds, batch_size=1, shuffle=False):\n",
    "    return DataLoader(ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=sparse_graph_collate,\n",
    "                      num_workers=0,\n",
    "                      pin_memory=True)    # fast GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec604253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset   = TarGraphDataset(\"../../data/swde_HTMLgraphs.tar\")\n",
    "# loader    = make_loader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# next(iter(loader))\n",
    "\n",
    "# count = 0\n",
    "# for fileinfo, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "#     print(fileinfo)\n",
    "#     count +=1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7c7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lazy loader for individual files\n",
    "\n",
    "# class LazyGraphDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Each graph lives in its own .npz / .pt / whatever on disk.\n",
    "#     __getitem__ loads it just-in-time.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, folderpaths):\n",
    "#         \"\"\"\n",
    "#         meta_csv: a CSV (or list of dicts) with columns:\n",
    "#             path_X, path_A_index, path_A_feat, path_L_index, path_L_feat, path_y\n",
    "#         Only these tiny strings stay in RAM.\n",
    "#         \"\"\"\n",
    "#         self.folderpaths = list(folderpaths)\n",
    "\n",
    "#     def _import_tensor(self, filename: str, dtype: torch.dtype, is_sparse: bool = False):\n",
    "#         \"\"\"\n",
    "#         Load a .npz CSR matrix and return either\n",
    "#         • a torch.sparse_csr_tensor              (if is_sparse=True)\n",
    "#         • a torch.Tensor (dense)                 (otherwise)\n",
    "#         \"\"\"\n",
    "#         csr = sparse.load_npz(filename).tocsr()\n",
    "\n",
    "#         if is_sparse:\n",
    "#             crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "#             col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "#             val  = torch.from_numpy(csr.data).to(dtype)\n",
    "#             return torch.sparse_csr_tensor(crow, col, val,size=csr.shape, dtype=dtype, requires_grad=False)\n",
    "#         # — otherwise densify —\n",
    "#         return torch.from_numpy(csr.toarray()).to(dtype)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         folder_path = self.folderpaths[idx]\n",
    "\n",
    "#         X = self._import_tensor((folder_path/\"X.npz\"), torch.float32, is_sparse=False)\n",
    "#         #A = self._import_tensor(folder_path/\"A.npz\", torch.long, True)\n",
    "#         Aef = self._import_tensor((folder_path/\"E.npz\"), torch.float32, is_sparse=True)\n",
    "#         Aei = torch.from_numpy(np.load((folder_path/\"edge_index.npy\")))\n",
    "#         Lef = self._import_tensor((folder_path/\"labels.npz\"), torch.float32, is_sparse=True)\n",
    "#         Lei = torch.from_numpy(np.load((folder_path/\"label_index.npy\")))\n",
    "#         y = torch.from_numpy(np.load((folder_path/\"label_value.npy\")))\n",
    "\n",
    "#         return X, Aei, Aef, Lei, Lef, y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.folderpaths)\n",
    "\n",
    "# def graph_collate(batch):\n",
    "#     # batch is a list of tuples\n",
    "#     xs, aei, aef, lei, lef, ys = zip(*batch)   # tuples of length B\n",
    "\n",
    "#     return (list(xs),                          # list of sparse X\n",
    "#             list(aei),                         # list of edge_index\n",
    "#             list(aef),                         # list of sparse A_edge_feat\n",
    "#             list(lei),\n",
    "#             list(lef),\n",
    "#             list(ys))                          # dense y can still be list/stack\n",
    "\n",
    "# def make_loader(dataset, batch_size=1, shuffle=False):\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=graph_collate, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f02934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def walk_limited(root: Path, max_depth: int, pat: str):\n",
    "#     root_depth = len(root.parts)\n",
    "#     for dirpath, dirnames, _ in os.walk(root):\n",
    "#         depth = len(Path(dirpath).parts) - root_depth\n",
    "#         if depth > max_depth:\n",
    "#             # prune traversal\n",
    "#             dirnames[:] = []\n",
    "#             continue\n",
    "#         for d in dirnames:\n",
    "#             p = Path(dirpath, d)\n",
    "#             if p.match(pat):\n",
    "#                 yield p\n",
    "\n",
    "# src = Path(\"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphs/movie/movie\")\n",
    "# batch_dirs = list(walk_limited(src, max_depth=2, pat='[0-9][0-9][0-9][0-9]'))\n",
    "# print(src.exists())\n",
    "# batchFiles = list(src.rglob(\"[0-9][0-9][0-9][0-9]\"))\n",
    "# print(len(batchFiles))\n",
    "# dataset = LazyGraphDataset(batchFiles)\n",
    "# dataloader = make_loader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d224978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Xs, Aeis, Aefs, Leis, Lefs, ys in dataloader:\n",
    "#     print(Xs[0].shape, Aeis[0].shape, Aefs[0].shape, Leis[0].shape, Lefs[0].shape, ys[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e379c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to normalise the A matrix\n",
    "# def symmetric_normalize(A_tilde):\n",
    "#     \"\"\"\n",
    "#     Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "#       A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "#     Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "#     A_tilde (N, N): Adj. matrix with self loops\n",
    "#     Returns:\n",
    "#       A_norm : (N, N)\n",
    "#     \"\"\"\n",
    "\n",
    "#     eps = 1e-5\n",
    "#     d = A_tilde.sum(dim=1) + eps\n",
    "#     D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "#     return (D_inv @ A_tilde @ D_inv).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0048a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_live(\n",
    "    train_vals,\n",
    "    val_vals,\n",
    "    p, r, f1,                 # new metric lists (same length as train/val)\n",
    "    save_path,\n",
    "    xlabel=\"Epoch\",\n",
    "    ylabel_left=\"Loss / Accuracy\",\n",
    "    ylabel_right=\"P · R · F1\",\n",
    "    title=\"Training progress\",\n",
    "    fig_ax=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Live-updating dual-axis plot.\n",
    "    Call once per epoch with the (growing) metric lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_vals, val_vals : list[float]\n",
    "        Main metric to compare (e.g. loss or accuracy).\n",
    "    p, r, f1 : list[float]\n",
    "        Precision, recall, f1 – plotted on a secondary y-axis.\n",
    "    save_path : str or Path\n",
    "        Where to write the PNG each time.\n",
    "    fig_ax : tuple(fig, (ax_left, ax_right)) | None\n",
    "        Pass back what you got from the previous call to avoid flicker.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (fig, (ax_left, ax_right))\n",
    "        Feed this straight back into the next call.\n",
    "    \"\"\"\n",
    "    # ---------- figure / axes boilerplate ----------\n",
    "    if fig_ax is None:\n",
    "        fig, ax_left = plt.subplots(figsize=(8, 5))\n",
    "        ax_right = ax_left.twinx()\n",
    "    else:\n",
    "        fig, (ax_left, ax_right) = fig_ax\n",
    "\n",
    "    # ---------- clear and redraw ----------\n",
    "    ax_left.cla()\n",
    "    ax_right.cla()\n",
    "\n",
    "    epochs = range(1, len(train_vals) + 1)\n",
    "\n",
    "    # left-axis curves\n",
    "    ax_left.plot(epochs, train_vals, \"-o\", label=\"Train\", markersize=4)\n",
    "    ax_left.plot(epochs, val_vals,   \"-s\", label=\"Val\",   markersize=4)\n",
    "    ax_left.set_xlabel(xlabel)\n",
    "    ax_left.set_ylabel(ylabel_left)\n",
    "    ax_left.grid(True, axis=\"both\")\n",
    "\n",
    "    # right-axis curves\n",
    "    ax_right.plot(epochs, p,  \"--d\", label=\"Precision\", markersize=4)\n",
    "    ax_right.plot(epochs, r,  \"--^\", label=\"Recall\",    markersize=4)\n",
    "    ax_right.plot(epochs, f1, \"--*\", label=\"F1\",        markersize=4)\n",
    "    ax_right.set_ylabel(ylabel_right)\n",
    "\n",
    "    # one combined legend\n",
    "    lines_l, labels_l = ax_left.get_legend_handles_labels()\n",
    "    lines_r, labels_r = ax_right.get_legend_handles_labels()\n",
    "    ax_left.legend(lines_l + lines_r, labels_l + labels_r, loc=\"upper center\", ncol=5)\n",
    "\n",
    "    ax_left.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(Path(save_path), dpi=150)\n",
    "\n",
    "    return fig, (ax_left, ax_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "749c7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To advance the model, use the methods in https://arxiv.org/pdf/2311.02921\n",
    "\n",
    "class GraphAttentionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    HTML‑graph model\n",
    "\n",
    "        X  ─╮\n",
    "            │  GAT( 96 → 64 )\n",
    "            │  ReLU\n",
    "            │  GAT( 64 → 32 )\n",
    "            │  ReLU\n",
    "            └─ Edge‑feature constructor\n",
    "                      [h_i ‖ h_j ‖ φ(e_ij)] ─► MLP(69 → 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_dim          : node‑feature size   (= 96)\n",
    "    edge_in_dim     : raw edge‑feature size (= 197)\n",
    "    edge_emb_dim    : Edge-feature MLP output dims\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int        = 96,\n",
    "                 edge_in_dim: int   = 197,\n",
    "                 edge_emb_dim: int  = 8,\n",
    "                 hidden1: int       = 128,\n",
    "                 hidden2: int       = 64,\n",
    "                 hidden3: int       = 32,\n",
    "                 heads:  int        = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        # ── Node-level encoder (edge-aware) ────────────────────────────\n",
    "        self.tr1 = TransformerConv(\n",
    "            in_channels      = in_dim,\n",
    "            out_channels     = hidden1,\n",
    "            heads            = heads,\n",
    "            edge_dim         = edge_emb_dim,\n",
    "            dropout          = 0.3,\n",
    "            beta             = True         # learnable α in α·x + (1-α)·attn\n",
    "        )\n",
    "        self.tr2 = TransformerConv(\n",
    "            in_channels      = hidden1 * heads,\n",
    "            out_channels     = hidden2,\n",
    "            heads            = 1,\n",
    "            edge_dim         = edge_emb_dim,\n",
    "            dropout          = 0.3,\n",
    "            beta             = True\n",
    "        )\n",
    "        # self.tr3 = TransformerConv(\n",
    "        #     in_channels      = hidden2 * heads,\n",
    "        #     out_channels     = hidden3,\n",
    "        #     heads            = 1,\n",
    "        #     edge_dim         = edge_emb_dim,\n",
    "        #     dropout          = 0.1,\n",
    "        #     beta             = True\n",
    "        # )\n",
    "\n",
    "        # ── Edge feature projector ────────────── (It is not an explicit linear layer as it works on a sparse matrix)\n",
    "        self.AW_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "        nn.init.xavier_uniform_(self.AW_edge)\n",
    "        self.EW_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "        nn.init.xavier_uniform_(self.EW_edge)\n",
    "\n",
    "        # ── Edge-level MLP decoder (unchanged) ────────────────────────\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden2 * 2 + edge_emb_dim, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_sparse: torch.Tensor,        # (N_nodes, 96)          sparse\n",
    "        A_edge_index: torch.Tensor,   # (2, nnz_A)             COO  (from A)\n",
    "        A_edge_attr: torch.Tensor,    # (nnz_A, 197)           dense / sparse.mm\n",
    "        E_edge_index: torch.Tensor,   # (2, N_E)               candidates\n",
    "        E_edge_attr: torch.Tensor,    # (N_E, 197)             sparse features\n",
    "        E_attr_dropout=0.0,            # Probability of dropping out a whole edge_attr when training\n",
    "        E_attr_include=True,\n",
    "        A_attr_include=True\n",
    "    ):\n",
    "        # 1) node features\n",
    "        x_dense = x_sparse.to_dense()\n",
    "        A_edge_emb = torch.sparse.mm(A_edge_attr, self.AW_edge)     # (nnz_A , 8)\n",
    "        #A_edge_emb = A_edge_attr.to_dense()\n",
    "        #E_edge_emb = E_edge_attr.to_dense()\n",
    "\n",
    "        if not A_attr_include:\n",
    "            A_edge_emb = torch.zeros_like(A_edge_emb)\n",
    "\n",
    "        # 2) edge-aware GATv2 layers\n",
    "        h = F.relu( self.tr1(x_dense, A_edge_index, A_edge_emb) )\n",
    "        h = F.relu( self.tr2(h,        A_edge_index, A_edge_emb) )\n",
    "        #h = F.relu( self.tr3(h,        A_edge_index, A_edge_emb) )\n",
    "\n",
    "        # 3) candidate-edge projection  φ(E) = E @ W_edge\n",
    "        E_edge_emb = torch.sparse.mm(E_edge_attr, self.EW_edge)     # (N_E , 8)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = torch.rand(E_edge_emb.size(0), 1,\n",
    "                            device=E_edge_emb.device) > E_attr_dropout   # (N_E,1)\n",
    "            E_edge_emb = E_edge_emb * mask\n",
    "\n",
    "        if not E_attr_include:\n",
    "            E_edge_emb = torch.zeros_like(E_edge_emb)\n",
    "\n",
    "        # 4) gather node embeddings and classify\n",
    "        src, dst = E_edge_index\n",
    "        z = torch.cat([h[src], h[dst], E_edge_emb], dim=1)      # (N_E , 72)\n",
    "        return self.edge_mlp(z).squeeze(-1)                   # (N_E ,) returns the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddb1095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, targets, alpha = 0.25, gamma = 2.0, reduction: str = \"mean\"):\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    p_t = torch.exp(-bce)          # = σ(z) if y==1 else 1-σ(z)\n",
    "    loss = (alpha * (1.0 - p_t).pow(gamma) * bce)\n",
    "    return loss.mean() if reduction == \"mean\" else loss.sum()\n",
    "\n",
    "\n",
    "# ---------- 2. Pair-wise AUC (logistic ranking) loss -------------------------\n",
    "def pairwise_auc_loss(logits, targets, sample_k: int | None = None):\n",
    "    \"\"\"\n",
    "    logits   : float tensor (B,)\n",
    "    targets  : {0,1} tensor (B,)\n",
    "    sample_k : optional int – #negatives to sample per positive.  If None,\n",
    "               uses *all* positives × negatives (can be heavy for big batches).\n",
    "    \"\"\"\n",
    "    pos_logits = logits[targets == 1]      # shape (P,)\n",
    "    neg_logits = logits[targets == 0]      # shape (N,)\n",
    "\n",
    "    if pos_logits.numel() == 0 or neg_logits.numel() == 0:\n",
    "        # No valid pairs (edge cases in small batches) – return 0 so it\n",
    "        # doesn't break the graph.\n",
    "        return logits.new_tensor(0.0, requires_grad=True)\n",
    "\n",
    "    # --- optional negative subsampling to save memory ---\n",
    "    if sample_k is not None and neg_logits.numel() > sample_k:\n",
    "        idx = torch.randperm(neg_logits.numel(), device=logits.device)[:sample_k]\n",
    "        neg_logits = neg_logits[idx]\n",
    "\n",
    "    # Broadcast positives against negatives: diff = s_pos - s_neg\n",
    "    diff = pos_logits[:, None] - neg_logits[None, :]        # (P, N) or (P, k)\n",
    "    loss = F.softplus(-diff)                                # log(1+e^(-diff))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# ---------- 3. Combined wrapper ----------------------------------------------\n",
    "class PairwiseAUCFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    total_loss = pairwise_auc_loss + lambda_focal * focal_loss\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 gamma: float = 2.0,\n",
    "                 alpha: float = 0.25,\n",
    "                 lambda_focal: float = 0.5,\n",
    "                 sample_k: int | None = None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambda_focal = lambda_focal\n",
    "        self.sample_k = sample_k\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        loss_rank = pairwise_auc_loss(\n",
    "            logits, targets, sample_k=self.sample_k\n",
    "        )\n",
    "        loss_focal = focal_loss(\n",
    "            logits, targets, alpha=self.alpha, gamma=self.gamma\n",
    "        )\n",
    "        return loss_rank * (1 - self.lambda_focal) + self.lambda_focal * loss_focal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96e35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_NORM = 2.0           # gradient clipping\n",
    "\n",
    "# ---------- one epoch --------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer,\n",
    "                criterion, epoch, totalEpoch, device=\"cpu\", **kwargs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    running_loss, running_edges = 0.0, 0\n",
    "    count = 0\n",
    "    l = len(loader)\n",
    "\n",
    "    for _, X_sparse, Aei, Aef, Lei, Lef, y in loader:\n",
    "        count += 1\n",
    "        X_sparse, Aei, Aef = X_sparse.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X_sparse, Aei, Aef, Lei, Lef, kwargs[\"p_Lef_drop\"], kwargs[\"use_E_attr\"], kwargs[\"use_A_attr\"])          # (N_label,)\n",
    "        loss   = criterion(logits, y.float())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "\n",
    "        if count % 20 == 0:\n",
    "            print(f\"epoch {count}/{l} \"\n",
    "                    f\"loss={loss:.4f}\")\n",
    "\n",
    "    return running_loss / running_edges\n",
    "\n",
    "\n",
    "# ---------- evaluation -------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def eval_edge_model(model, loader, criterion, device=\"cpu\", thr_type=\"median\", **kwargs):\n",
    "    model.eval()\n",
    "    TP = FP = FN = 0\n",
    "    TP2 = FP2 = FN2 = 0\n",
    "    running_loss, running_edges = 0.0, 0\n",
    "\n",
    "    filenames = []\n",
    "    for f, X_sparse, Aei, Aef, Lei, Lef, y in loader:\n",
    "        filenames += f\n",
    "        X_sparse, Aei, Aef = X_sparse.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "\n",
    "        #Complete Model\n",
    "        logits = model(X_sparse, Aei, Aef, Lei, Lef, 0, kwargs[\"use_E_attr\"], kwargs[\"use_A_attr\"])\n",
    "        loss   = criterion(logits, y.float())\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "        probs  = torch.sigmoid(logits)\n",
    "        if thr_type==\"median\":\n",
    "            thr = torch.median(probs)\n",
    "\n",
    "        pred = (probs >= thr).long()\n",
    "        TP  += ((pred == 1) & (y == 1)).sum().item()\n",
    "        FP  += ((pred == 1) & (y == 0)).sum().item()\n",
    "        FN  += ((pred == 0) & (y == 1)).sum().item()\n",
    "\n",
    "    print(f\"Validating {np.unique([filename[:-5] for filename in filenames])} website type\")\n",
    "    \n",
    "    prec = TP / (TP + FP + 1e-9)\n",
    "    rec  = TP / (TP + FN + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return running_loss / running_edges, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67146255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                load_checkpoint,\n",
    "                num_epochs     = 100,\n",
    "                lr             = 1e-3,\n",
    "                validate_every = 10,\n",
    "                patience       = 10,\n",
    "                device         = \"cpu\"):\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    model_path = \"./model_in_training.pt\"\n",
    "    if os.path.exists(model_path) and load_checkpoint:\n",
    "        print(\"loading existing model...\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    sched = lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\",\n",
    "                                          patience=patience, factor=0.5)\n",
    "    #criterion = focal_loss\n",
    "    criterion = PairwiseAUCFocalLoss(\n",
    "                gamma=2.0,\n",
    "                alpha=0.25,\n",
    "                lambda_focal=1,  # 0 ⇒ pure ranking loss; 1 ⇒ equal weight\n",
    "                sample_k=128     # speeds up training; set None for exact loss\n",
    "            )\n",
    "\n",
    "    best_f1, fig_ax, best_state = 0.0, None, None\n",
    "    train_loss, val_loss, precision, recall, f1score = [], [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        p_Lef_drop = 0#.3 - 0.3 * (epoch-2)/(num_epochs-2 + 1e-9)        \n",
    "        use_E_attr,  use_A_attr = (epoch>2), (epoch>1)\n",
    "\n",
    "        loss = train_epoch(model, train_loader, opt, criterion, epoch, num_epochs, device=device, use_E_attr=use_E_attr, use_A_attr = use_A_attr, p_Lef_drop = p_Lef_drop)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if epoch % validate_every == 0 or epoch == num_epochs:\n",
    "            loss, p, r, f1 = eval_edge_model(model, val_loader, criterion, device=device, use_E_attr=use_E_attr, use_A_attr = use_A_attr)\n",
    "            val_loss.append(loss)\n",
    "            sched.step(f1)\n",
    "\n",
    "            lr_now = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d}/{num_epochs} \"\n",
    "                  f\"loss={loss:.4f}  P={p:.3f} R={r:.3f} F1={f1:.3f}  lr={lr_now:.2e}  E_features={use_E_attr} A_features={use_A_attr}\")\n",
    "            precision.append(p)\n",
    "            recall.append(r)\n",
    "            f1score.append(f1)\n",
    "\n",
    "            if f1 >= best_f1:\n",
    "                best_f1, best_state = f1, copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # if lr_now < 1e-5:\n",
    "            #     print(\"Stop: LR < 1e-5\")\n",
    "            #     break\n",
    "\n",
    "            fig_ax = plot_metrics_live(\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                precision,recall,f1score,\n",
    "                \"CurrentRun\",\n",
    "                xlabel=\"Epoch\",\n",
    "                ylabel_left=\"Loss\",\n",
    "                ylabel_right=\"P · R · F1\",\n",
    "                title=\"Model Performance\",\n",
    "                fig_ax=fig_ax\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return best_state, train_loss, val_loss, fig_ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfe9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphAttentionNetwork(\n",
      "  (tr1): TransformerConv(114, 128, heads=2)\n",
      "  (tr2): TransformerConv(256, 32, heads=1)\n",
      "  (edge_mlp): Sequential(\n",
      "    (0): Linear(in_features=69, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch 20/194 loss=0.0411\n"
     ]
    }
   ],
   "source": [
    "dataset = TarGraphDataset(datafile)\n",
    "N = len(dataset)\n",
    "# n_train = int(0.95 * N)\n",
    "# n_val   = N - n_train\n",
    "# train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "matchcollege_start, matchcollege_end = dataset.get_sublen('university-matchcollege(2000)')\n",
    "allmovie_start, allmovie_end = dataset.get_sublen('movie-allmovie(2000)')\n",
    "imdb_start, imdb_end = dataset.get_sublen('movie-imdb(2000)')\n",
    "usatoday_start, usatoday_end = dataset.get_sublen('nbaplayer-usatoday(436)')\n",
    "yahoo_start, yahoo_end = dataset.get_sublen('nbaplayer-yahoo(438)')\n",
    "matchcollege_idx = list(range(matchcollege_start, matchcollege_end))\n",
    "allmovie_idx = list(range(allmovie_start, allmovie_end))\n",
    "imdb_idx = list(range(imdb_start, imdb_end))\n",
    "usatoday_idx=list(range(usatoday_start, usatoday_end))\n",
    "yahoo_idx=list(range(yahoo_start, yahoo_end))\n",
    "\n",
    "val_idx = list(set(imdb_idx))#list(set(matchcollege_idx[-10:])) + list(set(allmovie_idx[-10:]))#\n",
    "train_idx = list(set(range(N)) - set(val_idx) - set(usatoday_idx) - set(yahoo_idx))#list(set(matchcollege_idx + allmovie_idx) - set(val_idx))#\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = make_loader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = make_loader(val_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "model = GraphAttentionNetwork(in_dim = 114, edge_in_dim = 200, edge_emb_dim = 16, hidden1 = 64, hidden2 = 32, hidden3 = 0, heads = 2)\n",
    "\n",
    "load_checkpoint = False\n",
    "_, trainloss, valloss, fig_ax = train_model(model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            load_checkpoint,\n",
    "            num_epochs     = 10,\n",
    "            lr             = 1e-2,\n",
    "            validate_every = 1,\n",
    "            patience       = 1,\n",
    "            device         = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aabc1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), \"FirstGrahACTUALLYLEARNED.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d59625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6022417545318604\n",
      "0.5719375610351562\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"./model_in_training.pt\"\n",
    "# if os.path.exists(model_path) and load_checkpoint:\n",
    "#     print(\"loading existing model...\")\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "for layer in (model.tr1, model.tr2):\n",
    "    print(layer.lin_beta.weight.norm().item())        # should drift away from 1.0\n",
    "\n",
    "\n",
    "#eval_edge_model(model, val_loader, focal_loss, device=\"cuda\")\n",
    "#b4 submitting to A100\n",
    "#Experiemnt with the comparison loss\n",
    "#Do self layers myself\n",
    "#\n",
    "#Graphs of also without edges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
