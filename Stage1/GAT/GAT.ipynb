{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cff77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import io, tarfile, os\n",
    "\n",
    "datafile = \"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphs.tar\"\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "SEED = 16\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562900ca",
   "metadata": {},
   "source": [
    "***BELOW***\n",
    "If data-loading < 5-10 % of total epoch time with num_workers=0, stick with the simple path.\n",
    "Otherwise, parallel loading with share-friendly torch_sparse.SparseTensor\n",
    "almost always pays off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83fd8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────── Tar-reader dataset\n",
    "class TarGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each graph is stored under its own sub-directory *inside* one .tar:\n",
    "\n",
    "        graphs.tar\n",
    "        ├── 0001/X.npz\n",
    "        ├── 0001/E.npz\n",
    "        ├── 0001/edge_index.npy\n",
    "        ├── 0001/labels.npz\n",
    "        ├── 0001/label_index.npy\n",
    "        ├── 0001/label_value.npy\n",
    "        ├── 0002/…\n",
    "        └── …\n",
    "\n",
    "    The tar is opened once; __getitem__ streams the six files for graph *idx*\n",
    "    straight into memory, converts them to native PyTorch tensors and returns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tar_path: str | Path):\n",
    "        self.tar = tarfile.open(tar_path, mode=\"r:*\")      # gzip/none/…\n",
    "        self.index: dict[str, dict[str, tarfile.TarInfo]] = {}\n",
    "        self.sublen = {}\n",
    "\n",
    "        # Build a small lookup table in RAM  {gid: {filename: tarinfo}}\n",
    "        for member in self.tar.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "\n",
    "            p     = Path(member.name)\n",
    "            gid   = str(p.parent)   # '0007'\n",
    "            fname = p.name          # 'X.npz'\n",
    "\n",
    "            # keep only folders that really are 4-digit graph IDs\n",
    "            if gid[-4:].isdigit():\n",
    "                self.index.setdefault(gid, {})[fname] = member\n",
    "\n",
    "        self.gids = sorted(self.index)\n",
    "\n",
    "        # Remove thos with no labels\n",
    "        for gid, files in self.index.items():\n",
    "            if not files.get(\"labels.npz\"):\n",
    "                self.gids.remove(gid)\n",
    "\n",
    "        # Count\n",
    "        name, counts = np.unique([Path(gid).parent.name for gid in self.gids], return_counts=True)\n",
    "\n",
    "        # Get cumsum\n",
    "        running = 0\n",
    "        for lbl, cnt in zip(name, counts):\n",
    "            self.sublen[lbl] = (running, running + cnt)\n",
    "            running += cnt\n",
    "\n",
    "    # ------------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _npz_to_csr(buf: bytes, dtype=torch.float32):\n",
    "        csr = sparse.load_npz(io.BytesIO(buf)).tocsr()\n",
    "        crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "        col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "        val  = torch.from_numpy(csr.data).to(dtype)\n",
    "        return torch.sparse_csr_tensor(\n",
    "            crow, col, val, size=csr.shape, dtype=dtype, requires_grad=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _npy_to_tensor(buf: bytes, dtype):\n",
    "        arr = np.load(io.BytesIO(buf), allow_pickle=False)\n",
    "        return torch.from_numpy(arr).to(dtype)\n",
    "\n",
    "    def get_sublen(self, name):\n",
    "        return self.sublen[name]\n",
    "\n",
    "    # ------------- Dataset API ---------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.gids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gid   = self.gids[idx]\n",
    "        files = self.index[gid]\n",
    "\n",
    "        get = lambda name: self.tar.extractfile(files[name]).read()\n",
    "        \n",
    "        fileinfo = gid\n",
    "\n",
    "        X   = self._npz_to_csr(get(\"X.npz\"),       dtype=torch.float32)\n",
    "        Aef = self._npz_to_csr(get(\"E.npz\"),       dtype=torch.float32)\n",
    "        Lef = self._npz_to_csr(get(\"labels.npz\"),  dtype=torch.float32)\n",
    "\n",
    "        Aei = self._npy_to_tensor(get(\"edge_index.npy\"),  dtype=torch.int64)\n",
    "        Lei = self._npy_to_tensor(get(\"label_index.npy\"), dtype=torch.int64)\n",
    "        y   = self._npy_to_tensor(get(\"label_value.npy\"), dtype=torch.int64)\n",
    "\n",
    "        return fileinfo, X, Aei.t().contiguous(), Aef, Lei.t().contiguous(), Lef, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316ad981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csr(blocks):\n",
    "    \"\"\"\n",
    "    Vertically stack CSR matrices that all share the same n_cols.\n",
    "    Keeps sparsity and returns a single torch.sparse_csr_tensor.\n",
    "    \"\"\"\n",
    "    crow_bufs, col_bufs, val_bufs = [], [], []\n",
    "    nnz_so_far, n_rows, n_cols = 0, 0, blocks[0].size(1)\n",
    "\n",
    "    for k, csr in enumerate(blocks):\n",
    "        crow = csr.crow_indices().clone()          # (n_rows_k + 1,)\n",
    "\n",
    "        # 1) shift by *cumulative* nnz so far\n",
    "        crow += nnz_so_far\n",
    "\n",
    "        # 2) drop the leading 0 for every block after the first\n",
    "        if k > 0:\n",
    "            crow = crow[1:]\n",
    "\n",
    "        crow_bufs.append(crow)\n",
    "        col_bufs.append(csr.col_indices())\n",
    "        val_bufs.append(csr.values())\n",
    "\n",
    "        nnz_so_far += csr.values().numel()\n",
    "        n_rows     += csr.size(0)\n",
    "\n",
    "    crow_cat = torch.cat(crow_bufs)\n",
    "    col_cat  = torch.cat(col_bufs)\n",
    "    val_cat  = torch.cat(val_bufs)\n",
    "\n",
    "    return torch.sparse_csr_tensor(\n",
    "        crow_cat, col_cat, val_cat,\n",
    "        size=(n_rows, n_cols),\n",
    "        dtype=val_cat.dtype,\n",
    "        device=val_cat.device,\n",
    "        requires_grad=False\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_graph_collate(batch):\n",
    "    # unpack each graph\n",
    "    filenames, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "\n",
    "    # node-count prefix sum for shifting\n",
    "    node_offsets = torch.cumsum(\n",
    "        torch.tensor([0] + [x.size(0) for x in xs[:-1]]), 0)\n",
    "\n",
    "    # ----- merge node features (CSR) -----------------------------\n",
    "    X_batch = concat_csr(xs)\n",
    "\n",
    "    # ----- merge structural edges --------------------------------\n",
    "    Aei_shifted = []\n",
    "    for off, ei in zip(node_offsets, aei):\n",
    "        Aei_shifted.append(ei + off)   # shift both rows\n",
    "    Aei_batch = torch.cat(Aei_shifted, dim=1)   # (2 , E_tot)\n",
    "\n",
    "    Aef_batch = concat_csr(aef)\n",
    "\n",
    "    # ----- merge label edges -------------------------------------\n",
    "    Lei_shifted = []\n",
    "    for off, ei in zip(node_offsets, lei):\n",
    "        Lei_shifted.append(ei + off)\n",
    "    Lei_batch = torch.cat(Lei_shifted, dim=1)\n",
    "\n",
    "    Lef_batch = concat_csr(lef)\n",
    "    y_batch   = torch.cat(ys)\n",
    "\n",
    "    return filenames, X_batch, Aei_batch, Aef_batch, Lei_batch, Lef_batch, y_batch\n",
    "\n",
    "def debug_collate(batch):\n",
    "    _, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "    print(\"--- one mini-batch ---\")\n",
    "    for i, X in enumerate(xs):\n",
    "        print(f\"graph {i}:  nodes={X.size(0):4d}   \"\n",
    "              f\"struct-edges={aei[i].shape[1]:4d}   \"\n",
    "              f\"label-edges={lei[i].shape[1]:3d}\")\n",
    "    # then call the real collate to keep training code unchanged\n",
    "    return sparse_graph_collate(batch)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── loader utilities\n",
    "def identity_collate(batch):\n",
    "    \"\"\"batch == list of length 1 → return that single sample untouched.\"\"\"\n",
    "    return batch[0]\n",
    "\n",
    "def make_loader(ds, batch_size=1, shuffle=False):\n",
    "    return DataLoader(ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=sparse_graph_collate,\n",
    "                      num_workers=0,\n",
    "                      pin_memory=True)    # fast GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec604253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset   = TarGraphDataset(\"../../data/swde_HTMLgraphs.tar\")\n",
    "# loader    = make_loader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# next(iter(loader))\n",
    "\n",
    "# count = 0\n",
    "# for fileinfo, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "#     print(fileinfo)\n",
    "#     count +=1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lazy loader for individual files\n",
    "\n",
    "# class LazyGraphDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Each graph lives in its own .npz / .pt / whatever on disk.\n",
    "#     __getitem__ loads it just-in-time.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, folderpaths):\n",
    "#         \"\"\"\n",
    "#         meta_csv: a CSV (or list of dicts) with columns:\n",
    "#             path_X, path_A_index, path_A_feat, path_L_index, path_L_feat, path_y\n",
    "#         Only these tiny strings stay in RAM.\n",
    "#         \"\"\"\n",
    "#         self.folderpaths = list(folderpaths)\n",
    "\n",
    "#     def _import_tensor(self, filename: str, dtype: torch.dtype, is_sparse: bool = False):\n",
    "#         \"\"\"\n",
    "#         Load a .npz CSR matrix and return either\n",
    "#         • a torch.sparse_csr_tensor              (if is_sparse=True)\n",
    "#         • a torch.Tensor (dense)                 (otherwise)\n",
    "#         \"\"\"\n",
    "#         csr = sparse.load_npz(filename).tocsr()\n",
    "\n",
    "#         if is_sparse:\n",
    "#             crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "#             col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "#             val  = torch.from_numpy(csr.data).to(dtype)\n",
    "#             return torch.sparse_csr_tensor(crow, col, val,size=csr.shape, dtype=dtype, requires_grad=False)\n",
    "#         # — otherwise densify —\n",
    "#         return torch.from_numpy(csr.toarray()).to(dtype)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         folder_path = self.folderpaths[idx]\n",
    "\n",
    "#         X = self._import_tensor((folder_path/\"X.npz\"), torch.float32, is_sparse=False)\n",
    "#         #A = self._import_tensor(folder_path/\"A.npz\", torch.long, True)\n",
    "#         Aef = self._import_tensor((folder_path/\"E.npz\"), torch.float32, is_sparse=True)\n",
    "#         Aei = torch.from_numpy(np.load((folder_path/\"edge_index.npy\")))\n",
    "#         Lef = self._import_tensor((folder_path/\"labels.npz\"), torch.float32, is_sparse=True)\n",
    "#         Lei = torch.from_numpy(np.load((folder_path/\"label_index.npy\")))\n",
    "#         y = torch.from_numpy(np.load((folder_path/\"label_value.npy\")))\n",
    "\n",
    "#         return X, Aei, Aef, Lei, Lef, y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.folderpaths)\n",
    "\n",
    "# def graph_collate(batch):\n",
    "#     # batch is a list of tuples\n",
    "#     xs, aei, aef, lei, lef, ys = zip(*batch)   # tuples of length B\n",
    "\n",
    "#     return (list(xs),                          # list of sparse X\n",
    "#             list(aei),                         # list of edge_index\n",
    "#             list(aef),                         # list of sparse A_edge_feat\n",
    "#             list(lei),\n",
    "#             list(lef),\n",
    "#             list(ys))                          # dense y can still be list/stack\n",
    "\n",
    "# def make_loader(dataset, batch_size=1, shuffle=False):\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=graph_collate, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f02934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def walk_limited(root: Path, max_depth: int, pat: str):\n",
    "#     root_depth = len(root.parts)\n",
    "#     for dirpath, dirnames, _ in os.walk(root):\n",
    "#         depth = len(Path(dirpath).parts) - root_depth\n",
    "#         if depth > max_depth:\n",
    "#             # prune traversal\n",
    "#             dirnames[:] = []\n",
    "#             continue\n",
    "#         for d in dirnames:\n",
    "#             p = Path(dirpath, d)\n",
    "#             if p.match(pat):\n",
    "#                 yield p\n",
    "\n",
    "# src = Path(\"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphs/movie/movie\")\n",
    "# batch_dirs = list(walk_limited(src, max_depth=2, pat='[0-9][0-9][0-9][0-9]'))\n",
    "# print(src.exists())\n",
    "# batchFiles = list(src.rglob(\"[0-9][0-9][0-9][0-9]\"))\n",
    "# print(len(batchFiles))\n",
    "# dataset = LazyGraphDataset(batchFiles)\n",
    "# dataloader = make_loader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d224978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Xs, Aeis, Aefs, Leis, Lefs, ys in dataloader:\n",
    "#     print(Xs[0].shape, Aeis[0].shape, Aefs[0].shape, Leis[0].shape, Lefs[0].shape, ys[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e379c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to normalise the A matrix\n",
    "# def symmetric_normalize(A_tilde):\n",
    "#     \"\"\"\n",
    "#     Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "#       A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "#     Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "#     A_tilde (N, N): Adj. matrix with self loops\n",
    "#     Returns:\n",
    "#       A_norm : (N, N)\n",
    "#     \"\"\"\n",
    "\n",
    "#     eps = 1e-5\n",
    "#     d = A_tilde.sum(dim=1) + eps\n",
    "#     D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "#     return (D_inv @ A_tilde @ D_inv).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0048a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_live(\n",
    "    train_vals,\n",
    "    val_vals,\n",
    "    save_path,\n",
    "    xlabel=\"Epoch\",\n",
    "    ylabel=\"Metric\",\n",
    "    title=\"Training & Validation\",\n",
    "    fig_ax=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Live‐updating single‐axis plot of training vs validation values.\n",
    "    Call each epoch with the growing lists `train_vals` and `val_vals`.\n",
    "    Returns (fig, ax) so you can pass them back in.\n",
    "    \"\"\"\n",
    "    # First call: create figure & axis\n",
    "    if fig_ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "    else:\n",
    "        fig, ax = fig_ax\n",
    "\n",
    "    # Clear and redraw\n",
    "    ax.cla()\n",
    "    epochs = range(1, len(train_vals) + 1)\n",
    "\n",
    "    ax.plot(epochs, train_vals, '-o', label=\"Train\", markersize=4)\n",
    "    ax.plot(epochs, val_vals,   '-s', label=\"Val\",   markersize=4)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(Path(save_path), dpi=150)\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To advance the model, use the methods in https://arxiv.org/pdf/2311.02921\n",
    "\n",
    "class GraphAttentionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    HTML‑graph model\n",
    "\n",
    "        X  ─╮\n",
    "            │  GAT( 96 → 64 )\n",
    "            │  ReLU\n",
    "            │  GAT( 64 → 32 )\n",
    "            │  ReLU\n",
    "            └─ Edge‑feature constructor\n",
    "                      [h_i ‖ h_j ‖ φ(e_ij)] ─► MLP(69 → 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_dim          : node‑feature size   (= 96)\n",
    "    edge_in_dim     : raw edge‑feature size (= 197)\n",
    "    edge_emb_dim    : Edge-feature MLP output dims\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int        = 96,\n",
    "                 edge_in_dim: int   = 197,\n",
    "                 edge_emb_dim: int  = 8,\n",
    "                 hidden1: int       = 64,\n",
    "                 hidden2: int       = 32,\n",
    "                 heads:  int        = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ── Node-level encoder (edge-aware) ────────────────────────────\n",
    "        self.gat1 = GATv2Conv(in_dim,\n",
    "                              hidden1,\n",
    "                              heads=heads,\n",
    "                              concat=True,\n",
    "                              edge_dim=edge_emb_dim,\n",
    "                              fill_value=0.0)\n",
    "\n",
    "        self.gat2 = GATv2Conv(hidden1 * heads,\n",
    "                              hidden2,\n",
    "                              heads=1,\n",
    "                              concat=True,\n",
    "                              edge_dim=edge_emb_dim,\n",
    "                              fill_value=0.0)\n",
    "\n",
    "        # ── Edge feature projector ────────────── (It is not an explicit linear layer as it works on a sparse matrix)\n",
    "        self.W_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "        nn.init.xavier_uniform_(self.W_edge)\n",
    "\n",
    "        # ── Edge-level MLP decoder (unchanged) ────────────────────────\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden2 * 2 + edge_emb_dim, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dense: torch.Tensor,        # (N_nodes, 96)          sparse\n",
    "        A_edge_index: torch.Tensor,   # (2, nnz_A)             COO  (from A)\n",
    "        A_edge_attr: torch.Tensor,    # (nnz_A, 197)           dense / sparse.mm\n",
    "        E_edge_index: torch.Tensor,   # (2, N_E)               candidates\n",
    "        E_edge_attr: torch.Tensor     # (N_E, 197)             sparse features\n",
    "    ):\n",
    "        # 1) node features\n",
    "        #x = x_sparse.to_dense()\n",
    "        A_edge_emb = torch.sparse.mm(A_edge_attr, self.W_edge)     # (nnz_A , 8)\n",
    "        \n",
    "        # 2) edge-aware GATv2 layers\n",
    "        h = F.relu(self.gat1(x_dense, A_edge_index, A_edge_emb))\n",
    "        h = F.relu(self.gat2(h, A_edge_index, A_edge_emb))   # (N_nodes , 32)\n",
    "        \n",
    "        # 3) candidate-edge projection  φ(E) = E @ W_edge\n",
    "        E_edge_emb = torch.sparse.mm(E_edge_attr, self.W_edge)     # (N_E , 8)\n",
    "        \n",
    "        # 4) gather node embeddings and classify\n",
    "        src, dst = E_edge_index\n",
    "        z = torch.cat([h[src], h[dst], E_edge_emb], dim=1)      # (N_E , 72)\n",
    "        return self.edge_mlp(z).squeeze(-1)                   # (N_E ,) returns the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96e35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEwLogits\n",
    "\n",
    "CLIP_NORM = 1.0           # gradient clipping\n",
    "\n",
    "\n",
    "# ---------- one epoch --------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer,\n",
    "                criterion=BCEwLogits, device=\"cpu\"):\n",
    "\n",
    "    model.train()\n",
    "    running_loss, running_edges = 0.0, 0\n",
    "    count = 0\n",
    "    l = len(loader)\n",
    "\n",
    "    for _, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "        count += 1\n",
    "        X, Aei, Aef = X.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X, Aei, Aef, Lei, Lef)          # (N_label,)\n",
    "        loss   = criterion(logits, y.float())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "\n",
    "        if count % 20 == 0:\n",
    "            print(f\"file {count}/{l}\"\n",
    "                    f\"loss={loss:.4f}\")\n",
    "\n",
    "    return running_loss / running_edges\n",
    "\n",
    "\n",
    "# ---------- evaluation -------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def eval_edge_model(model, loader, criterion, device=\"cpu\", thr=0.5):\n",
    "    model.eval()\n",
    "    TP = FP = FN = 0\n",
    "    running_loss, running_edges = 0.0, 0\n",
    "\n",
    "    filenames = []\n",
    "    for f, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "        filenames += f\n",
    "        X, Aei, Aef = X.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "\n",
    "        logits = model(X, Aei, Aef, Lei, Lef)\n",
    "        loss   = criterion(logits, y.float())\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "        probs  = torch.sigmoid(logits)\n",
    "\n",
    "        pred = (probs >= thr).long()\n",
    "        TP  += ((pred == 1) & (y == 1)).sum().item()\n",
    "        FP  += ((pred == 1) & (y == 0)).sum().item()\n",
    "        FN  += ((pred == 0) & (y == 1)).sum().item()\n",
    "\n",
    "    print(f\"Validating {np.unique([filename[:-5] for filename in filenames])} website type\")\n",
    "    prec = TP / (TP + FP + 1e-9)\n",
    "    rec  = TP / (TP + FN + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return running_loss / running_edges, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67146255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                load_checkpoint,\n",
    "                num_epochs     = 100,\n",
    "                lr             = 1e-3,\n",
    "                validate_every = 10,\n",
    "                patience       = 10,\n",
    "                device         = \"cpu\"):\n",
    "\n",
    "    print(\"Woo lets go\")\n",
    "\n",
    "    model_path = \"./model_in_training.pth\"\n",
    "    if os.path.exists(model_path) and load_checkpoint:\n",
    "        print(\"loading existing model...\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    sched = lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\",\n",
    "                                          patience=patience, factor=0.5)\n",
    "    criterion = BCEwLogits\n",
    "\n",
    "    best_f1, fig_ax, best_state, train_loss, val_loss = 0.0, None, None, [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        loss = train_epoch(model, train_loader, opt, criterion=criterion, device=device)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if epoch % validate_every == 0 or epoch == num_epochs:\n",
    "            loss, p, r, f1 = eval_edge_model(model, val_loader, criterion, device=device)\n",
    "            val_loss.append(loss)\n",
    "            sched.step(f1)\n",
    "\n",
    "            lr_now = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d}/{num_epochs} \"\n",
    "                  f\"loss={loss:.4f}  P={p:.3f} R={r:.3f} F1={f1:.3f}  lr={lr_now:.2e}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_state = f1, copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if lr_now < 1e-5:\n",
    "                print(\"Stop: LR < 1e-5\")\n",
    "                break\n",
    "\n",
    "            fig_ax = plot_metrics_live(\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                \"CurrentRun\",\n",
    "                xlabel=\"Epoch\",\n",
    "                ylabel=\"Loss\",\n",
    "                title=\"Model Performance\",\n",
    "                fig_ax=fig_ax\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return best_state, train_loss, val_loss, fig_ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bbfe9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woo lets go\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3157144/3203468246.py:61: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 20/100loss=0.5032\n",
      "file 40/100loss=0.4583\n",
      "file 60/100loss=0.4210\n",
      "file 80/100loss=0.3726\n",
      "file 100/100loss=0.3772\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 001/50 loss=0.4399  P=0.822 R=0.677 F1=0.742  lr=1.00e-02\n",
      "file 20/100loss=0.3211\n",
      "file 40/100loss=0.3370\n",
      "file 60/100loss=0.2746\n",
      "file 80/100loss=0.3221\n",
      "file 100/100loss=0.4084\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 002/50 loss=0.4565  P=0.882 R=0.376 F1=0.527  lr=1.00e-02\n",
      "file 20/100loss=0.2644\n",
      "file 40/100loss=0.2597\n",
      "file 60/100loss=0.2649\n",
      "file 80/100loss=0.2193\n",
      "file 100/100loss=0.2346\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 003/50 loss=0.6120  P=0.552 R=0.255 F1=0.348  lr=5.00e-03\n",
      "file 20/100loss=0.2224\n",
      "file 40/100loss=0.2011\n",
      "file 60/100loss=0.2034\n",
      "file 80/100loss=0.2007\n",
      "file 100/100loss=0.1709\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 004/50 loss=0.8503  P=0.424 R=0.170 F1=0.242  lr=5.00e-03\n",
      "file 20/100loss=0.1819\n",
      "file 40/100loss=0.3193\n",
      "file 60/100loss=0.1714\n",
      "file 80/100loss=0.1665\n",
      "file 100/100loss=0.1649\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 005/50 loss=0.8960  P=0.398 R=0.148 F1=0.215  lr=2.50e-03\n",
      "file 20/100loss=0.1613\n",
      "file 40/100loss=0.1594\n",
      "file 60/100loss=0.1530\n",
      "file 80/100loss=0.1672\n",
      "file 100/100loss=0.1519\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 006/50 loss=0.9042  P=0.425 R=0.148 F1=0.220  lr=2.50e-03\n",
      "file 20/100loss=0.1469\n",
      "file 40/100loss=0.1504\n",
      "file 60/100loss=0.1496\n",
      "file 80/100loss=0.1863\n",
      "file 100/100loss=0.1370\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 007/50 loss=0.9189  P=0.437 R=0.148 F1=0.222  lr=1.25e-03\n",
      "file 20/100loss=0.1453\n",
      "file 40/100loss=0.1492\n",
      "file 60/100loss=0.1476\n",
      "file 80/100loss=0.1310\n",
      "file 100/100loss=0.1330\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 008/50 loss=1.0135  P=0.415 R=0.133 F1=0.202  lr=1.25e-03\n",
      "file 20/100loss=0.1367\n",
      "file 40/100loss=0.1386\n",
      "file 60/100loss=0.1293\n",
      "file 80/100loss=0.1398\n",
      "file 100/100loss=0.1235\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 009/50 loss=1.0528  P=0.405 R=0.122 F1=0.187  lr=6.25e-04\n",
      "file 20/100loss=0.1189\n",
      "file 40/100loss=0.1220\n",
      "file 60/100loss=0.1276\n",
      "file 80/100loss=0.1243\n",
      "file 100/100loss=0.1163\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 010/50 loss=1.0516  P=0.399 R=0.121 F1=0.186  lr=6.25e-04\n",
      "file 20/100loss=0.1456\n",
      "file 40/100loss=0.1312\n",
      "file 60/100loss=0.1291\n",
      "file 80/100loss=0.1277\n",
      "file 100/100loss=0.1181\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 011/50 loss=1.0616  P=0.410 R=0.122 F1=0.188  lr=3.13e-04\n",
      "file 20/100loss=0.1384\n",
      "file 40/100loss=0.1254\n",
      "file 60/100loss=0.1151\n",
      "file 80/100loss=0.1223\n",
      "file 100/100loss=0.1329\n",
      "Validating ['swde_HTMLgraphs/movie/movie/movie-allmovie(2000)'] website type\n",
      "Epoch 012/50 loss=1.1047  P=0.406 R=0.120 F1=0.185  lr=3.13e-04\n",
      "file 20/100loss=0.1207\n",
      "file 40/100loss=0.1144\n",
      "file 60/100loss=0.1195\n",
      "file 80/100loss=0.1225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m val_loader = make_loader(val_ds, batch_size=\u001b[32m256\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m model = GraphAttentionNetwork(in_dim = \u001b[32m96\u001b[39m, edge_in_dim = \u001b[32m197\u001b[39m, edge_emb_dim = \u001b[32m8\u001b[39m, hidden1 = \u001b[32m64\u001b[39m, hidden2 = \u001b[32m32\u001b[39m, heads = \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m _, trainloss, valloss, fig_ax = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_every\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, load_checkpoint, num_epochs, lr, validate_every, patience, device)\u001b[39m\n\u001b[32m     25\u001b[39m best_f1, fig_ax, best_state, train_loss, val_loss = \u001b[32m0.0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, [], []\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     train_loss.append(loss)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % validate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch == num_epochs:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     16\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m     17\u001b[39m l = \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAef\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAei\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAef\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mTarGraphDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     85\u001b[39m X   = \u001b[38;5;28mself\u001b[39m._npz_to_csr(get(\u001b[33m\"\u001b[39m\u001b[33mX.npz\u001b[39m\u001b[33m\"\u001b[39m),       dtype=torch.float32)\n\u001b[32m     86\u001b[39m Aef = \u001b[38;5;28mself\u001b[39m._npz_to_csr(get(\u001b[33m\"\u001b[39m\u001b[33mE.npz\u001b[39m\u001b[33m\"\u001b[39m),       dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m Lef = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_npz_to_csr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabels.npz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m Aei = \u001b[38;5;28mself\u001b[39m._npy_to_tensor(get(\u001b[33m\"\u001b[39m\u001b[33medge_index.npy\u001b[39m\u001b[33m\"\u001b[39m),  dtype=torch.int64)\n\u001b[32m     90\u001b[39m Lei = \u001b[38;5;28mself\u001b[39m._npy_to_tensor(get(\u001b[33m\"\u001b[39m\u001b[33mlabel_index.npy\u001b[39m\u001b[33m\"\u001b[39m), dtype=torch.int64)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mTarGraphDataset._npz_to_csr\u001b[39m\u001b[34m(buf, dtype)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_npz_to_csr\u001b[39m(buf: \u001b[38;5;28mbytes\u001b[39m, dtype=torch.float32):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     csr = \u001b[43msparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.tocsr()\n\u001b[32m     58\u001b[39m     crow = torch.from_numpy(csr.indptr.astype(np.int64))\n\u001b[32m     59\u001b[39m     col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/scipy/sparse/_matrix_io.py:157\u001b[39m, in \u001b[36mload_npz\u001b[39m\u001b[34m(file)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown format \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse_format \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbsr\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m((\u001b[43mloaded\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, loaded[\u001b[33m'\u001b[39m\u001b[33mindices\u001b[39m\u001b[33m'\u001b[39m], loaded[\u001b[33m'\u001b[39m\u001b[33mindptr\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m    158\u001b[39m                shape=loaded[\u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sparse_format == \u001b[33m'\u001b[39m\u001b[33mdia\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m((loaded[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m], loaded[\u001b[33m'\u001b[39m\u001b[33moffsets\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m    161\u001b[39m                shape=loaded[\u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mjh24/IAEA-thesis/.venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:246\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip.open(key) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m         magic = \u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mMAGIC_PREFIX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m         \u001b[38;5;28mbytes\u001b[39m.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    249\u001b[39m             \u001b[38;5;66;03m# FIXME: This seems like it will copy strings around\u001b[39;00m\n\u001b[32m    250\u001b[39m             \u001b[38;5;66;03m#   more than is strictly necessary.  The zipfile\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m             \u001b[38;5;66;03m#   (or at least uncompress) the data\u001b[39;00m\n\u001b[32m    256\u001b[39m             \u001b[38;5;66;03m#   directly into the array memory.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:989\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    991\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1079\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._left <= \u001b[32m0\u001b[39m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_crc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:999\u001b[39m, in \u001b[36mZipExtFile._update_crc\u001b[39m\u001b[34m(self, newdata)\u001b[39m\n\u001b[32m    996\u001b[39m         n -= \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_crc\u001b[39m(\u001b[38;5;28mself\u001b[39m, newdata):\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Update the CRC using the given data.\u001b[39;00m\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expected_crc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1002\u001b[39m         \u001b[38;5;66;03m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[32m   1003\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = TarGraphDataset(datafile)\n",
    "N = len(dataset)\n",
    "# n_train = int(0.95 * N)\n",
    "# n_val   = N - n_train\n",
    "# train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "val_start, val_end = dataset.get_sublen('movie-allmovie(2000)')\n",
    "val_idx = list(range(val_start, val_end))  \n",
    "train_idx = list(set(range(N)) - set(val_idx))\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "\n",
    "load_checkpoint = True\n",
    "\n",
    "train_loader = make_loader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader = make_loader(val_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "model = GraphAttentionNetwork(in_dim = 96, edge_in_dim = 197, edge_emb_dim = 8, hidden1 = 64, hidden2 = 32, heads = 1)\n",
    "\n",
    "_, trainloss, valloss, fig_ax = train_model(model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            load_checkpoint,\n",
    "            num_epochs     = 50,\n",
    "            lr             = 1e-3,\n",
    "            validate_every = 1,\n",
    "            patience       = 1,\n",
    "            device         = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aabc1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), \"modelbatch50epoch.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
