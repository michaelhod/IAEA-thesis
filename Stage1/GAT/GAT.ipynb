{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import io, tarfile\n",
    "\n",
    "datafile = \"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphs.tar\"\n",
    "\n",
    "SEED = 16\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562900ca",
   "metadata": {},
   "source": [
    "***BELOW***\n",
    "If data-loading < 5-10 % of total epoch time with num_workers=0, stick with the simple path.\n",
    "Otherwise, parallel loading with share-friendly torch_sparse.SparseTensor\n",
    "almost always pays off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fd8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────── Tar-reader dataset\n",
    "class TarGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each graph is stored under its own sub-directory *inside* one .tar:\n",
    "\n",
    "        graphs.tar\n",
    "        ├── 0001/X.npz\n",
    "        ├── 0001/E.npz\n",
    "        ├── 0001/edge_index.npy\n",
    "        ├── 0001/labels.npz\n",
    "        ├── 0001/label_index.npy\n",
    "        ├── 0001/label_value.npy\n",
    "        ├── 0002/…\n",
    "        └── …\n",
    "\n",
    "    The tar is opened once; __getitem__ streams the six files for graph *idx*\n",
    "    straight into memory, converts them to native PyTorch tensors and returns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tar_path: str | Path):\n",
    "        self.tar = tarfile.open(tar_path, mode=\"r:*\")      # gzip/none/…\n",
    "        self.index: dict[str, dict[str, tarfile.TarInfo]] = {}\n",
    "\n",
    "        # Build a small lookup table in RAM  {gid: {filename: tarinfo}}\n",
    "        for member in self.tar.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "\n",
    "            p     = Path(member.name)\n",
    "            gid   = str(p.parent)   # '0007'\n",
    "            fname = p.name          # 'X.npz'\n",
    "\n",
    "            # keep only folders that really are 4-digit graph IDs\n",
    "            if gid[-4:].isdigit():\n",
    "                self.index.setdefault(gid, {})[fname] = member\n",
    "\n",
    "\n",
    "        self.gids = sorted(self.index)\n",
    "\n",
    "        # Remove thos with no labels\n",
    "        for gid, files in self.index.items():\n",
    "            if not files.get(\"labels.npz\"):\n",
    "                self.gids.remove(gid)\n",
    "\n",
    "    # ------------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _npz_to_csr(buf: bytes, dtype=torch.float32):\n",
    "        csr = sparse.load_npz(io.BytesIO(buf)).tocsr()\n",
    "        crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "        col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "        val  = torch.from_numpy(csr.data).to(dtype)\n",
    "        return torch.sparse_csr_tensor(\n",
    "            crow, col, val, size=csr.shape, dtype=dtype, requires_grad=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _npy_to_tensor(buf: bytes, dtype):\n",
    "        arr = np.load(io.BytesIO(buf), allow_pickle=False)\n",
    "        return torch.from_numpy(arr).to(dtype)\n",
    "\n",
    "    # ------------- Dataset API ---------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.gids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gid   = self.gids[idx]\n",
    "        files = self.index[gid]\n",
    "\n",
    "        get = lambda name: self.tar.extractfile(files[name]).read()\n",
    "        \n",
    "        fileinfo = gid\n",
    "\n",
    "        X   = self._npz_to_csr(get(\"X.npz\"),       dtype=torch.float32)\n",
    "        Aef = self._npz_to_csr(get(\"E.npz\"),       dtype=torch.float32)\n",
    "        Lef = self._npz_to_csr(get(\"labels.npz\"),  dtype=torch.float32)\n",
    "\n",
    "        Aei = self._npy_to_tensor(get(\"edge_index.npy\"),  dtype=torch.int64)\n",
    "        Lei = self._npy_to_tensor(get(\"label_index.npy\"), dtype=torch.int64)\n",
    "        y   = self._npy_to_tensor(get(\"label_value.npy\"), dtype=torch.int64)\n",
    "\n",
    "        return fileinfo, X, Aei.t().contiguous(), Aef, Lei.t().contiguous(), Lef, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "316ad981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csr(blocks):\n",
    "    \"\"\"\n",
    "    Vertically stack CSR matrices that all share the same n_cols.\n",
    "    Keeps sparsity and returns a single torch.sparse_csr_tensor.\n",
    "    \"\"\"\n",
    "    crow_bufs, col_bufs, val_bufs = [], [], []\n",
    "    nnz_so_far, n_rows, n_cols = 0, 0, blocks[0].size(1)\n",
    "\n",
    "    for k, csr in enumerate(blocks):\n",
    "        crow = csr.crow_indices().clone()          # (n_rows_k + 1,)\n",
    "\n",
    "        # 1) shift by *cumulative* nnz so far\n",
    "        crow += nnz_so_far\n",
    "\n",
    "        # 2) drop the leading 0 for every block after the first\n",
    "        if k > 0:\n",
    "            crow = crow[1:]\n",
    "\n",
    "        crow_bufs.append(crow)\n",
    "        col_bufs.append(csr.col_indices())\n",
    "        val_bufs.append(csr.values())\n",
    "\n",
    "        nnz_so_far += csr.values().numel()\n",
    "        n_rows     += csr.size(0)\n",
    "\n",
    "    crow_cat = torch.cat(crow_bufs)\n",
    "    col_cat  = torch.cat(col_bufs)\n",
    "    val_cat  = torch.cat(val_bufs)\n",
    "\n",
    "    return torch.sparse_csr_tensor(\n",
    "        crow_cat, col_cat, val_cat,\n",
    "        size=(n_rows, n_cols),\n",
    "        dtype=val_cat.dtype,\n",
    "        device=val_cat.device,\n",
    "        requires_grad=False\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_graph_collate(batch):\n",
    "    # unpack each graph\n",
    "    _, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "\n",
    "    # node-count prefix sum for shifting\n",
    "    node_offsets = torch.cumsum(\n",
    "        torch.tensor([0] + [x.size(0) for x in xs[:-1]]), 0)\n",
    "\n",
    "    # ----- merge node features (CSR) -----------------------------\n",
    "    X_batch = concat_csr(xs)\n",
    "\n",
    "    # ----- merge structural edges --------------------------------\n",
    "    Aei_shifted = []\n",
    "    for off, ei in zip(node_offsets, aei):\n",
    "        Aei_shifted.append(ei + off)   # shift both rows\n",
    "    Aei_batch = torch.cat(Aei_shifted, dim=1)   # (2 , E_tot)\n",
    "\n",
    "    Aef_batch = concat_csr(aef)\n",
    "\n",
    "    # ----- merge label edges -------------------------------------\n",
    "    Lei_shifted = []\n",
    "    for off, ei in zip(node_offsets, lei):\n",
    "        Lei_shifted.append(ei + off)\n",
    "    Lei_batch = torch.cat(Lei_shifted, dim=1)\n",
    "\n",
    "    Lef_batch = concat_csr(lef)\n",
    "    y_batch   = torch.cat(ys)\n",
    "\n",
    "    return X_batch, Aei_batch, Aef_batch, Lei_batch, Lef_batch, y_batch\n",
    "\n",
    "def debug_collate(batch):\n",
    "    _, xs, aei, aef, lei, lef, ys = zip(*batch)\n",
    "    print(\"--- one mini-batch ---\")\n",
    "    for i, X in enumerate(xs):\n",
    "        print(f\"graph {i}:  nodes={X.size(0):4d}   \"\n",
    "              f\"struct-edges={aei[i].shape[1]:4d}   \"\n",
    "              f\"label-edges={lei[i].shape[1]:3d}\")\n",
    "    # then call the real collate to keep training code unchanged\n",
    "    return sparse_graph_collate(batch)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── loader utilities\n",
    "def identity_collate(batch):\n",
    "    \"\"\"batch == list of length 1 → return that single sample untouched.\"\"\"\n",
    "    return batch[0]\n",
    "\n",
    "def make_loader(ds, batch_size=1, shuffle=False):\n",
    "    return DataLoader(ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=debug_collate,\n",
    "                      num_workers=0,      # ← stays serial\n",
    "                      pin_memory=True)    # fast GPU transfer if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec604253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- one mini-batch ---\n",
      "graph 0:  nodes= 621   struct-edges=2758   label-edges=360\n",
      "graph 1:  nodes= 635   struct-edges=2832   label-edges=420\n",
      "graph 2:  nodes= 616   struct-edges=2672   label-edges=312\n",
      "graph 3:  nodes= 648   struct-edges=2822   label-edges=390\n",
      "graph 4:  nodes= 612   struct-edges=2672   label-edges=342\n",
      "graph 5:  nodes= 767   struct-edges=4760   label-edges=570\n",
      "graph 6:  nodes= 672   struct-edges=3058   label-edges=396\n",
      "graph 7:  nodes= 682   struct-edges=2982   label-edges=372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(crow_indices=tensor([    0,     2,     4,  ..., 10502, 10504, 10506]),\n",
       "        col_indices=tensor([37, 95,  9,  ..., 95,  0, 95]),\n",
       "        values=tensor([1., 1., 1.,  ..., 1., 1., 1.]), size=(5253, 96),\n",
       "        nnz=10506, layout=torch.sparse_csr),\n",
       " tensor([[   0,    1,    1,  ..., 5250, 5251, 5252],\n",
       "         [   1,    0,    2,  ..., 5220, 5223, 5224]]),\n",
       " tensor(crow_indices=tensor([     0,      9,     18,  ..., 165000, 165007,\n",
       "                             165014]),\n",
       "        col_indices=tensor([ 37,  95, 105,  ..., 192, 195, 196]),\n",
       "        values=tensor([ 1.0000,  1.0000,  1.0000,  ...,  1.0000, 73.3438,\n",
       "                        1.0000]), size=(24556, 197), nnz=165014,\n",
       "        layout=torch.sparse_csr),\n",
       " tensor([[ 546,  509,  516,  ..., 5233, 4848, 4935],\n",
       "         [ 509,  546,  548,  ..., 5231, 4935, 4848]]),\n",
       " tensor(crow_indices=tensor([    0,     7,    14,  ..., 25214, 25223, 25232]),\n",
       "        col_indices=tensor([ 69,  95, 154,  ..., 194, 195, 196]),\n",
       "        values=tensor([   1.0000,    1.0000,    1.0000,  ..., -406.0000,\n",
       "                        228.1250,   15.0000]), size=(3162, 197), nnz=25232,\n",
       "        layout=torch.sparse_csr),\n",
       " tensor([1, 1, 1,  ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset   = TarGraphDataset(\"../../data/swde_HTMLgraphs.tar\")\n",
    "loader    = make_loader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "next(iter(loader))\n",
    "\n",
    "# count = 0\n",
    "# for fileinfo, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "#     print(fileinfo)\n",
    "#     count +=1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lazy loader for individual files\n",
    "\n",
    "# class LazyGraphDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Each graph lives in its own .npz / .pt / whatever on disk.\n",
    "#     __getitem__ loads it just-in-time.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, folderpaths):\n",
    "#         \"\"\"\n",
    "#         meta_csv: a CSV (or list of dicts) with columns:\n",
    "#             path_X, path_A_index, path_A_feat, path_L_index, path_L_feat, path_y\n",
    "#         Only these tiny strings stay in RAM.\n",
    "#         \"\"\"\n",
    "#         self.folderpaths = list(folderpaths)\n",
    "\n",
    "#     def _import_tensor(self, filename: str, dtype: torch.dtype, is_sparse: bool = False):\n",
    "#         \"\"\"\n",
    "#         Load a .npz CSR matrix and return either\n",
    "#         • a torch.sparse_csr_tensor              (if is_sparse=True)\n",
    "#         • a torch.Tensor (dense)                 (otherwise)\n",
    "#         \"\"\"\n",
    "#         csr = sparse.load_npz(filename).tocsr()\n",
    "\n",
    "#         if is_sparse:\n",
    "#             crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "#             col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "#             val  = torch.from_numpy(csr.data).to(dtype)\n",
    "#             return torch.sparse_csr_tensor(crow, col, val,size=csr.shape, dtype=dtype, requires_grad=False)\n",
    "#         # — otherwise densify —\n",
    "#         return torch.from_numpy(csr.toarray()).to(dtype)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         folder_path = self.folderpaths[idx]\n",
    "\n",
    "#         X = self._import_tensor((folder_path/\"X.npz\"), torch.float32, is_sparse=False)\n",
    "#         #A = self._import_tensor(folder_path/\"A.npz\", torch.long, True)\n",
    "#         Aef = self._import_tensor((folder_path/\"E.npz\"), torch.float32, is_sparse=True)\n",
    "#         Aei = torch.from_numpy(np.load((folder_path/\"edge_index.npy\")))\n",
    "#         Lef = self._import_tensor((folder_path/\"labels.npz\"), torch.float32, is_sparse=True)\n",
    "#         Lei = torch.from_numpy(np.load((folder_path/\"label_index.npy\")))\n",
    "#         y = torch.from_numpy(np.load((folder_path/\"label_value.npy\")))\n",
    "\n",
    "#         return X, Aei, Aef, Lei, Lef, y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.folderpaths)\n",
    "\n",
    "# def graph_collate(batch):\n",
    "#     # batch is a list of tuples\n",
    "#     xs, aei, aef, lei, lef, ys = zip(*batch)   # tuples of length B\n",
    "\n",
    "#     return (list(xs),                          # list of sparse X\n",
    "#             list(aei),                         # list of edge_index\n",
    "#             list(aef),                         # list of sparse A_edge_feat\n",
    "#             list(lei),\n",
    "#             list(lef),\n",
    "#             list(ys))                          # dense y can still be list/stack\n",
    "\n",
    "# def make_loader(dataset, batch_size=1, shuffle=False):\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=graph_collate, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f02934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def walk_limited(root: Path, max_depth: int, pat: str):\n",
    "#     root_depth = len(root.parts)\n",
    "#     for dirpath, dirnames, _ in os.walk(root):\n",
    "#         depth = len(Path(dirpath).parts) - root_depth\n",
    "#         if depth > max_depth:\n",
    "#             # prune traversal\n",
    "#             dirnames[:] = []\n",
    "#             continue\n",
    "#         for d in dirnames:\n",
    "#             p = Path(dirpath, d)\n",
    "#             if p.match(pat):\n",
    "#                 yield p\n",
    "\n",
    "# src = Path(\"/vol/bitbucket/mjh24/IAEA-thesis/data/swde_HTMLgraphs/movie/movie\")\n",
    "# batch_dirs = list(walk_limited(src, max_depth=2, pat='[0-9][0-9][0-9][0-9]'))\n",
    "# print(src.exists())\n",
    "# batchFiles = list(src.rglob(\"[0-9][0-9][0-9][0-9]\"))\n",
    "# print(len(batchFiles))\n",
    "# dataset = LazyGraphDataset(batchFiles)\n",
    "# dataloader = make_loader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d224978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Xs, Aeis, Aefs, Leis, Lefs, ys in dataloader:\n",
    "#     print(Xs[0].shape, Aeis[0].shape, Aefs[0].shape, Leis[0].shape, Lefs[0].shape, ys[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e379c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to normalise the A matrix\n",
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return (D_inv @ A_tilde @ D_inv).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749c7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To advance the model, use the methods in https://arxiv.org/pdf/2311.02921\n",
    "\n",
    "class GraphAttentionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    HTML‑graph model\n",
    "\n",
    "        X  ─╮\n",
    "            │  GAT( 96 → 64 )\n",
    "            │  ReLU\n",
    "            │  GAT( 64 → 32 )\n",
    "            │  ReLU\n",
    "            └─ Edge‑feature constructor\n",
    "                      [h_i ‖ h_j ‖ φ(e_ij)] ─► MLP(69 → 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_dim          : node‑feature size   (= 96)\n",
    "    edge_in_dim     : raw edge‑feature size (= 197)\n",
    "    edge_emb_dim    : Edge-feature MLP output dims\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int        = 96,\n",
    "                 edge_in_dim: int   = 197,\n",
    "                 edge_emb_dim: int  = 8,\n",
    "                 hidden1: int       = 64,\n",
    "                 hidden2: int       = 32,\n",
    "                 heads:  int        = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ── Node-level encoder (edge-aware) ────────────────────────────\n",
    "        self.gat1 = GATv2Conv(in_dim,\n",
    "                              hidden1,\n",
    "                              heads=heads,\n",
    "                              concat=True,\n",
    "                              edge_dim=edge_emb_dim,\n",
    "                              fill_value=0.0)\n",
    "\n",
    "        self.gat2 = GATv2Conv(hidden1 * heads,\n",
    "                              hidden2,\n",
    "                              heads=1,\n",
    "                              concat=True,\n",
    "                              edge_dim=edge_emb_dim,\n",
    "                              fill_value=0.0)\n",
    "\n",
    "        # ── Edge feature projector ────────────── (It is not an explicit linear layer as it works on a sparse matrix)\n",
    "        self.W_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "        nn.init.xavier_uniform_(self.W_edge)\n",
    "\n",
    "        # ── Edge-level MLP decoder (unchanged) ────────────────────────\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden2 * 2 + edge_emb_dim, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dense: torch.Tensor,        # (N_nodes, 96)          sparse\n",
    "        A_edge_index: torch.Tensor,   # (2, nnz_A)             COO  (from A)\n",
    "        A_edge_attr: torch.Tensor,    # (nnz_A, 197)           dense / sparse.mm\n",
    "        E_edge_index: torch.Tensor,   # (2, N_E)               candidates\n",
    "        E_edge_attr: torch.Tensor     # (N_E, 197)             sparse features\n",
    "    ):\n",
    "        # 1) node features\n",
    "        #x = x_sparse.to_dense()\n",
    "        A_edge_emb = torch.sparse.mm(A_edge_attr, self.W_edge)     # (nnz_A , 8)\n",
    "        \n",
    "        # 2) edge-aware GATv2 layers\n",
    "        h = F.relu(self.gat1(x_dense, A_edge_index, A_edge_emb))\n",
    "        h = F.relu(self.gat2(h, A_edge_index, A_edge_emb))   # (N_nodes , 32)\n",
    "        \n",
    "        # 3) candidate-edge projection  φ(E) = E @ W_edge\n",
    "        E_edge_emb = torch.sparse.mm(E_edge_attr, self.W_edge)     # (N_E , 8)\n",
    "        \n",
    "        # 4) gather node embeddings and classify\n",
    "        src, dst = E_edge_index\n",
    "        z = torch.cat([h[src], h[dst], E_edge_emb], dim=1)      # (N_E , 72)\n",
    "        return self.edge_mlp(z).squeeze(-1)                   # (N_E ,) returns the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEwLogits\n",
    "\n",
    "CLIP_NORM = 1.0           # gradient clipping\n",
    "\n",
    "\n",
    "# ---------- one epoch --------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer,\n",
    "                criterion=BCEwLogits, device=\"cpu\"):\n",
    "\n",
    "    model.train()\n",
    "    running_loss, running_edges = 0.0, 0\n",
    "    count = 0\n",
    "    l = len(loader)\n",
    "\n",
    "    for _, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "        count += 1\n",
    "        X, Aei, Aef = X.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X, Aei, Aef, Lei, Lef)          # (N_label,)\n",
    "        loss   = criterion(logits, y.float())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss  += loss.item() * y.numel()\n",
    "        running_edges += y.numel()\n",
    "\n",
    "        if count % 200 == 0:\n",
    "            print(f\"file {count}/{l}\"\n",
    "                    f\"loss={loss:.4f}\")\n",
    "\n",
    "    return running_loss / running_edges\n",
    "\n",
    "\n",
    "# ---------- evaluation -------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def eval_edge_model(model, loader, device=\"cpu\", thr=0.5):\n",
    "    model.eval()\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    for _, X, Aei, Aef, Lei, Lef, y in loader:\n",
    "        X, Aei, Aef = X.to(device), Aei.to(device), Aef.to(device)\n",
    "        Lei, Lef, y = Lei.to(device), Lef.to(device), y.to(device)\n",
    "\n",
    "        logits = model(X, Aei, Aef, Lei, Lef)\n",
    "        probs  = torch.sigmoid(logits)\n",
    "\n",
    "        pred = (probs >= thr).long()\n",
    "        TP  += ((pred == 1) & (y == 1)).sum().item()\n",
    "        FP  += ((pred == 1) & (y == 0)).sum().item()\n",
    "        FN  += ((pred == 0) & (y == 1)).sum().item()\n",
    "\n",
    "    prec = TP / (TP + FP + 1e-9)\n",
    "    rec  = TP / (TP + FN + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67146255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                num_epochs     = 100,\n",
    "                lr             = 1e-3,\n",
    "                validate_every = 10,\n",
    "                patience       = 10,\n",
    "                device         = \"cpu\"):\n",
    "\n",
    "    print(\"Woo lets go\")\n",
    "    model.to(device)\n",
    "    \n",
    "    opt   = optim.AdamW(model.parameters(), lr=lr)\n",
    "    sched = lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\",\n",
    "                                          patience=patience, factor=0.5)\n",
    "\n",
    "    best_f1, best_state, hist = 0.0, None, []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        loss = train_epoch(model, train_loader, opt, device=device)\n",
    "        hist.append(loss)\n",
    "\n",
    "        if epoch % validate_every == 0 or epoch == num_epochs:\n",
    "            p, r, f1 = eval_edge_model(model, val_loader, device=device)\n",
    "            sched.step(f1)\n",
    "\n",
    "            lr_now = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d}/{num_epochs} \"\n",
    "                  f\"loss={loss:.4f}  P={p:.3f} R={r:.3f} F1={f1:.3f}  lr={lr_now:.2e}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_state = f1, copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if lr_now < 1e-7:\n",
    "                print(\"Stop: LR < 1e-7\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return hist, best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bbfe9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woo lets go\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2716432/4124973121.py:52: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 200/22000loss=0.4453\n",
      "file 400/22000loss=0.5902\n",
      "file 600/22000loss=0.5678\n",
      "file 800/22000loss=0.5438\n",
      "file 1000/22000loss=0.4507\n",
      "file 1200/22000loss=0.5518\n",
      "file 1400/22000loss=0.4783\n",
      "file 1600/22000loss=0.4861\n",
      "file 1800/22000loss=0.5610\n",
      "file 2000/22000loss=0.5755\n",
      "file 2200/22000loss=1.4501\n",
      "file 2400/22000loss=0.8033\n",
      "file 2600/22000loss=0.4729\n",
      "file 2800/22000loss=0.7555\n",
      "file 3000/22000loss=0.4974\n",
      "file 3200/22000loss=0.5076\n",
      "file 3400/22000loss=0.5104\n",
      "file 3600/22000loss=0.2514\n",
      "file 3800/22000loss=0.4917\n",
      "file 4000/22000loss=0.4570\n",
      "file 4200/22000loss=0.4262\n",
      "file 4400/22000loss=0.7084\n",
      "file 4600/22000loss=0.6611\n",
      "file 4800/22000loss=0.5019\n",
      "file 5000/22000loss=0.4323\n",
      "file 5200/22000loss=0.4366\n",
      "file 5400/22000loss=1.7150\n",
      "file 5600/22000loss=0.6222\n",
      "file 5800/22000loss=0.4248\n",
      "file 6000/22000loss=0.6691\n",
      "file 6200/22000loss=0.4138\n",
      "file 6400/22000loss=0.3800\n",
      "file 6600/22000loss=0.4186\n",
      "file 6800/22000loss=1.9058\n",
      "file 7000/22000loss=0.6892\n",
      "file 7200/22000loss=0.8430\n",
      "file 7400/22000loss=0.3226\n",
      "file 7600/22000loss=0.2897\n",
      "file 7800/22000loss=1.1040\n",
      "file 8000/22000loss=0.1356\n",
      "file 8200/22000loss=0.2262\n",
      "file 8400/22000loss=0.7491\n",
      "file 8600/22000loss=0.3572\n",
      "file 8800/22000loss=0.2967\n",
      "file 9000/22000loss=0.1133\n",
      "file 9200/22000loss=0.4752\n",
      "file 9400/22000loss=1.0223\n",
      "file 9600/22000loss=0.3008\n",
      "file 9800/22000loss=0.2891\n",
      "file 10000/22000loss=0.2688\n",
      "file 10200/22000loss=0.3592\n",
      "file 10400/22000loss=0.1699\n",
      "file 10600/22000loss=0.3496\n",
      "file 10800/22000loss=0.4257\n",
      "file 11000/22000loss=0.4512\n",
      "file 11200/22000loss=0.3150\n",
      "file 11400/22000loss=0.2157\n",
      "file 11600/22000loss=2.6345\n",
      "file 11800/22000loss=0.2638\n",
      "file 12000/22000loss=0.3662\n",
      "file 12200/22000loss=0.1605\n",
      "file 12400/22000loss=0.2348\n",
      "file 12600/22000loss=0.3809\n",
      "file 12800/22000loss=0.2364\n",
      "file 13000/22000loss=0.1496\n",
      "file 13200/22000loss=0.1944\n",
      "file 13400/22000loss=0.3600\n",
      "file 13600/22000loss=0.5475\n",
      "file 13800/22000loss=0.5274\n",
      "file 14000/22000loss=0.2686\n",
      "file 14200/22000loss=0.0299\n",
      "file 14400/22000loss=0.4836\n",
      "file 14600/22000loss=0.0627\n",
      "file 14800/22000loss=0.1548\n",
      "file 15000/22000loss=0.4997\n",
      "file 15200/22000loss=0.2187\n",
      "file 15400/22000loss=0.1968\n",
      "file 15600/22000loss=0.3220\n",
      "file 15800/22000loss=0.2141\n",
      "file 16000/22000loss=0.3268\n",
      "file 16200/22000loss=0.4328\n",
      "file 16400/22000loss=0.3220\n",
      "file 16600/22000loss=0.3928\n",
      "file 16800/22000loss=0.6407\n",
      "file 17000/22000loss=0.1759\n",
      "file 17200/22000loss=0.1851\n",
      "file 17400/22000loss=0.0309\n",
      "file 17600/22000loss=0.0368\n",
      "file 17800/22000loss=0.1396\n",
      "file 18000/22000loss=0.1985\n",
      "file 18200/22000loss=0.1540\n",
      "file 18400/22000loss=0.2655\n",
      "file 18600/22000loss=0.6559\n",
      "file 18800/22000loss=0.1297\n",
      "file 19000/22000loss=0.3375\n",
      "file 19200/22000loss=0.2607\n",
      "file 19400/22000loss=0.2084\n",
      "file 19600/22000loss=0.2077\n",
      "file 19800/22000loss=0.6132\n",
      "file 20000/22000loss=0.4084\n",
      "file 20200/22000loss=0.4007\n",
      "file 20400/22000loss=0.0173\n",
      "file 20600/22000loss=0.5404\n",
      "file 20800/22000loss=0.4427\n",
      "file 21000/22000loss=0.5681\n",
      "file 21200/22000loss=0.2375\n",
      "file 21400/22000loss=0.3943\n",
      "file 21600/22000loss=0.1573\n",
      "file 21800/22000loss=0.1587\n",
      "file 22000/22000loss=0.0130\n",
      "Epoch 001/10 loss=0.5572  P=0.939 R=0.258 F1=0.405  lr=1.00e-04\n",
      "file 200/22000loss=0.1528\n",
      "file 400/22000loss=0.5509\n",
      "file 600/22000loss=0.1353\n",
      "file 800/22000loss=0.0111\n",
      "file 1000/22000loss=0.1663\n",
      "file 1200/22000loss=0.1685\n",
      "file 1400/22000loss=0.1444\n",
      "file 1600/22000loss=0.2961\n",
      "file 1800/22000loss=0.1128\n",
      "file 2000/22000loss=0.1318\n",
      "file 2200/22000loss=0.7939\n",
      "file 2400/22000loss=0.1616\n",
      "file 2600/22000loss=0.0891\n",
      "file 2800/22000loss=0.3339\n",
      "file 3000/22000loss=0.0136\n",
      "file 3200/22000loss=0.3488\n",
      "file 3400/22000loss=0.1451\n",
      "file 3600/22000loss=0.1389\n",
      "file 3800/22000loss=0.4708\n",
      "file 4000/22000loss=0.1098\n",
      "file 4200/22000loss=0.2619\n",
      "file 4400/22000loss=0.2253\n",
      "file 4600/22000loss=0.5259\n",
      "file 4800/22000loss=0.3152\n",
      "file 5000/22000loss=0.1798\n",
      "file 5200/22000loss=0.2028\n",
      "file 5400/22000loss=0.1019\n",
      "file 5600/22000loss=0.1471\n",
      "file 5800/22000loss=0.1015\n",
      "file 6000/22000loss=0.0326\n",
      "file 6200/22000loss=0.0472\n",
      "file 6400/22000loss=0.1343\n",
      "file 6600/22000loss=0.3253\n",
      "file 6800/22000loss=0.0600\n",
      "file 7000/22000loss=0.1302\n",
      "file 7200/22000loss=0.2537\n",
      "file 7400/22000loss=0.2831\n",
      "file 7600/22000loss=0.1403\n",
      "file 7800/22000loss=0.1857\n",
      "file 8000/22000loss=0.0800\n",
      "file 8200/22000loss=0.0924\n",
      "file 8400/22000loss=0.2963\n",
      "file 8600/22000loss=0.1493\n",
      "file 8800/22000loss=0.1237\n",
      "file 9000/22000loss=0.2886\n",
      "file 9200/22000loss=0.1699\n",
      "file 9400/22000loss=0.8164\n",
      "file 9600/22000loss=0.2073\n",
      "file 9800/22000loss=0.0900\n",
      "file 10000/22000loss=0.1074\n",
      "file 10200/22000loss=0.1629\n",
      "file 10400/22000loss=0.5257\n",
      "file 10600/22000loss=0.1244\n",
      "file 10800/22000loss=0.0633\n",
      "file 11000/22000loss=0.1027\n",
      "file 11200/22000loss=0.2971\n",
      "file 11400/22000loss=0.0689\n",
      "file 11600/22000loss=0.1359\n",
      "file 11800/22000loss=0.0896\n",
      "file 12000/22000loss=0.0661\n",
      "file 12200/22000loss=0.2141\n",
      "file 12400/22000loss=0.0735\n",
      "file 12600/22000loss=1.2733\n",
      "file 12800/22000loss=0.0886\n",
      "file 13000/22000loss=3.0811\n",
      "file 13200/22000loss=0.6230\n",
      "file 13400/22000loss=0.1621\n",
      "file 13600/22000loss=0.1431\n",
      "file 13800/22000loss=0.1059\n",
      "file 14000/22000loss=0.0990\n",
      "file 14200/22000loss=0.2808\n",
      "file 14400/22000loss=0.1346\n",
      "file 14600/22000loss=0.0896\n",
      "file 14800/22000loss=0.0604\n",
      "file 15000/22000loss=0.0759\n",
      "file 15200/22000loss=0.1729\n",
      "file 15400/22000loss=0.3850\n",
      "file 15600/22000loss=0.1997\n",
      "file 15800/22000loss=0.8323\n",
      "file 16000/22000loss=0.1482\n",
      "file 16200/22000loss=0.1829\n",
      "file 16400/22000loss=0.2785\n",
      "file 16600/22000loss=0.1321\n",
      "file 16800/22000loss=0.1563\n",
      "file 17000/22000loss=0.0743\n",
      "file 17200/22000loss=0.3321\n",
      "file 17400/22000loss=0.5031\n",
      "file 17600/22000loss=0.7572\n",
      "file 17800/22000loss=0.1698\n",
      "file 18000/22000loss=2.6912\n",
      "file 18200/22000loss=1.7824\n",
      "file 18400/22000loss=0.0306\n",
      "file 18600/22000loss=0.1433\n",
      "file 18800/22000loss=0.3176\n",
      "file 19000/22000loss=0.2786\n",
      "file 19200/22000loss=0.2221\n",
      "file 19400/22000loss=0.3056\n",
      "file 19600/22000loss=0.0981\n",
      "file 19800/22000loss=0.7595\n",
      "file 20000/22000loss=0.1177\n",
      "file 20200/22000loss=0.1637\n",
      "file 20400/22000loss=1.9928\n",
      "file 20600/22000loss=0.5312\n",
      "file 20800/22000loss=1.1775\n",
      "file 21000/22000loss=0.1153\n",
      "file 21200/22000loss=0.3826\n",
      "file 21400/22000loss=0.0134\n",
      "file 21600/22000loss=0.0680\n",
      "file 21800/22000loss=2.2432\n",
      "file 22000/22000loss=0.2027\n",
      "Epoch 002/10 loss=0.3448  P=0.952 R=0.276 F1=0.428  lr=1.00e-04\n",
      "file 200/22000loss=1.4912\n",
      "file 400/22000loss=0.2840\n",
      "file 600/22000loss=0.2963\n",
      "file 800/22000loss=0.1313\n",
      "file 1000/22000loss=0.0745\n",
      "file 1200/22000loss=0.4228\n",
      "file 1400/22000loss=0.3948\n",
      "file 1600/22000loss=0.3243\n",
      "file 1800/22000loss=0.3144\n",
      "file 2000/22000loss=0.0563\n",
      "file 2200/22000loss=0.3900\n",
      "file 2400/22000loss=0.3215\n",
      "file 2600/22000loss=0.0885\n",
      "file 2800/22000loss=0.1115\n",
      "file 3000/22000loss=0.1181\n",
      "file 3200/22000loss=0.0465\n",
      "file 3400/22000loss=0.0011\n",
      "file 3600/22000loss=0.0833\n",
      "file 3800/22000loss=0.0629\n",
      "file 4000/22000loss=0.1042\n",
      "file 4200/22000loss=0.0612\n",
      "file 4400/22000loss=0.2218\n",
      "file 4600/22000loss=0.2710\n",
      "file 4800/22000loss=0.0499\n",
      "file 5000/22000loss=0.0999\n",
      "file 5200/22000loss=0.2153\n",
      "file 5400/22000loss=0.1016\n",
      "file 5600/22000loss=0.1006\n",
      "file 5800/22000loss=0.2021\n",
      "file 6000/22000loss=0.0229\n",
      "file 6200/22000loss=0.0744\n",
      "file 6400/22000loss=0.2645\n",
      "file 6600/22000loss=0.0323\n",
      "file 6800/22000loss=0.1863\n",
      "file 7000/22000loss=0.0584\n",
      "file 7200/22000loss=0.2137\n",
      "file 7400/22000loss=0.1547\n",
      "file 7600/22000loss=0.9383\n",
      "file 7800/22000loss=0.1082\n",
      "file 8000/22000loss=0.1093\n",
      "file 8200/22000loss=0.0642\n",
      "file 8400/22000loss=0.5525\n",
      "file 8600/22000loss=0.3130\n",
      "file 8800/22000loss=0.0760\n",
      "file 9000/22000loss=0.1691\n",
      "file 9200/22000loss=0.0914\n",
      "file 9400/22000loss=0.0489\n",
      "file 9600/22000loss=0.0310\n",
      "file 9800/22000loss=0.2409\n",
      "file 10000/22000loss=0.0886\n",
      "file 10200/22000loss=0.0499\n",
      "file 10400/22000loss=1.1591\n",
      "file 10600/22000loss=0.0886\n",
      "file 10800/22000loss=0.0393\n",
      "file 11000/22000loss=0.0979\n",
      "file 11200/22000loss=0.0180\n",
      "file 11400/22000loss=0.2550\n",
      "file 11600/22000loss=0.0283\n",
      "file 11800/22000loss=0.4756\n",
      "file 12000/22000loss=0.2212\n",
      "file 12200/22000loss=0.1560\n",
      "file 12400/22000loss=0.3854\n",
      "file 12600/22000loss=0.0283\n",
      "file 12800/22000loss=0.4257\n",
      "file 13000/22000loss=0.0244\n",
      "file 13200/22000loss=0.1427\n",
      "file 13400/22000loss=0.3350\n",
      "file 13600/22000loss=0.1068\n",
      "file 13800/22000loss=0.0739\n",
      "file 14000/22000loss=0.0244\n",
      "file 14200/22000loss=0.1264\n",
      "file 14400/22000loss=0.0454\n",
      "file 14600/22000loss=0.0567\n",
      "file 14800/22000loss=0.0353\n",
      "file 15000/22000loss=0.0055\n",
      "file 15200/22000loss=0.1923\n",
      "file 15400/22000loss=1.0093\n",
      "file 15600/22000loss=0.0786\n",
      "file 15800/22000loss=0.3360\n",
      "file 16000/22000loss=0.0039\n",
      "file 16200/22000loss=0.0754\n",
      "file 16400/22000loss=0.0402\n",
      "file 16600/22000loss=0.0181\n",
      "file 16800/22000loss=0.2912\n",
      "file 17000/22000loss=0.0144\n",
      "file 17200/22000loss=0.0344\n",
      "file 17400/22000loss=0.2697\n",
      "file 17600/22000loss=0.2472\n",
      "file 17800/22000loss=0.0310\n",
      "file 18000/22000loss=0.0013\n",
      "file 18200/22000loss=0.6284\n",
      "file 18400/22000loss=0.0177\n",
      "file 18600/22000loss=0.1951\n",
      "file 18800/22000loss=0.0033\n",
      "file 19000/22000loss=0.0361\n",
      "file 19200/22000loss=0.3263\n",
      "file 19400/22000loss=0.0301\n",
      "file 19600/22000loss=2.0256\n",
      "file 19800/22000loss=0.0876\n",
      "file 20000/22000loss=0.2578\n",
      "file 20200/22000loss=0.3649\n",
      "file 20400/22000loss=0.1701\n",
      "file 20600/22000loss=0.0291\n",
      "file 20800/22000loss=0.4506\n",
      "file 21000/22000loss=0.2249\n",
      "file 21200/22000loss=0.3851\n",
      "file 21400/22000loss=0.0658\n",
      "file 21600/22000loss=0.0334\n",
      "file 21800/22000loss=0.0805\n",
      "file 22000/22000loss=0.0173\n",
      "Epoch 003/10 loss=0.2802  P=0.905 R=0.296 F1=0.446  lr=1.00e-04\n",
      "file 200/22000loss=0.2586\n",
      "file 400/22000loss=0.0035\n",
      "file 600/22000loss=0.1881\n",
      "file 800/22000loss=0.0409\n",
      "file 1000/22000loss=1.0502\n",
      "file 1200/22000loss=0.0012\n",
      "file 1400/22000loss=0.0317\n",
      "file 1600/22000loss=0.0706\n",
      "file 1800/22000loss=0.1427\n",
      "file 2000/22000loss=0.8910\n",
      "file 2200/22000loss=0.1052\n",
      "file 2400/22000loss=0.2438\n",
      "file 2600/22000loss=0.0582\n",
      "file 2800/22000loss=0.0202\n",
      "file 3000/22000loss=0.0300\n",
      "file 3200/22000loss=0.1506\n",
      "file 3400/22000loss=0.0979\n",
      "file 3600/22000loss=0.4538\n",
      "file 3800/22000loss=0.1342\n",
      "file 4000/22000loss=0.0284\n",
      "file 4200/22000loss=0.1978\n",
      "file 4400/22000loss=0.0618\n",
      "file 4600/22000loss=0.0527\n",
      "file 4800/22000loss=0.0251\n",
      "file 5000/22000loss=0.3217\n",
      "file 5200/22000loss=0.1315\n",
      "file 5400/22000loss=0.3582\n",
      "file 5600/22000loss=0.1172\n",
      "file 5800/22000loss=0.2686\n",
      "file 6000/22000loss=0.2687\n",
      "file 6200/22000loss=0.2812\n",
      "file 6400/22000loss=0.2762\n",
      "file 6600/22000loss=0.1307\n",
      "file 6800/22000loss=0.0010\n",
      "file 7000/22000loss=0.2635\n",
      "file 7200/22000loss=0.9608\n",
      "file 7400/22000loss=0.1307\n",
      "file 7600/22000loss=0.0341\n",
      "file 7800/22000loss=0.2511\n",
      "file 8000/22000loss=0.2636\n",
      "file 8200/22000loss=0.2264\n",
      "file 8400/22000loss=0.1072\n",
      "file 8600/22000loss=0.0457\n",
      "file 8800/22000loss=0.2717\n",
      "file 9000/22000loss=0.0836\n",
      "file 9200/22000loss=0.0520\n",
      "file 9400/22000loss=0.4889\n",
      "file 9600/22000loss=0.3095\n",
      "file 9800/22000loss=0.1833\n",
      "file 10000/22000loss=0.0343\n",
      "file 10200/22000loss=0.0908\n",
      "file 10400/22000loss=0.0353\n",
      "file 10600/22000loss=0.1927\n",
      "file 10800/22000loss=0.1212\n",
      "file 11000/22000loss=0.6233\n",
      "file 11200/22000loss=0.0158\n",
      "file 11400/22000loss=0.4473\n",
      "file 11600/22000loss=0.4120\n",
      "file 11800/22000loss=0.0788\n",
      "file 12000/22000loss=0.1399\n",
      "file 12200/22000loss=0.4528\n",
      "file 12400/22000loss=0.0112\n",
      "file 12600/22000loss=0.2204\n",
      "file 12800/22000loss=0.0010\n",
      "file 13000/22000loss=0.0329\n",
      "file 13200/22000loss=0.3399\n",
      "file 13400/22000loss=0.1440\n",
      "file 13600/22000loss=0.1873\n",
      "file 13800/22000loss=0.0732\n",
      "file 14000/22000loss=0.2766\n",
      "file 14200/22000loss=0.0020\n",
      "file 14400/22000loss=0.1790\n",
      "file 14600/22000loss=0.0203\n",
      "file 14800/22000loss=0.0417\n",
      "file 15000/22000loss=0.2304\n",
      "file 15200/22000loss=0.3664\n",
      "file 15400/22000loss=0.0160\n",
      "file 15600/22000loss=0.2200\n",
      "file 15800/22000loss=0.0290\n",
      "file 16000/22000loss=0.0078\n",
      "file 16200/22000loss=0.3848\n",
      "file 16400/22000loss=0.5375\n",
      "file 16600/22000loss=1.5356\n",
      "file 16800/22000loss=0.0152\n",
      "file 17000/22000loss=0.0411\n",
      "file 17200/22000loss=0.0743\n",
      "file 17400/22000loss=0.0259\n",
      "file 17600/22000loss=0.4237\n",
      "file 17800/22000loss=0.0461\n",
      "file 18000/22000loss=0.0024\n",
      "file 18200/22000loss=0.0169\n",
      "file 18400/22000loss=0.3564\n",
      "file 18600/22000loss=0.0009\n",
      "file 18800/22000loss=0.0012\n",
      "file 19000/22000loss=0.0007\n",
      "file 19200/22000loss=0.2116\n",
      "file 19400/22000loss=0.0109\n",
      "file 19600/22000loss=0.1632\n",
      "file 19800/22000loss=0.0205\n",
      "file 20000/22000loss=0.2726\n",
      "file 20200/22000loss=0.5965\n",
      "file 20400/22000loss=0.9697\n",
      "file 20600/22000loss=0.0017\n",
      "file 20800/22000loss=0.3245\n",
      "file 21000/22000loss=0.5599\n",
      "file 21200/22000loss=0.0931\n",
      "file 21400/22000loss=0.0884\n",
      "file 21600/22000loss=0.0222\n",
      "file 21800/22000loss=0.4617\n",
      "file 22000/22000loss=0.0102\n",
      "Epoch 004/10 loss=0.2539  P=0.940 R=0.305 F1=0.461  lr=1.00e-04\n",
      "file 200/22000loss=0.3457\n",
      "file 400/22000loss=0.0473\n",
      "file 600/22000loss=0.1172\n",
      "file 800/22000loss=0.0826\n",
      "file 1000/22000loss=0.2025\n",
      "file 1200/22000loss=0.1479\n",
      "file 1400/22000loss=0.1077\n",
      "file 1600/22000loss=0.0209\n",
      "file 1800/22000loss=0.0264\n",
      "file 2000/22000loss=0.0749\n",
      "file 2200/22000loss=1.4311\n",
      "file 2400/22000loss=0.0196\n",
      "file 2600/22000loss=0.1860\n",
      "file 2800/22000loss=0.0244\n",
      "file 3000/22000loss=0.0864\n",
      "file 3200/22000loss=0.2723\n",
      "file 3400/22000loss=0.0021\n",
      "file 3600/22000loss=0.0730\n",
      "file 3800/22000loss=0.0244\n",
      "file 4000/22000loss=0.0223\n",
      "file 4200/22000loss=2.8412\n",
      "file 4400/22000loss=0.2213\n",
      "file 4600/22000loss=0.1914\n",
      "file 4800/22000loss=0.0033\n",
      "file 5000/22000loss=0.0188\n",
      "file 5200/22000loss=0.2513\n",
      "file 5400/22000loss=0.2604\n",
      "file 5600/22000loss=0.1939\n",
      "file 5800/22000loss=0.0166\n",
      "file 6000/22000loss=0.1044\n",
      "file 6200/22000loss=0.0148\n",
      "file 6400/22000loss=0.1003\n",
      "file 6600/22000loss=0.0228\n",
      "file 6800/22000loss=0.0402\n",
      "file 7000/22000loss=0.0457\n",
      "file 7200/22000loss=0.4696\n",
      "file 7400/22000loss=0.1230\n",
      "file 7600/22000loss=0.0130\n",
      "file 7800/22000loss=0.0275\n",
      "file 8000/22000loss=0.0697\n",
      "file 8200/22000loss=0.5476\n",
      "file 8400/22000loss=0.0207\n",
      "file 8600/22000loss=0.2100\n",
      "file 8800/22000loss=0.0343\n",
      "file 9000/22000loss=0.0074\n",
      "file 9200/22000loss=0.0164\n",
      "file 9400/22000loss=0.0015\n",
      "file 9600/22000loss=0.0008\n",
      "file 9800/22000loss=0.0500\n",
      "file 10000/22000loss=0.1229\n",
      "file 10200/22000loss=0.1405\n",
      "file 10400/22000loss=0.2239\n",
      "file 10600/22000loss=0.0017\n",
      "file 10800/22000loss=0.6200\n",
      "file 11000/22000loss=0.0004\n",
      "file 11200/22000loss=0.0190\n",
      "file 11400/22000loss=0.1376\n",
      "file 11600/22000loss=0.1106\n",
      "file 11800/22000loss=0.0179\n",
      "file 12000/22000loss=0.3144\n",
      "file 12200/22000loss=0.2244\n",
      "file 12400/22000loss=0.1739\n",
      "file 12600/22000loss=0.0006\n",
      "file 12800/22000loss=0.5008\n",
      "file 13000/22000loss=0.0020\n",
      "file 13200/22000loss=0.0195\n",
      "file 13400/22000loss=0.0488\n",
      "file 13600/22000loss=0.0347\n",
      "file 13800/22000loss=0.1840\n",
      "file 14000/22000loss=0.0943\n",
      "file 14200/22000loss=0.0219\n",
      "file 14400/22000loss=1.5443\n",
      "file 14600/22000loss=0.0239\n",
      "file 14800/22000loss=0.1138\n",
      "file 15000/22000loss=0.0386\n",
      "file 15200/22000loss=0.0187\n",
      "file 15400/22000loss=0.0607\n",
      "file 15600/22000loss=0.0992\n",
      "file 15800/22000loss=0.3900\n",
      "file 16000/22000loss=0.0003\n",
      "file 16200/22000loss=0.0003\n",
      "file 16400/22000loss=0.2340\n",
      "file 16600/22000loss=0.1010\n",
      "file 16800/22000loss=0.0510\n",
      "file 17000/22000loss=0.3453\n",
      "file 17200/22000loss=2.3704\n",
      "file 17400/22000loss=0.0075\n",
      "file 17600/22000loss=0.0752\n",
      "file 17800/22000loss=0.0006\n",
      "file 18000/22000loss=0.1686\n",
      "file 18200/22000loss=0.0315\n",
      "file 18400/22000loss=0.0217\n",
      "file 18600/22000loss=0.0701\n",
      "file 18800/22000loss=0.0013\n",
      "file 19000/22000loss=0.1409\n",
      "file 19200/22000loss=0.7553\n",
      "file 19400/22000loss=0.0999\n",
      "file 19600/22000loss=0.4558\n",
      "file 19800/22000loss=0.0773\n",
      "file 20000/22000loss=0.2553\n",
      "file 20200/22000loss=0.4289\n",
      "file 20400/22000loss=0.0131\n",
      "file 20600/22000loss=0.4644\n",
      "file 20800/22000loss=0.0197\n",
      "file 21000/22000loss=0.0333\n",
      "file 21200/22000loss=0.0041\n",
      "file 21400/22000loss=0.0155\n",
      "file 21600/22000loss=0.0370\n",
      "file 21800/22000loss=0.2196\n",
      "file 22000/22000loss=0.3836\n",
      "Epoch 005/10 loss=0.2449  P=0.817 R=0.287 F1=0.424  lr=1.00e-04\n",
      "file 200/22000loss=0.0190\n",
      "file 400/22000loss=0.0808\n",
      "file 600/22000loss=0.0037\n",
      "file 800/22000loss=0.3488\n",
      "file 1000/22000loss=0.0008\n",
      "file 1200/22000loss=0.0284\n",
      "file 1400/22000loss=0.0080\n",
      "file 1600/22000loss=0.0004\n",
      "file 1800/22000loss=0.1221\n",
      "file 2000/22000loss=0.3058\n",
      "file 2200/22000loss=0.2791\n",
      "file 2400/22000loss=0.4056\n",
      "file 2600/22000loss=0.0677\n",
      "file 2800/22000loss=0.0549\n",
      "file 3000/22000loss=0.3443\n",
      "file 3200/22000loss=0.0480\n",
      "file 3400/22000loss=0.4558\n",
      "file 3600/22000loss=0.0088\n",
      "file 3800/22000loss=1.0909\n",
      "file 4000/22000loss=0.0053\n",
      "file 4200/22000loss=0.1256\n",
      "file 4400/22000loss=0.1075\n",
      "file 4600/22000loss=0.0055\n",
      "file 4800/22000loss=0.0130\n",
      "file 5000/22000loss=0.1479\n",
      "file 5200/22000loss=0.0327\n",
      "file 5400/22000loss=0.2030\n",
      "file 5600/22000loss=0.0177\n",
      "file 5800/22000loss=0.3461\n",
      "file 6000/22000loss=0.2207\n",
      "file 6200/22000loss=0.1081\n",
      "file 6400/22000loss=0.2827\n",
      "file 6600/22000loss=0.0227\n",
      "file 6800/22000loss=0.0042\n",
      "file 7000/22000loss=0.0798\n",
      "file 7200/22000loss=0.2267\n",
      "file 7400/22000loss=0.0534\n",
      "file 7600/22000loss=0.0177\n",
      "file 7800/22000loss=0.0010\n",
      "file 8000/22000loss=0.0850\n",
      "file 8200/22000loss=0.0652\n",
      "file 8400/22000loss=0.1693\n",
      "file 8600/22000loss=0.1353\n",
      "file 8800/22000loss=0.2984\n",
      "file 9000/22000loss=0.1343\n",
      "file 9200/22000loss=0.3033\n",
      "file 9400/22000loss=0.2829\n",
      "file 9600/22000loss=0.2422\n",
      "file 9800/22000loss=0.0666\n",
      "file 10000/22000loss=0.0149\n",
      "file 10200/22000loss=0.0223\n",
      "file 10400/22000loss=0.3388\n",
      "file 10600/22000loss=0.0004\n",
      "file 10800/22000loss=0.2218\n",
      "file 11000/22000loss=0.1508\n",
      "file 11200/22000loss=0.0128\n",
      "file 11400/22000loss=0.0122\n",
      "file 11600/22000loss=0.0005\n",
      "file 11800/22000loss=0.2702\n",
      "file 12000/22000loss=0.0237\n",
      "file 12200/22000loss=0.0003\n",
      "file 12400/22000loss=0.2008\n",
      "file 12600/22000loss=0.2286\n",
      "file 12800/22000loss=0.0003\n",
      "file 13000/22000loss=0.9644\n",
      "file 13200/22000loss=0.1113\n",
      "file 13400/22000loss=0.0129\n",
      "file 13600/22000loss=0.2079\n",
      "file 13800/22000loss=0.0284\n",
      "file 14000/22000loss=0.8116\n",
      "file 14200/22000loss=0.5743\n",
      "file 14400/22000loss=0.0695\n",
      "file 14600/22000loss=0.1923\n",
      "file 14800/22000loss=0.0184\n",
      "file 15000/22000loss=0.0004\n",
      "file 15200/22000loss=0.5174\n",
      "file 15400/22000loss=0.5224\n",
      "file 15600/22000loss=0.0083\n",
      "file 15800/22000loss=0.0004\n",
      "file 16000/22000loss=0.0059\n",
      "file 16200/22000loss=0.0629\n",
      "file 16400/22000loss=0.2840\n",
      "file 16600/22000loss=0.0121\n",
      "file 16800/22000loss=0.2291\n",
      "file 17000/22000loss=0.1072\n",
      "file 17200/22000loss=0.0004\n",
      "file 17400/22000loss=0.1869\n",
      "file 17600/22000loss=0.1388\n",
      "file 17800/22000loss=0.0127\n",
      "file 18000/22000loss=0.0352\n",
      "file 18200/22000loss=0.0937\n",
      "file 18400/22000loss=0.7914\n",
      "file 18600/22000loss=0.0808\n",
      "file 18800/22000loss=0.0888\n",
      "file 19000/22000loss=0.3215\n",
      "file 19200/22000loss=0.0279\n",
      "file 19400/22000loss=0.0003\n",
      "file 19600/22000loss=0.2245\n",
      "file 19800/22000loss=0.0648\n",
      "file 20000/22000loss=0.3417\n",
      "file 20200/22000loss=0.3826\n",
      "file 20400/22000loss=1.4707\n",
      "file 20600/22000loss=0.1788\n",
      "file 20800/22000loss=0.0186\n",
      "file 21000/22000loss=0.0570\n",
      "file 21200/22000loss=0.2185\n",
      "file 21400/22000loss=0.0244\n",
      "file 21600/22000loss=0.0176\n",
      "file 21800/22000loss=0.0004\n",
      "file 22000/22000loss=0.2723\n",
      "Epoch 006/10 loss=0.2362  P=0.824 R=0.300 F1=0.440  lr=1.00e-04\n",
      "file 200/22000loss=0.0055\n",
      "file 400/22000loss=0.1616\n",
      "file 600/22000loss=0.0992\n",
      "file 800/22000loss=0.5697\n",
      "file 1000/22000loss=0.0459\n",
      "file 1200/22000loss=0.3891\n",
      "file 1400/22000loss=0.0178\n",
      "file 1600/22000loss=0.1877\n",
      "file 1800/22000loss=0.7206\n",
      "file 2000/22000loss=0.1305\n",
      "file 2200/22000loss=0.8784\n",
      "file 2400/22000loss=0.2354\n",
      "file 2600/22000loss=0.0093\n",
      "file 2800/22000loss=0.0004\n",
      "file 3000/22000loss=0.3418\n",
      "file 3200/22000loss=0.1030\n",
      "file 3400/22000loss=0.0014\n",
      "file 3600/22000loss=0.2402\n",
      "file 3800/22000loss=0.0387\n",
      "file 4000/22000loss=0.1377\n",
      "file 4200/22000loss=0.0077\n",
      "file 4400/22000loss=0.0152\n",
      "file 4600/22000loss=0.1054\n",
      "file 4800/22000loss=0.2106\n",
      "file 5000/22000loss=0.2634\n",
      "file 5200/22000loss=0.1336\n",
      "file 5400/22000loss=0.0309\n",
      "file 5600/22000loss=0.0056\n",
      "file 5800/22000loss=0.0906\n",
      "file 6000/22000loss=0.0231\n",
      "file 6200/22000loss=0.0120\n",
      "file 6400/22000loss=0.3269\n",
      "file 6600/22000loss=0.1487\n",
      "file 6800/22000loss=0.1057\n",
      "file 7000/22000loss=0.1685\n",
      "file 7200/22000loss=0.1585\n",
      "file 7400/22000loss=0.0076\n",
      "file 7600/22000loss=0.0171\n",
      "file 7800/22000loss=0.3833\n",
      "file 8000/22000loss=0.0106\n",
      "file 8200/22000loss=0.0208\n",
      "file 8400/22000loss=0.1686\n",
      "file 8600/22000loss=0.0099\n",
      "file 8800/22000loss=0.0035\n",
      "file 9000/22000loss=0.0378\n",
      "file 9200/22000loss=0.0683\n",
      "file 9400/22000loss=0.0081\n",
      "file 9600/22000loss=0.0676\n",
      "file 9800/22000loss=0.6646\n",
      "file 10000/22000loss=0.1882\n",
      "file 10200/22000loss=0.0795\n",
      "file 10400/22000loss=0.5373\n",
      "file 10600/22000loss=0.0040\n",
      "file 10800/22000loss=0.0422\n",
      "file 11000/22000loss=0.0148\n",
      "file 11200/22000loss=0.2091\n",
      "file 11400/22000loss=0.2474\n",
      "file 11600/22000loss=0.1627\n",
      "file 11800/22000loss=0.0381\n",
      "file 12000/22000loss=0.0215\n",
      "file 12200/22000loss=0.0104\n",
      "file 12400/22000loss=0.0005\n",
      "file 12600/22000loss=0.0051\n",
      "file 12800/22000loss=0.0521\n",
      "file 13000/22000loss=0.2679\n",
      "file 13200/22000loss=0.5027\n",
      "file 13400/22000loss=0.0082\n",
      "file 13600/22000loss=0.0792\n",
      "file 13800/22000loss=0.0198\n",
      "file 14000/22000loss=0.0133\n",
      "file 14200/22000loss=0.1732\n",
      "file 14400/22000loss=0.0363\n",
      "file 14600/22000loss=0.3093\n",
      "file 14800/22000loss=0.1659\n",
      "file 15000/22000loss=0.2867\n",
      "file 15200/22000loss=0.0004\n",
      "file 15400/22000loss=0.1496\n",
      "file 15600/22000loss=0.1367\n",
      "file 15800/22000loss=0.2487\n",
      "file 16000/22000loss=0.2208\n",
      "file 16200/22000loss=0.2186\n",
      "file 16400/22000loss=0.2227\n",
      "file 16600/22000loss=0.0723\n",
      "file 16800/22000loss=0.0003\n",
      "file 17000/22000loss=0.3409\n",
      "file 17200/22000loss=0.1567\n",
      "file 17400/22000loss=0.0007\n",
      "file 17600/22000loss=0.0131\n",
      "file 17800/22000loss=0.2571\n",
      "file 18000/22000loss=0.1655\n",
      "file 18200/22000loss=0.0464\n",
      "file 18400/22000loss=0.0780\n",
      "file 18600/22000loss=0.0052\n",
      "file 18800/22000loss=0.0064\n",
      "file 19000/22000loss=0.0674\n",
      "file 19200/22000loss=0.0207\n",
      "file 19400/22000loss=9.0068\n",
      "file 19600/22000loss=0.1200\n",
      "file 19800/22000loss=0.0190\n",
      "file 20000/22000loss=0.3067\n",
      "file 20200/22000loss=0.2359\n",
      "file 20400/22000loss=0.0003\n",
      "file 20600/22000loss=0.1569\n",
      "file 20800/22000loss=0.3777\n",
      "file 21000/22000loss=0.6562\n",
      "file 21200/22000loss=0.0050\n",
      "file 21400/22000loss=0.3340\n",
      "file 21600/22000loss=0.0056\n",
      "file 21800/22000loss=0.2597\n",
      "file 22000/22000loss=0.2851\n",
      "Epoch 007/10 loss=0.2277  P=0.940 R=0.294 F1=0.448  lr=1.00e-04\n",
      "file 200/22000loss=0.0105\n",
      "file 400/22000loss=0.0612\n",
      "file 600/22000loss=0.1797\n",
      "file 800/22000loss=0.0005\n",
      "file 1000/22000loss=0.2705\n",
      "file 1200/22000loss=0.3741\n",
      "file 1400/22000loss=0.7246\n",
      "file 1600/22000loss=0.2774\n",
      "file 1800/22000loss=0.2743\n",
      "file 2000/22000loss=0.0002\n",
      "file 2200/22000loss=3.8198\n",
      "file 2400/22000loss=0.4858\n",
      "file 2600/22000loss=0.3012\n",
      "file 2800/22000loss=0.4897\n",
      "file 3000/22000loss=0.2766\n",
      "file 3200/22000loss=0.0088\n",
      "file 3400/22000loss=0.3535\n",
      "file 3600/22000loss=0.0087\n",
      "file 3800/22000loss=0.0405\n",
      "file 4000/22000loss=0.0071\n",
      "file 4200/22000loss=0.0028\n",
      "file 4400/22000loss=0.0110\n",
      "file 4600/22000loss=0.0111\n",
      "file 4800/22000loss=0.0388\n",
      "file 5000/22000loss=0.0719\n",
      "file 5200/22000loss=0.0002\n",
      "file 5400/22000loss=0.3663\n",
      "file 5600/22000loss=0.0006\n",
      "file 5800/22000loss=0.0141\n",
      "file 6000/22000loss=0.0006\n",
      "file 6200/22000loss=0.0148\n",
      "file 6400/22000loss=0.0101\n",
      "file 6600/22000loss=0.2099\n",
      "file 6800/22000loss=0.0140\n",
      "file 7000/22000loss=0.7485\n",
      "file 7200/22000loss=0.0284\n",
      "file 7400/22000loss=0.2201\n",
      "file 7600/22000loss=0.0758\n",
      "file 7800/22000loss=6.2215\n",
      "file 8000/22000loss=0.2992\n",
      "file 8200/22000loss=0.0133\n",
      "file 8400/22000loss=0.0022\n",
      "file 8600/22000loss=0.2462\n",
      "file 8800/22000loss=0.3800\n",
      "file 9000/22000loss=0.1355\n",
      "file 9200/22000loss=0.0005\n",
      "file 9400/22000loss=0.3217\n",
      "file 9600/22000loss=0.0160\n",
      "file 9800/22000loss=0.0003\n",
      "file 10000/22000loss=0.0031\n",
      "file 10200/22000loss=0.7983\n",
      "file 10400/22000loss=0.0700\n",
      "file 10600/22000loss=0.9720\n",
      "file 10800/22000loss=0.0206\n",
      "file 11000/22000loss=0.0093\n",
      "file 11200/22000loss=0.0149\n",
      "file 11400/22000loss=0.0122\n",
      "file 11600/22000loss=0.0385\n",
      "file 11800/22000loss=0.0261\n",
      "file 12000/22000loss=0.2235\n",
      "file 12200/22000loss=0.0023\n",
      "file 12400/22000loss=0.0033\n",
      "file 12600/22000loss=0.0268\n",
      "file 12800/22000loss=0.2785\n",
      "file 13000/22000loss=0.0898\n",
      "file 13200/22000loss=0.0043\n",
      "file 13400/22000loss=0.2794\n",
      "file 13600/22000loss=0.2494\n",
      "file 13800/22000loss=0.0080\n",
      "file 14000/22000loss=0.0042\n",
      "file 14200/22000loss=0.6225\n",
      "file 14400/22000loss=0.0391\n",
      "file 14600/22000loss=0.9094\n",
      "file 14800/22000loss=0.0031\n",
      "file 15000/22000loss=0.3222\n",
      "file 15200/22000loss=0.0008\n",
      "file 15400/22000loss=0.0454\n",
      "file 15600/22000loss=0.3026\n",
      "file 15800/22000loss=0.0926\n",
      "file 16000/22000loss=0.0161\n",
      "file 16200/22000loss=0.0029\n",
      "file 16400/22000loss=0.0057\n",
      "file 16600/22000loss=0.0022\n",
      "file 16800/22000loss=0.1703\n",
      "file 17000/22000loss=0.1838\n",
      "file 17200/22000loss=0.0904\n",
      "file 17400/22000loss=0.0067\n",
      "file 17600/22000loss=0.0809\n",
      "file 17800/22000loss=0.2016\n",
      "file 18000/22000loss=0.1358\n",
      "file 18200/22000loss=0.2209\n",
      "file 18400/22000loss=0.1471\n",
      "file 18600/22000loss=0.1881\n",
      "file 18800/22000loss=0.2907\n",
      "file 19000/22000loss=0.1801\n",
      "file 19200/22000loss=0.4252\n",
      "file 19400/22000loss=0.0132\n",
      "file 19600/22000loss=0.0809\n",
      "file 19800/22000loss=0.0145\n",
      "file 20000/22000loss=0.0163\n",
      "file 20200/22000loss=1.7006\n",
      "file 20400/22000loss=0.0040\n",
      "file 20600/22000loss=0.1311\n",
      "file 20800/22000loss=0.0160\n",
      "file 21000/22000loss=0.0734\n",
      "file 21200/22000loss=0.1240\n",
      "file 21400/22000loss=0.0491\n",
      "file 21600/22000loss=0.1809\n",
      "file 21800/22000loss=0.4084\n",
      "file 22000/22000loss=0.2872\n",
      "Epoch 008/10 loss=0.2268  P=0.939 R=0.257 F1=0.404  lr=1.00e-04\n",
      "file 200/22000loss=0.0118\n",
      "file 400/22000loss=0.0006\n",
      "file 600/22000loss=0.0005\n",
      "file 800/22000loss=0.0011\n",
      "file 1000/22000loss=0.1384\n",
      "file 1200/22000loss=5.3389\n",
      "file 1400/22000loss=0.0026\n",
      "file 1600/22000loss=0.3248\n",
      "file 1800/22000loss=0.2278\n",
      "file 2000/22000loss=0.0023\n",
      "file 2200/22000loss=0.6561\n",
      "file 2400/22000loss=3.8418\n",
      "file 2600/22000loss=0.0571\n",
      "file 2800/22000loss=0.0209\n",
      "file 3000/22000loss=0.0003\n",
      "file 3200/22000loss=0.0094\n",
      "file 3400/22000loss=0.2046\n",
      "file 3600/22000loss=0.0998\n",
      "file 3800/22000loss=0.4779\n",
      "file 4000/22000loss=0.0087\n",
      "file 4200/22000loss=0.5948\n",
      "file 4400/22000loss=0.0001\n",
      "file 4600/22000loss=0.4987\n",
      "file 4800/22000loss=0.0098\n",
      "file 5000/22000loss=0.0028\n",
      "file 5200/22000loss=0.0003\n",
      "file 5400/22000loss=2.2256\n",
      "file 5600/22000loss=0.0002\n",
      "file 5800/22000loss=1.2812\n",
      "file 6000/22000loss=0.3518\n",
      "file 6200/22000loss=0.0079\n",
      "file 6400/22000loss=0.1735\n",
      "file 6600/22000loss=0.1466\n",
      "file 6800/22000loss=0.0165\n",
      "file 7000/22000loss=0.0074\n",
      "file 7200/22000loss=0.0005\n",
      "file 7400/22000loss=0.0147\n",
      "file 7600/22000loss=0.0141\n",
      "file 7800/22000loss=0.0869\n",
      "file 8000/22000loss=0.2993\n",
      "file 8200/22000loss=0.2394\n",
      "file 8400/22000loss=0.2980\n",
      "file 8600/22000loss=0.0433\n",
      "file 8800/22000loss=0.4211\n",
      "file 9000/22000loss=0.0822\n",
      "file 9200/22000loss=0.0075\n",
      "file 9400/22000loss=0.0997\n",
      "file 9600/22000loss=0.0240\n",
      "file 9800/22000loss=0.1270\n",
      "file 10000/22000loss=0.2755\n",
      "file 10200/22000loss=0.0102\n",
      "file 10400/22000loss=0.2444\n",
      "file 10600/22000loss=0.1594\n",
      "file 10800/22000loss=0.1123\n",
      "file 11000/22000loss=0.3942\n",
      "file 11200/22000loss=0.0059\n",
      "file 11400/22000loss=0.0767\n",
      "file 11600/22000loss=0.3706\n",
      "file 11800/22000loss=0.0154\n",
      "file 12000/22000loss=0.1520\n",
      "file 12200/22000loss=0.3188\n",
      "file 12400/22000loss=0.2470\n",
      "file 12600/22000loss=3.5357\n",
      "file 12800/22000loss=0.1551\n",
      "file 13000/22000loss=0.0002\n",
      "file 13200/22000loss=0.0121\n",
      "file 13400/22000loss=0.0020\n",
      "file 13600/22000loss=0.4953\n",
      "file 13800/22000loss=0.6783\n",
      "file 14000/22000loss=0.0318\n",
      "file 14200/22000loss=0.0030\n",
      "file 14400/22000loss=0.0063\n",
      "file 14600/22000loss=0.0055\n",
      "file 14800/22000loss=0.0089\n",
      "file 15000/22000loss=0.0060\n",
      "file 15200/22000loss=0.0007\n",
      "file 15400/22000loss=0.0007\n",
      "file 15600/22000loss=0.1936\n",
      "file 15800/22000loss=0.0770\n",
      "file 16000/22000loss=0.0138\n",
      "file 16200/22000loss=1.0009\n",
      "file 16400/22000loss=0.0057\n",
      "file 16600/22000loss=0.0022\n",
      "file 16800/22000loss=0.5263\n",
      "file 17000/22000loss=0.1838\n",
      "file 17200/22000loss=0.2790\n",
      "file 17400/22000loss=0.0986\n",
      "file 17600/22000loss=0.0022\n",
      "file 17800/22000loss=0.0852\n",
      "file 18000/22000loss=1.1162\n",
      "file 18200/22000loss=5.4437\n",
      "file 18400/22000loss=0.1654\n",
      "file 18600/22000loss=1.4033\n",
      "file 18800/22000loss=0.1207\n",
      "file 19000/22000loss=0.0114\n",
      "file 19200/22000loss=0.0477\n",
      "file 19400/22000loss=0.0819\n",
      "file 19600/22000loss=0.0002\n",
      "file 19800/22000loss=0.0134\n",
      "file 20000/22000loss=0.1209\n",
      "file 20200/22000loss=0.3816\n",
      "file 20400/22000loss=0.2244\n",
      "file 20600/22000loss=0.0054\n",
      "file 20800/22000loss=0.0126\n",
      "file 21000/22000loss=0.2703\n",
      "file 21200/22000loss=0.0142\n",
      "file 21400/22000loss=0.0044\n",
      "file 21600/22000loss=0.7971\n",
      "file 21800/22000loss=0.0741\n",
      "file 22000/22000loss=0.0047\n",
      "Epoch 009/10 loss=0.2169  P=0.944 R=0.278 F1=0.430  lr=1.00e-04\n",
      "file 200/22000loss=0.5810\n",
      "file 400/22000loss=0.0492\n",
      "file 600/22000loss=0.0371\n",
      "file 800/22000loss=0.0113\n",
      "file 1000/22000loss=0.0133\n",
      "file 1200/22000loss=0.2251\n",
      "file 1400/22000loss=0.0093\n",
      "file 1600/22000loss=0.0066\n",
      "file 1800/22000loss=0.0035\n",
      "file 2000/22000loss=0.0889\n",
      "file 2200/22000loss=0.1964\n",
      "file 2400/22000loss=0.3267\n",
      "file 2600/22000loss=0.1111\n",
      "file 2800/22000loss=0.1795\n",
      "file 3000/22000loss=0.0008\n",
      "file 3200/22000loss=0.0926\n",
      "file 3400/22000loss=0.0031\n",
      "file 3600/22000loss=0.0106\n",
      "file 3800/22000loss=0.0126\n",
      "file 4000/22000loss=0.0061\n",
      "file 4200/22000loss=0.0402\n",
      "file 4400/22000loss=0.0002\n",
      "file 4600/22000loss=0.2293\n",
      "file 4800/22000loss=1.6077\n",
      "file 5000/22000loss=0.0304\n",
      "file 5200/22000loss=0.0212\n",
      "file 5400/22000loss=0.1762\n",
      "file 5600/22000loss=0.3749\n",
      "file 5800/22000loss=0.0161\n",
      "file 6000/22000loss=0.0006\n",
      "file 6200/22000loss=0.2044\n",
      "file 6400/22000loss=0.0069\n",
      "file 6600/22000loss=0.0992\n",
      "file 6800/22000loss=0.0050\n",
      "file 7000/22000loss=0.3671\n",
      "file 7200/22000loss=0.1538\n",
      "file 7400/22000loss=0.1788\n",
      "file 7600/22000loss=0.7119\n",
      "file 7800/22000loss=0.1040\n",
      "file 8000/22000loss=0.0007\n",
      "file 8200/22000loss=0.7171\n",
      "file 8400/22000loss=0.2173\n",
      "file 8600/22000loss=0.0049\n",
      "file 8800/22000loss=0.2920\n",
      "file 9000/22000loss=0.0023\n",
      "file 9200/22000loss=0.0044\n",
      "file 9400/22000loss=0.0030\n",
      "file 9600/22000loss=0.0181\n",
      "file 9800/22000loss=0.0354\n",
      "file 10000/22000loss=0.0025\n",
      "file 10200/22000loss=0.0043\n",
      "file 10400/22000loss=0.1587\n",
      "file 10600/22000loss=0.0020\n",
      "file 10800/22000loss=0.0220\n",
      "file 11000/22000loss=0.1044\n",
      "file 11200/22000loss=0.0727\n",
      "file 11400/22000loss=0.0365\n",
      "file 11600/22000loss=0.0418\n",
      "file 11800/22000loss=0.1403\n",
      "file 12000/22000loss=0.0012\n",
      "file 12200/22000loss=0.0030\n",
      "file 12400/22000loss=0.0429\n",
      "file 12600/22000loss=0.1460\n",
      "file 12800/22000loss=0.2274\n",
      "file 13000/22000loss=0.1868\n",
      "file 13200/22000loss=0.0051\n",
      "file 13400/22000loss=0.0087\n",
      "file 13600/22000loss=0.0024\n",
      "file 13800/22000loss=0.2352\n",
      "file 14000/22000loss=0.0474\n",
      "file 14200/22000loss=0.0230\n",
      "file 14400/22000loss=0.0049\n",
      "file 14600/22000loss=0.0003\n",
      "file 14800/22000loss=0.0079\n",
      "file 15000/22000loss=0.3721\n",
      "file 15200/22000loss=4.6302\n",
      "file 15400/22000loss=0.2810\n",
      "file 15600/22000loss=0.0059\n",
      "file 15800/22000loss=1.8975\n",
      "file 16000/22000loss=0.0339\n",
      "file 16200/22000loss=0.0510\n",
      "file 16400/22000loss=0.0418\n",
      "file 16600/22000loss=0.1597\n",
      "file 16800/22000loss=0.0596\n",
      "file 17000/22000loss=0.0647\n",
      "file 17200/22000loss=0.1207\n",
      "file 17400/22000loss=0.0301\n",
      "file 17600/22000loss=0.0926\n",
      "file 17800/22000loss=0.0005\n",
      "file 18000/22000loss=1.8017\n",
      "file 18200/22000loss=0.2364\n",
      "file 18400/22000loss=0.0799\n",
      "file 18600/22000loss=0.0203\n",
      "file 18800/22000loss=0.0032\n",
      "file 19000/22000loss=0.2039\n",
      "file 19200/22000loss=0.0092\n",
      "file 19400/22000loss=0.1796\n",
      "file 19600/22000loss=0.0148\n",
      "file 19800/22000loss=1.4308\n",
      "file 20000/22000loss=0.0182\n",
      "file 20200/22000loss=0.1741\n",
      "file 20400/22000loss=0.0139\n",
      "file 20600/22000loss=5.9886\n",
      "file 20800/22000loss=0.0012\n",
      "file 21000/22000loss=0.0036\n",
      "file 21200/22000loss=0.4060\n",
      "file 21400/22000loss=0.0060\n",
      "file 21600/22000loss=0.1620\n",
      "file 21800/22000loss=0.0004\n",
      "file 22000/22000loss=2.6405\n",
      "Epoch 010/10 loss=0.2122  P=0.942 R=0.264 F1=0.412  lr=1.00e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5571868148887226,\n",
       "  0.3448221714387861,\n",
       "  0.2802136015731672,\n",
       "  0.25391838585303544,\n",
       "  0.2449102310859377,\n",
       "  0.2361602805621658,\n",
       "  0.22774925860335418,\n",
       "  0.2268090441666642,\n",
       "  0.2168904802005753,\n",
       "  0.21220221504694345],\n",
       " OrderedDict([('W_edge',\n",
       "               tensor([[ 0.1551,  0.0952,  0.3230,  ..., -0.2524, -0.0649, -0.5254],\n",
       "                       [ 0.0184,  0.0859, -0.1465,  ..., -0.0144, -0.1180, -0.1406],\n",
       "                       [ 0.0187, -0.1316, -0.1434,  ..., -0.0386,  0.1325,  0.0396],\n",
       "                       ...,\n",
       "                       [ 0.4404,  0.4248, -0.5805,  ...,  0.0558, -0.0717,  0.2591],\n",
       "                       [ 0.0347, -0.0468, -0.0090,  ..., -0.2057, -0.0389,  0.0642],\n",
       "                       [ 0.1833, -0.0169,  0.2727,  ...,  0.1874,  0.0725,  0.0346]],\n",
       "                      device='cuda:0')),\n",
       "              ('gat1.att',\n",
       "               tensor([[[ 0.2603, -0.1332,  0.3011, -0.2978,  0.2721,  0.1490,  0.1085,\n",
       "                          0.0150,  0.1932, -0.3347, -0.2347, -0.1280, -0.2379,  0.2188,\n",
       "                         -0.1853, -0.1468, -0.1615, -0.0956,  0.3124,  0.3876, -0.1985,\n",
       "                          0.3260,  0.3879,  0.0525,  0.2823, -0.0061, -0.2783,  0.0619,\n",
       "                         -0.2854, -0.0830, -0.1182, -0.1931, -0.1050, -0.2673,  0.3149,\n",
       "                         -0.2566,  0.1174, -0.2950,  0.1679, -0.3859,  0.1377, -0.3091,\n",
       "                          0.1184,  0.0746, -0.2560, -0.2551,  0.2575,  0.3046,  0.1436,\n",
       "                         -0.0322, -0.0910,  0.1734,  0.2624,  0.1720,  0.2399,  0.2672,\n",
       "                         -0.0733,  0.1088, -0.2780, -0.1233, -0.0452,  0.2364,  0.0707,\n",
       "                         -0.2029]]], device='cuda:0')),\n",
       "              ('gat1.bias',\n",
       "               tensor([ 0.2034,  0.0114,  0.1457,  0.1052,  0.0651,  0.1581, -0.0131,  0.0554,\n",
       "                        0.0377, -0.0659,  0.0367,  0.1586,  0.1067,  0.1270,  0.2927,  0.0998,\n",
       "                        0.0615,  0.2692,  0.1903,  0.0281,  0.1283,  0.0412,  0.2677,  0.0928,\n",
       "                        0.1830,  0.0877,  0.0801,  0.1691,  0.0761,  0.1132,  0.1633,  0.1227,\n",
       "                        0.0113,  0.0265,  0.0656,  0.1310,  0.0301,  0.0841,  0.0594,  0.2593,\n",
       "                        0.0320, -0.0739,  0.0624,  0.0411,  0.1616,  0.1408,  0.1307, -0.0093,\n",
       "                        0.0661,  0.0562,  0.1057,  0.1406,  0.2899, -0.0694,  0.1849,  0.0530,\n",
       "                        0.1700,  0.1107,  0.1575,  0.0747,  0.0026,  0.1698,  0.0488,  0.3251],\n",
       "                      device='cuda:0')),\n",
       "              ('gat1.lin_l.weight',\n",
       "               tensor([[ 0.3428, -0.1360, -0.0076,  ...,  0.1373, -0.1528, -0.0838],\n",
       "                       [ 0.2632, -0.1111, -0.0847,  ..., -0.1563, -0.0831,  0.1696],\n",
       "                       [-0.5216, -0.1324, -0.1284,  ..., -0.0507,  0.1133, -0.0023],\n",
       "                       ...,\n",
       "                       [ 0.4479,  0.1373,  0.0256,  ...,  0.0439, -0.1119, -0.0858],\n",
       "                       [ 0.4405, -0.1512, -0.0715,  ..., -0.0272, -0.0819,  0.1430],\n",
       "                       [ 0.2993, -0.1359,  0.0881,  ..., -0.0363, -0.0181,  0.0122]],\n",
       "                      device='cuda:0')),\n",
       "              ('gat1.lin_l.bias',\n",
       "               tensor([ 0.1936, -0.0697,  0.2506,  0.1225,  0.0085,  0.0990,  0.0142,  0.0392,\n",
       "                        0.1302, -0.1396,  0.1163,  0.0996,  0.0420,  0.0717,  0.3004,  0.1665,\n",
       "                        0.0952,  0.2193,  0.2082,  0.1331,  0.0462,  0.0454,  0.3031,  0.0668,\n",
       "                        0.1382,  0.0065,  0.1390,  0.2062,  0.0802,  0.1937,  0.1335,  0.1380,\n",
       "                        0.0774,  0.0089,  0.0271,  0.0635, -0.0236,  0.1328, -0.0166,  0.3620,\n",
       "                        0.0157, -0.1441,  0.1039,  0.0191,  0.1679,  0.2336,  0.1163,  0.1007,\n",
       "                        0.1403,  0.0748,  0.1408,  0.0966,  0.3334, -0.1149,  0.1317,  0.0004,\n",
       "                        0.1468,  0.1691,  0.1392,  0.0330,  0.0923,  0.0216, -0.0037,  0.3547],\n",
       "                      device='cuda:0')),\n",
       "              ('gat1.lin_r.weight',\n",
       "               tensor([[-2.4980e-01, -1.3074e-01, -3.9046e-04,  ...,  1.1434e-01,\n",
       "                        -5.0918e-03,  9.3009e-02],\n",
       "                       [ 1.5229e-01,  3.1195e-02, -6.1738e-02,  ...,  1.0358e-01,\n",
       "                         1.1250e-01,  2.0079e-01],\n",
       "                       [ 4.7475e-01,  5.8185e-02,  8.4410e-02,  ...,  1.5468e-01,\n",
       "                         1.7069e-01,  1.0985e-01],\n",
       "                       ...,\n",
       "                       [-3.2909e-01, -3.9355e-03, -2.2010e-02,  ...,  6.2768e-02,\n",
       "                         1.1430e-01,  8.9302e-02],\n",
       "                       [ 3.3615e-01,  1.8476e-02,  1.4629e-01,  ...,  4.4904e-03,\n",
       "                         4.4279e-02, -8.2932e-02],\n",
       "                       [ 1.3953e-01,  4.3250e-02,  6.7761e-02,  ..., -7.9694e-02,\n",
       "                         1.0649e-01, -2.9238e-01]], device='cuda:0')),\n",
       "              ('gat1.lin_r.bias',\n",
       "               tensor([ 0.0266,  0.0601,  0.4665,  0.3953, -0.0193, -0.0746,  0.2608, -0.1453,\n",
       "                        0.2713,  0.0314,  0.2002, -0.2191, -0.0357,  0.0190,  0.1823,  0.0204,\n",
       "                       -0.0039,  0.2972, -0.1971, -0.0554, -0.0282, -0.0456,  0.5303, -0.1848,\n",
       "                        0.1117,  0.0560, -0.1547, -0.3595, -0.0224,  0.0063,  0.0917,  0.1620,\n",
       "                        0.1703,  0.0094, -0.0022,  0.1465, -0.0920, -0.0920,  0.2239,  0.1594,\n",
       "                        0.2405,  0.1352,  0.1212, -0.4479,  0.0707, -0.0084,  0.4906,  0.4138,\n",
       "                        0.0773,  0.1514, -0.0639, -0.5317, -0.2353, -0.3059, -0.0818, -0.0708,\n",
       "                       -0.2101,  0.1951, -0.0332,  0.1720,  0.0382, -0.1504, -0.0821, -0.2452],\n",
       "                      device='cuda:0')),\n",
       "              ('gat1.lin_edge.weight',\n",
       "               tensor([[-1.2290e-02,  1.2529e-01,  1.4935e-01,  1.3390e-01, -1.8208e-01,\n",
       "                         1.6961e-01, -1.8421e-01, -7.9462e-02],\n",
       "                       [ 7.0533e-02,  2.3326e-03,  2.4330e-01, -2.2630e-02, -2.5056e-01,\n",
       "                         2.2422e-01,  1.2221e-01, -1.9786e-01],\n",
       "                       [-5.5742e-02,  2.1141e-01, -1.4094e-01,  4.0373e-02,  1.4534e-01,\n",
       "                         1.3533e-01,  2.4876e-01,  2.4154e-01],\n",
       "                       [-1.0376e-01, -1.7179e-01, -2.1489e-02, -1.2280e-01,  6.2401e-02,\n",
       "                        -9.8417e-02,  1.7718e-02, -2.3714e-01],\n",
       "                       [-2.8132e-01,  2.2652e-01, -2.2454e-01,  3.1020e-01,  8.0019e-02,\n",
       "                        -1.9722e-01, -1.9065e-01,  6.4354e-02],\n",
       "                       [ 9.5509e-02, -1.9887e-01, -5.4190e-03,  2.7425e-01, -1.0987e-01,\n",
       "                         1.1719e-01,  8.4107e-02, -1.3055e-01],\n",
       "                       [-1.2371e-02, -9.8102e-02, -2.0100e-01, -1.4415e-01,  1.7438e-01,\n",
       "                        -1.3785e-01,  3.4978e-01, -9.7570e-03],\n",
       "                       [ 1.8555e-01, -1.7793e-01,  1.8333e-01, -1.0734e-02, -2.1966e-01,\n",
       "                        -1.2942e-02,  1.3067e-01, -4.7543e-02],\n",
       "                       [ 1.3119e-01,  1.8298e-01,  1.9360e-01, -2.5435e-01,  1.6968e-01,\n",
       "                        -1.3214e-01, -2.6017e-01,  2.2879e-01],\n",
       "                       [ 9.5653e-02,  2.0424e-01, -1.0984e-01,  7.8746e-02, -2.9430e-01,\n",
       "                        -3.4015e-01, -2.3842e-01, -2.5912e-01],\n",
       "                       [ 7.6555e-03,  1.9645e-01,  2.6821e-01, -7.3591e-02, -1.9501e-01,\n",
       "                         1.5790e-01, -4.5442e-02, -3.0574e-01],\n",
       "                       [-6.1064e-02, -2.3800e-02, -3.9960e-02, -6.0964e-02,  2.1380e-02,\n",
       "                         2.8925e-01, -6.3741e-02, -2.8944e-01],\n",
       "                       [-7.1168e-03, -2.0698e-01, -1.1645e-01,  4.3719e-02, -1.7414e-01,\n",
       "                        -1.0617e-01,  8.7530e-02, -6.0317e-02],\n",
       "                       [ 3.1715e-02,  1.9523e-01, -1.0230e-01, -1.7682e-01, -5.8734e-02,\n",
       "                        -7.4842e-02, -2.2312e-01,  8.3477e-02],\n",
       "                       [ 1.2524e-01,  1.6951e-01, -2.4274e-01,  1.6344e-01, -3.1448e-01,\n",
       "                        -8.8262e-02, -3.1064e-01, -1.3982e-01],\n",
       "                       [ 2.5915e-01, -2.3962e-01,  8.0708e-02, -5.9958e-02, -4.5954e-02,\n",
       "                         1.1173e-01, -1.2884e-01, -9.2989e-02],\n",
       "                       [ 2.9919e-01,  7.4230e-02, -1.1146e-01,  1.1932e-01,  4.3689e-02,\n",
       "                        -1.3726e-01, -2.0194e-01,  1.7765e-01],\n",
       "                       [ 1.3723e-01, -2.1278e-01,  2.3000e-01,  1.3654e-01, -2.4347e-01,\n",
       "                         1.1251e-01, -1.2551e-01, -1.0748e-01],\n",
       "                       [ 2.4839e-01, -1.3607e-01,  2.5920e-01, -9.2351e-02, -1.1171e-01,\n",
       "                         2.5187e-01,  8.1023e-02,  1.5386e-01],\n",
       "                       [-1.9276e-01,  8.4408e-02,  6.9534e-02, -8.7033e-02, -1.5308e-01,\n",
       "                        -3.0946e-02,  1.0436e-01,  2.1791e-01],\n",
       "                       [-1.8472e-01,  2.2467e-01,  6.7857e-02, -2.6586e-01, -4.0268e-02,\n",
       "                        -1.3425e-01, -3.4412e-01, -1.4420e-01],\n",
       "                       [-1.4166e-01,  1.2412e-01,  5.4332e-02, -1.7619e-01, -9.8961e-02,\n",
       "                        -2.3574e-01,  3.0735e-01,  1.1355e-01],\n",
       "                       [ 7.9361e-02, -1.3696e-01, -2.3984e-01,  1.0350e-01, -1.3580e-01,\n",
       "                         1.4750e-01, -2.4108e-02, -1.2066e-01],\n",
       "                       [ 1.6886e-01, -1.3188e-01,  6.0268e-02,  1.6053e-01,  1.9998e-01,\n",
       "                        -1.6726e-01, -1.6720e-01, -1.4525e-01],\n",
       "                       [ 1.4850e-01, -6.1168e-02, -1.5834e-01, -1.1258e-01, -1.5460e-02,\n",
       "                        -2.8215e-01,  3.2548e-02,  2.5103e-01],\n",
       "                       [ 8.5057e-02, -6.4216e-03,  1.5451e-01, -2.0057e-01, -2.5671e-02,\n",
       "                         1.1144e-01,  5.5253e-02, -4.0375e-02],\n",
       "                       [ 8.2626e-02,  1.4675e-01, -1.4978e-01, -2.6285e-02,  1.2330e-01,\n",
       "                         9.4968e-02, -1.1163e-01,  1.0441e-02],\n",
       "                       [-1.7089e-01,  1.2793e-01,  9.0860e-02, -2.2925e-01,  5.3871e-02,\n",
       "                        -9.6642e-02, -8.8792e-02, -9.5969e-02],\n",
       "                       [-2.6887e-02, -8.0762e-02, -2.2538e-01,  2.1826e-01, -2.3822e-01,\n",
       "                         1.9392e-02, -2.6470e-01,  1.4979e-02],\n",
       "                       [ 1.5115e-01, -7.7713e-02,  2.2164e-01,  4.1578e-02,  1.3689e-02,\n",
       "                        -1.1860e-01, -1.1772e-01,  1.1855e-01],\n",
       "                       [-7.4827e-02, -2.1474e-01, -7.1793e-02, -1.7180e-01,  3.1970e-02,\n",
       "                         1.5843e-01,  1.0421e-01,  1.8186e-01],\n",
       "                       [-1.8384e-01, -1.7786e-01, -5.8990e-02,  1.5352e-01, -1.8486e-01,\n",
       "                        -8.2503e-02,  1.0179e-01, -6.7303e-02],\n",
       "                       [-2.2814e-01, -7.2199e-02,  4.3482e-03,  1.3828e-01, -7.0334e-02,\n",
       "                         1.2462e-01, -3.8543e-02, -2.1166e-01],\n",
       "                       [-3.0778e-02,  1.5322e-01, -7.9961e-02, -1.0242e-01,  1.7086e-02,\n",
       "                         6.1867e-02, -2.1323e-01,  1.4355e-01],\n",
       "                       [ 2.7525e-04, -1.8568e-01, -2.2810e-01, -1.4986e-01,  9.1579e-02,\n",
       "                         8.7969e-03,  2.2425e-01,  2.4274e-01],\n",
       "                       [ 1.3661e-01, -1.1476e-01,  5.6740e-02, -1.0626e-01, -1.3879e-01,\n",
       "                        -2.8072e-01,  1.0405e-01, -1.3619e-01],\n",
       "                       [-1.5917e-01,  1.7286e-01,  2.6478e-01,  2.7723e-01, -8.0164e-02,\n",
       "                        -1.3843e-01, -1.5796e-01, -1.0345e-02],\n",
       "                       [ 6.4191e-02,  2.3536e-01,  1.1941e-02, -2.0822e-01, -1.7507e-02,\n",
       "                         3.0057e-02, -3.0186e-01,  6.6286e-02],\n",
       "                       [-8.5773e-02,  6.7328e-02,  2.1560e-01, -2.0364e-01, -1.6445e-02,\n",
       "                         1.5297e-01, -4.1535e-02, -9.8080e-02],\n",
       "                       [-2.5003e-01,  1.1663e-01, -1.9442e-01,  1.1986e-01, -1.9865e-01,\n",
       "                         2.2098e-01, -1.9597e-01, -1.4504e-01],\n",
       "                       [-1.3430e-01,  8.7548e-02, -5.2908e-02, -1.1755e-01,  1.2098e-01,\n",
       "                         7.2010e-03,  3.1189e-01,  1.1985e-01],\n",
       "                       [ 1.2455e-01, -1.8528e-01,  8.1030e-02, -2.7950e-02, -1.0085e-01,\n",
       "                        -2.6346e-01, -3.3797e-01, -6.4829e-02],\n",
       "                       [-1.0484e-01,  1.0605e-01,  1.8073e-01, -2.4820e-02, -6.4192e-02,\n",
       "                        -6.0842e-02, -4.0431e-02,  1.4462e-01],\n",
       "                       [-4.0129e-02, -2.5688e-01, -2.7135e-02,  9.1915e-02,  5.1155e-02,\n",
       "                         3.4707e-02,  2.2435e-01, -2.3268e-01],\n",
       "                       [ 1.3850e-01, -1.2157e-01,  2.2540e-01,  2.0367e-01,  6.4865e-02,\n",
       "                         6.7587e-02,  1.5595e-01,  1.0506e-01],\n",
       "                       [ 8.3523e-02, -2.4291e-01, -3.2921e-02,  3.5323e-02,  1.4507e-01,\n",
       "                        -8.4483e-02, -3.5168e-02, -2.3732e-01],\n",
       "                       [ 1.4832e-02,  2.4132e-01, -2.6180e-03, -3.4594e-02,  6.9596e-02,\n",
       "                         9.7636e-02, -4.3504e-02,  5.9612e-02],\n",
       "                       [ 1.6257e-01, -1.7802e-01, -1.3698e-01, -7.1355e-02, -9.7210e-02,\n",
       "                         9.2749e-03,  5.4352e-02, -7.5139e-02],\n",
       "                       [ 2.7879e-02,  1.7935e-01, -2.2313e-01, -6.3671e-02, -1.2919e-01,\n",
       "                        -2.0626e-01, -5.7298e-02,  1.5301e-01],\n",
       "                       [ 4.0860e-02, -8.4272e-02,  7.7896e-02, -1.3118e-01,  1.3643e-01,\n",
       "                        -5.5645e-02, -4.1266e-03,  6.7223e-02],\n",
       "                       [-2.3898e-01,  1.0862e-01,  7.6121e-02, -1.9782e-01,  2.6478e-01,\n",
       "                         1.0852e-01, -6.1893e-03, -3.0136e-01],\n",
       "                       [ 4.5642e-02, -1.5710e-02,  2.7605e-01, -6.2646e-02, -1.6020e-01,\n",
       "                        -2.5302e-01, -1.3710e-01,  1.3262e-01],\n",
       "                       [-2.2626e-01,  1.9192e-01,  1.4098e-01, -6.2069e-02, -2.7150e-02,\n",
       "                         6.0375e-02,  3.2869e-01, -8.8438e-02],\n",
       "                       [-1.4454e-01, -1.0774e-01,  2.1917e-01, -5.2177e-02, -1.0593e-01,\n",
       "                        -2.1455e-01,  1.9462e-01, -1.5057e-01],\n",
       "                       [-1.0301e-01,  2.5529e-01,  1.4054e-01, -1.7082e-02,  2.4146e-01,\n",
       "                         7.8372e-02,  3.0156e-01,  1.3898e-01],\n",
       "                       [-1.5935e-01, -2.2972e-01,  5.9326e-02,  1.6126e-01, -1.8985e-01,\n",
       "                        -3.7415e-02,  3.0348e-01,  7.0668e-02],\n",
       "                       [ 1.2727e-01,  2.0387e-01, -2.0861e-01, -2.5680e-01,  8.6589e-02,\n",
       "                        -5.7803e-02,  2.2480e-01,  1.4982e-01],\n",
       "                       [ 1.4054e-01,  1.8410e-01, -2.4281e-01,  1.3757e-01,  1.1480e-01,\n",
       "                         4.2854e-02, -1.0125e-01, -1.1120e-01],\n",
       "                       [ 8.5540e-02,  2.4802e-01,  8.9839e-02,  7.7797e-03,  1.5800e-01,\n",
       "                         1.2269e-01,  5.9959e-02,  1.0455e-01],\n",
       "                       [ 1.2545e-01, -1.5805e-01,  2.0211e-01, -6.7789e-02,  4.7644e-02,\n",
       "                         1.2655e-02,  1.9230e-01,  1.4368e-01],\n",
       "                       [-2.1154e-01, -9.2513e-02,  2.3161e-01,  2.3327e-01, -4.4935e-03,\n",
       "                         2.9214e-03,  2.0825e-01, -2.5735e-01],\n",
       "                       [-1.7214e-02, -1.7907e-01,  4.1965e-02,  1.2794e-01,  5.6721e-02,\n",
       "                        -1.4876e-01,  9.7281e-02, -4.9787e-03],\n",
       "                       [-2.1834e-01, -3.6940e-02,  4.5229e-03, -5.2681e-02,  7.4498e-02,\n",
       "                        -2.3972e-01, -5.0535e-02,  1.7594e-01],\n",
       "                       [ 3.0325e-01,  8.1950e-02,  7.1927e-02, -1.7807e-02,  4.3399e-02,\n",
       "                        -6.0712e-02, -2.4675e-01,  2.1154e-02]], device='cuda:0')),\n",
       "              ('gat2.att',\n",
       "               tensor([[[-0.4685,  0.2953,  0.4250,  0.4178, -0.0609, -0.4048, -0.4107,\n",
       "                         -0.1918, -0.3796,  0.1935,  0.0859, -0.2356, -0.3070, -0.3174,\n",
       "                          0.4457,  0.4202, -0.0450, -0.1979,  0.1576, -0.3339,  0.1752,\n",
       "                         -0.1185,  0.4114, -0.1663, -0.3156, -0.4570,  0.2100,  0.1135,\n",
       "                          0.1360, -0.3396, -0.1715,  0.1738]]], device='cuda:0')),\n",
       "              ('gat2.bias',\n",
       "               tensor([ 0.2389,  0.0671,  0.0097,  0.1435,  0.0335, -0.0020,  0.0317,  0.0011,\n",
       "                        0.0774,  0.0795,  0.0508,  0.0258,  0.0716,  0.0306, -0.0061,  0.3165,\n",
       "                        0.1028,  0.2722,  0.0334, -0.0525,  0.0973,  0.4045,  0.0880,  0.0595,\n",
       "                        0.0291,  0.0926,  0.0363, -0.0949,  0.0890,  0.2149,  0.0315,  0.0090],\n",
       "                      device='cuda:0')),\n",
       "              ('gat2.lin_l.weight',\n",
       "               tensor([[ 0.7235, -0.1024,  0.1883,  ...,  0.8365,  0.0635,  0.4350],\n",
       "                       [-0.0513,  0.2132,  0.0136,  ...,  0.1012,  0.3669,  0.4129],\n",
       "                       [-0.1100,  0.0455, -0.0640,  ...,  0.4056,  0.0145,  0.3494],\n",
       "                       ...,\n",
       "                       [ 0.3578,  0.3525, -0.2761,  ...,  0.0260,  0.2352,  0.2857],\n",
       "                       [ 0.3928,  0.1227, -0.1315,  ...,  0.4190,  0.0808, -0.0347],\n",
       "                       [ 0.2193, -0.1061,  0.2865,  ...,  0.0850,  0.0964,  0.3805]],\n",
       "                      device='cuda:0')),\n",
       "              ('gat2.lin_l.bias',\n",
       "               tensor([ 0.0311,  0.1080, -0.1706,  0.0578,  0.0277,  0.0085,  0.1513, -0.0729,\n",
       "                        0.0666,  0.1435,  0.0050, -0.1555, -0.0255, -0.0336,  0.0517,  0.2158,\n",
       "                        0.1215,  0.0676,  0.0871,  0.0490, -0.0301,  0.3765,  0.1115,  0.0357,\n",
       "                       -0.0037,  0.0178, -0.0296, -0.1522,  0.1333,  0.2329, -0.0122, -0.0342],\n",
       "                      device='cuda:0')),\n",
       "              ('gat2.lin_r.weight',\n",
       "               tensor([[ 0.0367,  0.1572,  0.1566,  ...,  0.1244,  0.0609,  0.2908],\n",
       "                       [-0.1042, -0.0038,  0.0493,  ..., -0.0243, -0.1756,  0.0141],\n",
       "                       [-0.5465,  0.0187, -0.0092,  ..., -0.3703,  0.2238,  0.0949],\n",
       "                       ...,\n",
       "                       [-0.3002, -0.1572, -0.1567,  ..., -0.5274,  0.2239, -0.1759],\n",
       "                       [ 0.1151,  0.1849, -0.0689,  ...,  0.4509,  0.1330, -0.1553],\n",
       "                       [-0.0947, -0.1546, -0.2795,  ..., -0.4079,  0.2388,  0.0245]],\n",
       "                      device='cuda:0')),\n",
       "              ('gat2.lin_r.bias',\n",
       "               tensor([-0.1749, -0.1283, -0.2252, -0.0747,  0.1552,  0.1471,  0.1487,  0.1621,\n",
       "                        0.1779, -0.0882, -0.1434, -0.0446, -0.0898, -0.1509, -0.0112,  0.0327,\n",
       "                       -0.0401,  0.0471, -0.1166,  0.0715, -0.1950, -0.1228,  0.0627,  0.1709,\n",
       "                       -0.0026, -0.1816, -0.1954,  0.1045, -0.1083, -0.0303,  0.2375, -0.1272],\n",
       "                      device='cuda:0')),\n",
       "              ('gat2.lin_edge.weight',\n",
       "               tensor([[-0.2739, -0.1865, -0.0357, -0.1611,  0.3063, -0.2675, -0.2549,  0.2815],\n",
       "                       [ 0.0398, -0.2387, -0.0930, -0.1015,  0.0875, -0.1842, -0.2303,  0.0393],\n",
       "                       [-0.2081,  0.1286,  0.0923,  0.3429,  0.2315, -0.0475,  0.0542,  0.1150],\n",
       "                       [-0.0428, -0.1193, -0.0993, -0.0934,  0.2300,  0.0985,  0.1441, -0.0232],\n",
       "                       [-0.2258,  0.0369, -0.1270,  0.2268, -0.2873,  0.3539,  0.2868, -0.0523],\n",
       "                       [-0.3351,  0.3326,  0.0832, -0.0317,  0.3461, -0.2517,  0.0344, -0.3622],\n",
       "                       [ 0.0556, -0.4040,  0.1599,  0.0466, -0.0826,  0.0181, -0.3728, -0.1519],\n",
       "                       [-0.1640, -0.0474, -0.1526,  0.1072,  0.1881,  0.0187,  0.0216, -0.2544],\n",
       "                       [-0.0656, -0.2783,  0.4522,  0.1547,  0.2218,  0.2755, -0.2591,  0.0533],\n",
       "                       [ 0.0022, -0.2534, -0.0544,  0.3046, -0.0205,  0.1836, -0.2547, -0.2634],\n",
       "                       [ 0.0214,  0.2541, -0.0151,  0.0416,  0.3080, -0.0979, -0.2709, -0.0837],\n",
       "                       [ 0.1215,  0.2282,  0.1952,  0.1827, -0.3559,  0.0252, -0.2545, -0.2183],\n",
       "                       [-0.2328,  0.0502, -0.1324, -0.0146,  0.3671, -0.2551, -0.2195,  0.2994],\n",
       "                       [-0.0160, -0.1700,  0.4120, -0.1781, -0.2479,  0.3452, -0.1833, -0.0464],\n",
       "                       [ 0.3618, -0.0072, -0.1824, -0.0983,  0.1690, -0.3146,  0.2463,  0.1313],\n",
       "                       [ 0.0546,  0.2700,  0.0790, -0.1215,  0.2554,  0.1523,  0.2971, -0.0481],\n",
       "                       [ 0.1913, -0.0217,  0.3274,  0.3182, -0.1124,  0.0839,  0.2327,  0.0654],\n",
       "                       [-0.0540, -0.0184, -0.2372, -0.3254, -0.2373,  0.1635,  0.0890,  0.0563],\n",
       "                       [-0.3782, -0.1716,  0.4637, -0.2693,  0.1344,  0.2636,  0.4141, -0.3337],\n",
       "                       [-0.2377,  0.1631, -0.1299, -0.2057,  0.1082,  0.2198,  0.0370,  0.0662],\n",
       "                       [-0.3148, -0.1585,  0.2830, -0.1481,  0.0579, -0.0149, -0.0408, -0.2338],\n",
       "                       [ 0.0974, -0.2533, -0.1729,  0.3468,  0.1807,  0.2861, -0.0688,  0.3076],\n",
       "                       [-0.0154, -0.1392, -0.3853, -0.1103,  0.3736, -0.2984,  0.3092, -0.1881],\n",
       "                       [ 0.2053,  0.2086, -0.0256, -0.2749, -0.2409,  0.2463, -0.2429,  0.0104],\n",
       "                       [ 0.1650,  0.0169,  0.0949,  0.1152,  0.0202,  0.3009,  0.2192,  0.2659],\n",
       "                       [-0.2538,  0.3241, -0.1306,  0.3252,  0.2090,  0.0780, -0.4268,  0.1965],\n",
       "                       [-0.1743, -0.1780,  0.0891,  0.3126, -0.2692, -0.0732,  0.3736, -0.1685],\n",
       "                       [-0.2287,  0.0491, -0.3999,  0.3283,  0.0701, -0.1253,  0.0845, -0.1457],\n",
       "                       [ 0.2739, -0.3355, -0.2543,  0.0408, -0.1783, -0.3576, -0.1071,  0.3349],\n",
       "                       [-0.2775,  0.0071, -0.2885,  0.2768,  0.2806, -0.2759, -0.3703,  0.2624],\n",
       "                       [ 0.3123, -0.2105,  0.0810, -0.1322, -0.1337,  0.3047,  0.2156, -0.2090],\n",
       "                       [ 0.0819, -0.1934, -0.2979, -0.3648,  0.1741, -0.0562,  0.2116, -0.0563]],\n",
       "                      device='cuda:0')),\n",
       "              ('edge_mlp.0.weight',\n",
       "               tensor([[-0.5499,  0.0449,  0.2856,  ..., -0.1724,  0.0132,  0.1327],\n",
       "                       [-0.4706,  0.0122, -0.0265,  ..., -0.1522, -0.0519, -0.0437],\n",
       "                       [-0.9390,  0.3126, -0.0078,  ...,  0.1114, -0.0433, -0.0270],\n",
       "                       ...,\n",
       "                       [ 0.2944, -0.2681,  0.1150,  ..., -0.0390,  0.0937,  0.2099],\n",
       "                       [-0.0424,  0.0797,  0.0664,  ..., -0.1538,  0.1319, -0.0281],\n",
       "                       [ 0.1257,  0.0738, -0.1272,  ..., -0.1315, -0.1119,  0.1954]],\n",
       "                      device='cuda:0')),\n",
       "              ('edge_mlp.0.bias',\n",
       "               tensor([ 0.0255,  0.0518,  0.0466,  0.0746, -0.0859,  0.0525, -0.0209,  0.1161,\n",
       "                        0.1086, -0.0038,  0.0233, -0.0704, -0.0593, -0.2053, -0.2010, -0.0198,\n",
       "                       -0.0009, -0.0380,  0.0564,  0.0142, -0.1770,  0.0555,  0.0945,  0.0741,\n",
       "                       -0.0696, -0.0952, -0.0018,  0.0125,  0.0756,  0.1357,  0.0784, -0.2071],\n",
       "                      device='cuda:0')),\n",
       "              ('edge_mlp.2.weight',\n",
       "               tensor([[ 0.3262,  0.1559,  0.2329,  0.2821, -0.2041, -0.1991,  0.1678, -0.2905,\n",
       "                         0.3498, -0.3515,  0.2339,  0.7017, -0.1686, -0.3762, -0.3759,  0.2424,\n",
       "                         0.2408,  0.2254,  0.2336, -0.2721, -0.2865,  0.1539,  0.2548,  0.2006,\n",
       "                        -0.1904, -0.2999,  0.4081,  0.3798, -0.2808,  0.2220,  0.3312, -0.2366]],\n",
       "                      device='cuda:0')),\n",
       "              ('edge_mlp.2.bias', tensor([0.0247], device='cuda:0'))]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TarGraphDataset(datafile)\n",
    "split = int(len(dataset)*0.8)\n",
    "train_ds = Subset(dataset, range(split))\n",
    "val_ds   = Subset(dataset, range(split, len(dataset)))\n",
    "train_loader = make_loader(train_ds, shuffle=True)\n",
    "val_loader = make_loader(val_ds, shuffle=True)\n",
    "\n",
    "model = GraphAttentionNetwork(in_dim = 96, edge_in_dim = 197, edge_emb_dim = 8, hidden1 = 64, hidden2 = 32, heads = 1)\n",
    "\n",
    "train_model(model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            num_epochs     = 10,\n",
    "            lr             = 1e-4,\n",
    "            validate_every = 1,\n",
    "            patience       = 10,\n",
    "            device         = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), \"model10epoch.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
