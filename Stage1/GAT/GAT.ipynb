{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 16\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tensor(filename: str,\n",
    "                  dtype    : torch.dtype,\n",
    "                  is_sparse: bool = False):\n",
    "    \"\"\"\n",
    "    Load a .npz CSR matrix and return either\n",
    "      • a torch.sparse_csr_tensor              (if is_sparse=True)\n",
    "      • a torch.Tensor (dense)                 (otherwise)\n",
    "    \"\"\"\n",
    "    csr = sparse.load_npz(filename).tocsr()\n",
    "\n",
    "    if is_sparse:\n",
    "        crow = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "        col  = torch.from_numpy(csr.indices.astype(np.int64))\n",
    "        val  = torch.from_numpy(csr.data).to(dtype)\n",
    "\n",
    "        return torch.sparse_csr_tensor(\n",
    "            crow, col, val,\n",
    "            size=csr.shape,\n",
    "            dtype=dtype,\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "    # — otherwise densify —\n",
    "    return torch.from_numpy(csr.toarray()).to(dtype)\n",
    "\n",
    "def read_data(folder_paths, has_label=True):\n",
    "    graph_data = []\n",
    "    for folder_path in folder_paths:\n",
    "        X = import_tensor(folder_path/\"X.npz\", torch.long, False)\n",
    "        A = import_tensor(folder_path/\"A.npz\", torch.long, True)\n",
    "        E = import_tensor(folder_path/\"E.npz\", torch.float32, True)\n",
    "        edge_index = np.load(folder_path/\"edge_index.npy\")\n",
    "        if has_label:\n",
    "            label_features = import_tensor(folder_path/\"labels.npz\", torch.float32, True)\n",
    "            label_index = np.load(folder_path/\"label_index.npy\")\n",
    "            label_value = np.load(folder_path/\"label_value.npy\")\n",
    "        else:\n",
    "            label_features,label_index,label_value = None, None, None\n",
    "        \n",
    "        graph_data.append((X,A,E,edge_index,label_features,label_index,label_value))\n",
    "    \n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = Path(\"../../data/swde_HTMLgraphs/movie/movie\")\n",
    "batchFiles = list(src.rglob(\"[0-9][0-9][0-9][0-9]\"))\n",
    "data = read_data(batchFiles[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d224978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[546, 509],\n",
       "       [516, 548],\n",
       "       [516, 549],\n",
       "       [521, 470],\n",
       "       [538, 616],\n",
       "       [522, 403],\n",
       "       [518, 588],\n",
       "       [523, 473],\n",
       "       [528, 555],\n",
       "       [528, 556],\n",
       "       [528, 557],\n",
       "       [528, 558],\n",
       "       [524, 589],\n",
       "       [530, 593],\n",
       "       [530, 594],\n",
       "       [530, 595],\n",
       "       [530, 596],\n",
       "       [530, 597],\n",
       "       [530, 598],\n",
       "       [530, 599],\n",
       "       [530, 600],\n",
       "       [530, 601],\n",
       "       [530, 602],\n",
       "       [530, 603],\n",
       "       [519, 398],\n",
       "       [542, 584],\n",
       "       [536, 615],\n",
       "       [278, 325],\n",
       "       [325, 326],\n",
       "       [301, 381],\n",
       "       [544, 618],\n",
       "       [544, 619],\n",
       "       [544, 620],\n",
       "       [514, 387],\n",
       "       [540, 617],\n",
       "       [257, 305],\n",
       "       [257, 307],\n",
       "       [257, 309],\n",
       "       [257, 311],\n",
       "       [257, 313],\n",
       "       [257, 315],\n",
       "       [257, 317],\n",
       "       [257, 319],\n",
       "       [257, 321],\n",
       "       [257, 323],\n",
       "       [532, 604],\n",
       "       [532, 605],\n",
       "       [532, 606],\n",
       "       [532, 607],\n",
       "       [532, 608],\n",
       "       [534, 609],\n",
       "       [534, 610],\n",
       "       [534, 611],\n",
       "       [534, 612],\n",
       "       [534, 613],\n",
       "       [534, 614],\n",
       "       [526, 590],\n",
       "       [526, 591],\n",
       "       [526, 592],\n",
       "       [513, 456],\n",
       "       [326, 321],\n",
       "       [321, 326],\n",
       "       [301,  12],\n",
       "       [ 12, 301],\n",
       "       [313, 275],\n",
       "       [275, 313],\n",
       "       [305, 269],\n",
       "       [269, 305],\n",
       "       [611, 611],\n",
       "       [611, 611],\n",
       "       [599, 595],\n",
       "       [595, 599],\n",
       "       [257, 381],\n",
       "       [381, 257],\n",
       "       [257, 326],\n",
       "       [326, 257],\n",
       "       [594, 593],\n",
       "       [593, 594],\n",
       "       [313, 267],\n",
       "       [267, 313],\n",
       "       [257, 301],\n",
       "       [301, 257],\n",
       "       [598, 593],\n",
       "       [593, 598],\n",
       "       [323, 259],\n",
       "       [259, 323],\n",
       "       [594, 597],\n",
       "       [597, 594],\n",
       "       [311, 275],\n",
       "       [275, 311],\n",
       "       [257, 381],\n",
       "       [381, 257],\n",
       "       [536, 536],\n",
       "       [536, 536],\n",
       "       [307, 259],\n",
       "       [259, 307],\n",
       "       [317, 309],\n",
       "       [309, 317],\n",
       "       [381, 280],\n",
       "       [280, 381],\n",
       "       [313, 265],\n",
       "       [265, 313],\n",
       "       [278, 320],\n",
       "       [320, 278],\n",
       "       [321, 318],\n",
       "       [318, 321],\n",
       "       [257, 265],\n",
       "       [265, 257],\n",
       "       [325, 324],\n",
       "       [324, 325],\n",
       "       [257, 327],\n",
       "       [327, 257],\n",
       "       [257, 381],\n",
       "       [381, 257],\n",
       "       [544, 381],\n",
       "       [381, 544],\n",
       "       [257, 277],\n",
       "       [277, 257],\n",
       "       [601, 599],\n",
       "       [599, 601],\n",
       "       [309, 269],\n",
       "       [269, 309],\n",
       "       [313, 323],\n",
       "       [323, 313],\n",
       "       [584, 584],\n",
       "       [584, 584],\n",
       "       [540, 540],\n",
       "       [540, 540],\n",
       "       [257, 304],\n",
       "       [304, 257],\n",
       "       [321, 265],\n",
       "       [265, 321],\n",
       "       [603, 596],\n",
       "       [596, 603],\n",
       "       [602, 593],\n",
       "       [593, 602],\n",
       "       [619, 619],\n",
       "       [619, 619],\n",
       "       [257, 320],\n",
       "       [320, 257],\n",
       "       [594, 594],\n",
       "       [594, 594],\n",
       "       [557, 557],\n",
       "       [557, 557],\n",
       "       [257, 256],\n",
       "       [256, 257],\n",
       "       [257, 267],\n",
       "       [267, 257],\n",
       "       [317, 277],\n",
       "       [277, 317],\n",
       "       [325, 316],\n",
       "       [316, 325],\n",
       "       [321, 324],\n",
       "       [324, 321],\n",
       "       [326, 302],\n",
       "       [302, 326],\n",
       "       [323, 301],\n",
       "       [301, 323],\n",
       "       [534, 534],\n",
       "       [534, 534],\n",
       "       [592, 591],\n",
       "       [591, 592],\n",
       "       [313, 256],\n",
       "       [256, 313],\n",
       "       [534, 603],\n",
       "       [603, 534],\n",
       "       [523, 523],\n",
       "       [523, 523],\n",
       "       [257, 256],\n",
       "       [256, 257],\n",
       "       [319, 309],\n",
       "       [309, 319],\n",
       "       [596, 602],\n",
       "       [602, 596],\n",
       "       [532, 532],\n",
       "       [532, 532],\n",
       "       [257, 275],\n",
       "       [275, 257],\n",
       "       [618, 620],\n",
       "       [620, 618]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e379c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to normalise the A matrix\n",
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return (D_inv @ A_tilde @ D_inv).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To advance the model, use the methods in https://arxiv.org/pdf/2311.02921\n",
    "\n",
    "class GraphAttentionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    HTML‑graph model\n",
    "\n",
    "        X  ─╮\n",
    "            │  GAT( 96 → 64 )\n",
    "            │  ReLU\n",
    "            │  GAT( 64 → 32 )\n",
    "            │  ReLU\n",
    "            └─ Edge‑feature constructor\n",
    "                      [h_i ‖ h_j ‖ φ(e_ij)] ─► MLP(69 → 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_dim          : node‑feature size   (= 96)\n",
    "    edge_in_dim     : raw edge‑feature size (= 197)\n",
    "    edge_emb_dim    : Edge-feature MLP output dims\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int        = 96,\n",
    "                 edge_in_dim: int   = 197,\n",
    "                 edge_emb_dim: int  = 8,\n",
    "                 hidden1: int       = 64,\n",
    "                 hidden2: int       = 32,\n",
    "                 heads:  int        = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ── Node-level encoder (edge-aware) ────────────────────────────\n",
    "        self.gat1 = GATv2Conv(in_dim,\n",
    "                              hidden1,\n",
    "                              heads=heads,\n",
    "                              concat=True,\n",
    "                              edge_dim=edge_emb_dim,\n",
    "                              fill_value=\"zeros\")\n",
    "\n",
    "        self.gat2 = GATv2Conv(hidden1 * heads,\n",
    "                              hidden2,\n",
    "                              heads=1,\n",
    "                              concat=True,\n",
    "                              edge_dim=edge_emb_dim,\n",
    "                              fill_value=\"zeros\")\n",
    "\n",
    "        # ── Edge feature projector ────────────── (It is not a linear layer as it does works on a sparse matrix)\n",
    "        self.W_edge = nn.Parameter(torch.empty(edge_in_dim, edge_emb_dim))\n",
    "        nn.init.xavier_uniform_(self.W_edge)\n",
    "\n",
    "        # ── Edge-level MLP decoder (unchanged) ────────────────────────\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden2 * 2 + edge_emb_dim, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dense: torch.Tensor,        # (N_nodes, 96)          sparse\n",
    "        A_edge_index: torch.Tensor,   # (2, nnz_A)             COO  (from A)\n",
    "        A_edge_attr: torch.Tensor,    # (nnz_A, 197)           dense / sparse.mm\n",
    "        E_edge_index: torch.Tensor,   # (2, N_E)               candidates\n",
    "        E_edge_attr: torch.Tensor     # (N_E, 197)             sparse features\n",
    "    ):\n",
    "        # 1) node features\n",
    "        #x = x_sparse.to_dense()\n",
    "        A_edge_emb = torch.sparse.mm(A_edge_attr, self.W_edge)     # (nnz_A , 8)\n",
    "\n",
    "        # 2) edge-aware GATv2 layers\n",
    "        h = F.relu(self.gat1(x_dense, A_edge_index, A_edge_emb))\n",
    "        h = F.relu(self.gat2(h, A_edge_index, A_edge_emb))   # (N_nodes , 32)\n",
    "\n",
    "        # 3) candidate-edge projection  φ(E) = E @ W_edge\n",
    "        E_edge_emb = torch.sparse.mm(E_edge_attr, self.W_edge)     # (N_E , 8)\n",
    "\n",
    "        # 4) gather node embeddings and classify\n",
    "        src, dst = E_edge_index\n",
    "        z = torch.cat([h[src], h[dst], E_edge_emb], dim=1)      # (N_E , 72)\n",
    "        return self.edge_mlp(z).squeeze(-1)                   # (N_E ,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96e35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEwLogits\n",
    "\n",
    "IGNORE_LABEL = -1      # change if you use another sentinel for “no label”\n",
    "\n",
    "# ---------- 1. One training epoch -------------------------------------------\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,               # iterable that yields (X, A, E, edge_index, y)\n",
    "    optimizer,\n",
    "    criterion=BCEwLogits,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    model.train()\n",
    "    total_loss, total_edges = 0.0, 0\n",
    "\n",
    "    for X, A, E, edge_index, y in dataloader:\n",
    "        # Move to device ------------------------------------------------------\n",
    "        X          = X.to(device)\n",
    "        A          = A.to(device)\n",
    "        E          = E.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        y          = y.to(device)\n",
    "\n",
    "        mask = (y != IGNORE_LABEL)          # only supervise labelled edges\n",
    "        if mask.sum() == 0:                 # nothing to learn in this sample\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(X, A, E, edge_index) # (N_edges,)\n",
    "\n",
    "        loss = criterion(logits[mask], y[mask].float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss   += loss.item() * mask.sum().item()\n",
    "        total_edges  += mask.sum().item()\n",
    "\n",
    "    return total_loss / max(total_edges, 1)   # average over labelled edges\n",
    "\n",
    "\n",
    "# ---------- 2. Full training loop -------------------------------------------\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,                 # edge‑level dataloader\n",
    "    val_loader,                   # edge‑level dataloader\n",
    "    num_epochs       = 100,\n",
    "    lr               = 1e-3,\n",
    "    validate_every   = 10,\n",
    "    patience         = 10,\n",
    "    device           = \"cpu\"\n",
    "):\n",
    "    \"\"\"Train `model` to predict whether an edge exists.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer  = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler  = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=0.5, verbose=False\n",
    "    )\n",
    "\n",
    "    best_val_f1, best_state = 0.0, None\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # ---- validation -----------------------------------------------------\n",
    "        if epoch % validate_every == 0 or epoch == num_epochs:\n",
    "            val_prec, val_rec, val_f1 = evaluate_edge_model(\n",
    "                model, val_loader, device=device\n",
    "            )\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(\n",
    "                f\"Epoch {epoch:03d}/{num_epochs}  \"\n",
    "                f\"loss={loss:.4f}  \"\n",
    "                f\"P={val_prec:.3f}  R={val_rec:.3f}  F1={val_f1:.3f}  \"\n",
    "                f\"lr={current_lr:.2e}\"\n",
    "            )\n",
    "\n",
    "            # keep the best‑F1 checkpoint\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_state  = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # early stop if LR too small\n",
    "            if current_lr < 1e-5:\n",
    "                print(\"LR below 1e‑5 → stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return loss_history, best_state\n",
    "\n",
    "\n",
    "# ---------- 3. Helper: evaluation (precision / recall / F1) -----------------\n",
    "@torch.no_grad()\n",
    "def evaluate_edge_model(model, dataloader, device=\"cpu\", thr=0.5):\n",
    "    model.eval()\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    for X, A, E, edge_index, y in dataloader:\n",
    "        X, A, E = X.to(device), A.to(device), E.to(device)\n",
    "        edge_index, y = edge_index.to(device), y.to(device)\n",
    "\n",
    "        mask = (y != IGNORE_LABEL)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        logits = model(X, A, E, edge_index)\n",
    "        probs  = torch.sigmoid(logits)\n",
    "\n",
    "        pred = (probs >= thr).long()\n",
    "        tp  += ((pred == 1) & (y == 1) & mask).sum().item()\n",
    "        fp  += ((pred == 1) & (y == 0) & mask).sum().item()\n",
    "        fn  += ((pred == 0) & (y == 1) & mask).sum().item()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-9)\n",
    "    recall    = tp / (tp + fn + 1e-9)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67146255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
